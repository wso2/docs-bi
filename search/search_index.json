{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"WSO2 Integrator: BI is a low-code integration solution built on Ballerina, enabling fast and efficient integration development with minimal coding. The WSO2 Integrator: BI extension for Visual Studio Code (VS Code) provides a familiar, AI-assisted environment that streamlines tasks and enhances accuracy, accelerating digital transformation efforts.          Get Started \ud83d\ude80 <ul> <li> Quick Start Guide </li> <li> Install WSO2 Integrator: BI </li> <li> Develop Integration as API </li> <li> Develop Automation </li> <li> Develop File Integration </li> <li> Develop Event Integration </li> <li> Develop AI Agent </li> </ul> Developer Guides \ud83d\udee0\ufe0f  <ul> <li> Design the Integrations </li> <li> Data Mapping </li> <li> Testing </li> <li> Debugging &amp; Troubleshooting </li> <li> Protocols and Connectors </li> <li> Integration Tools </li> <li> Migration Tools </li> <li> Other Tools </li> </ul> AI \ud83e\udd16 <ul> <li> AI Agents and other Gen AI Integrations </li> <li> AI for Integration </li> </ul> Integration Guides \ud83d\udcda <ul> <li> AI Agents and other Gen AI Integrations </li> <li> Integration as API </li> <li> File Integration </li> </ul> Deploy \u2699\ufe0f <ul> <li> Overview </li> <li> Deploy to Devant </li> <li> Containerized Deployment </li> <li> VM-based Deployment </li> <li> Managing Configurations </li> <li> Capacity Planning </li> <li> Deployment Best Practices </li> </ul> Observability and Monitoring \ud83d\udcc8  <ul> <li> Overview </li> <li> Monitoring with WSO2 Integrator: ICP </li> <li> Observability with Devant </li> <li> Supported Observability Tools and Platforms </li> </ul> References \ud83d\udcd6 <ul> <li> Enterprise Integration Patterns </li> <li> System requirements </li> <li> AI Usage and Data Handling Guidelines </li> <li> Connector Development Guidelines </li> </ul> Community &amp; Support \u2753 <ul> <li> GitHub </li> <li> Discord </li> <li> Enterprise Support </li> <li> Release Note </li> </ul>"},{"location":"page-not-found/","title":"Page not found","text":"<p>Try one of the navigation links above or use the search engine in the top right corner.</p>"},{"location":"wso2-integrator-bi-release-notes/","title":"About the Release","text":""},{"location":"wso2-integrator-bi-release-notes/#whats-new-in-wso2-integrator-bi-150-release","title":"What's new in WSO2 Integrator: BI 1.5.0 release?","text":"<p>WSO2 Integrator: BI 1.5.0 introduces the following features and enhancements:</p>"},{"location":"wso2-integrator-bi-release-notes/#new-features","title":"New features","text":"Multi-Project Workspace Support<p>Users can open and manage mono-repositories containing multiple projects,  with comprehensive visual editing capabilities for complex integration scenarios across multiple projects.</p> Natural Language Function Support<p>Introduced natural language functions that allow users to directly invoke Large Language Models (LLMs) within integration flows.</p>"},{"location":"wso2-integrator-bi-release-notes/#fixed-issues","title":"Fixed issues","text":"<ul> <li>WSO2 Integrator: BI Issues</li> </ul>"},{"location":"wso2-integrator-bi-release-notes/#whats-new-in-wso2-integrator-bi-140-release","title":"What's new in WSO2 Integrator: BI 1.4.0 release?","text":"<p>WSO2 Integrator: BI 1.4.0 introduces the following features and enhancements:</p>"},{"location":"wso2-integrator-bi-release-notes/#new-features_1","title":"New features","text":"MCP AI Integration<p>Added support for MCP (Model Context Protocol) AI Integration to expose tools via an MCP service.</p> Solace Event Integration<p>Added support for Solace Event integration, expanding supported event integration capabilities. The Event Integration flows have been redesigned to provide a more streamlined experience, complemented by AI-powered payload generation for faster development.</p> Service Designer Enhancements<p>Revamped the Service Designer view with better organization of listener and service properties. Features more readable listener names and improved metadata display, making service configuration more intuitive and accessible.</p> Data Mapper Enhancements<p>Introduced an intelligent Data Mapper powered by Large Language Models (LLMs), enabling more intuitive and efficient data transformation workflows with AI assistance. Enhanced breadcrumb labels for better navigation and refactored preview behavior for output-side arrays, resulting in a more predictable and user-friendly mapping experience.</p> GraphQL Designer Enhancements<p>Added comprehensive Schema-based service generation support, allowing developers to generate GraphQL services directly from schemas. New features include GraphQL-based type suggestions, GraphQL ID support, and enhanced documentation capabilities on GraphQL fields for better API clarity.</p> Expression Editor Enhancements<p>Improved support for the expression editor with better usability and functionality, making it easier to create and manage expressions across different contexts.</p> Improved AI &amp; Copilot Capabilities<p>Enhanced AI code generation with better formatting, improved step handling, and refined system prompts for more structured responses.</p> UI &amp; UX Enhancements<p>Updated the Helper Pane UI and navigation with refined styles in the Resource form. The expression helper now supports different modes for various use cases, and the Type Editor has been improved for better type management. These changes contribute to a cleaner, more efficient development experience.</p>"},{"location":"wso2-integrator-bi-release-notes/#fixed-issues_1","title":"Fixed issues","text":"<ul> <li>WSO2 Integrator: BI Issues</li> </ul>"},{"location":"wso2-integrator-bi-release-notes/#whats-new-in-wso2-integrator-bi-130-release","title":"What's new in WSO2 Integrator: BI 1.3.0 release?","text":"<p>WSO2 Integrator: BI 1.3.0 introduces the following features and enhancements:</p> New Welcome Page<p>A completely redesigned welcome page now supports advanced project creation options, including organization name and version information.</p> Migration Tooling Support<p>Comprehensive tooling for importing Mule and Tibco projects, enabling seamless migration to WSO2 Integrator: BI integrations. This reduces migration complexity and accelerates the transition from Mule and Tibco-based solutions.</p> AI Integration<p>Advanced AI capabilities, including document generation, enhanced knowledge-base management, smarter agent creation, and improved AI suggestions. New chunking tools (Chunker, Dataloader) and reusable model providers streamline agent development and knowledge workflows. Enhanced RAG (Retrieval-Augmented Generation) and improved template management are also included.</p> New Expression Editor<p>The expression editor has been redesigned to open below the input box, providing intuitive support for creating values, using variables, calling functions, and referencing configurable values.</p> Improved Data Mapper<p>Performance improvements for large, deeply nested records, a more intuitive design, and a new expression editor simplify data transformations. The Data Mapper now supports enums/unions, constants, nested arrays, optional fields, and transformation function mappings, making complex scenarios more manageable.</p> Connector Page<p>Introduced support for importing private connectors from a user's private organization in Ballerina Central. Local Connectors are now called Custom Connectors, with a new tab-based UI and improved project switching for a more seamless and efficient workflow.</p> GraphQL Upgrades<p>Expanded GraphQL support with advanced configurations at both service and field levels, including context and metadata handling. These upgrades enable more sophisticated integrations and greater control over data flow.</p> Type Diagram Optimization<p>Optimized views for diagrams with high node counts, including node deletion and support for read-only types via TypeEditor, providing better type management.</p> Improved User Experience<p>Comprehensive UX improvements, including collapsible node palette groupings, a cleaner UI, better connector flows, and improved record rendering.</p>"},{"location":"wso2-integrator-bi-release-notes/#fixed-issues_2","title":"Fixed issues","text":"<ul> <li>WSO2 Integrator: BI Issues</li> </ul>"},{"location":"wso2-integrator-bi-release-notes/#whats-new-in-wso2-integrator-bi-120-release","title":"What's new in WSO2 Integrator: BI 1.2.0 release?","text":"Enhanced Inline Data Mapper<p>The Inline Data Mapper was redesigned for a better user experience, featuring AI-driven mapping suggestions and a new sub-mapping form for complex data transformations.</p> Data Mapper Improvements<p>Improved search, label positioning, and performance. The Data Mapper now refreshes automatically when code changes. Multiple bugs in mapping generation and type resolution were fixed, resulting in a more robust transformation experience.</p> Advanced AI Capabilities<p>Added low-code support for advanced RAG (Retrieval-Augmented Generation) workflows. Integrated Anthropic's Claude Sonnet v4 for code generation. Introduced a Vector Knowledge Base node for RAG workflows and new configuration options for default AI model providers in the Flow Diagram.</p> AI Copilot<p>Upgraded the AI Copilot to use ballerina/ai packages, streamlining flows for greater user-friendliness and agent capability. Resolved re-rendering bugs and authentication flow issues for a smoother AI experience.</p> Editor &amp; IDE Improvements<p>Added a new VSCode setting to manage Sequence Diagram visibility and an option to include the current organization in search results. Improved state management, addressed UI freezing, and enhanced project handling in multi-root workspaces for a more stable development environment.</p>"},{"location":"wso2-integrator-bi-release-notes/#fixed-issues_3","title":"Fixed issues","text":"<ul> <li>WSO2 Integrator: BI Issues</li> </ul>"},{"location":"wso2-integrator-bi-release-notes/#whats-new-in-wso2-integrator-bi-110-release","title":"What's new in WSO2 Integrator: BI 1.1.0 release?","text":"Configurable Editor Redesign<p>Complete redesign of the configuration editor with a modern UI/UX and improved functionality, making configuration management more intuitive and efficient.</p> Type Editor &amp; Diagram Upgrades<p>The type editor was revamped for better feature discoverability and user experience. Type Diagram and GraphQL designer now offer improved visual presentation and clarity.</p> Data Mapper Enhancements<p>Fixed issues when working with complex data types from imported modules. Improved visualization of array types and nested data structures for more accurate data mapping.</p> Bundled Language Server<p>The Ballerina Language Server is now bundled with the extension, eliminating separate installation requirements and significantly improving startup performance.</p> AI Copilot<p>Enhanced AI file upload support with additional file types for improved analysis. Signature Help now displays documentation for a better developer experience during code completion. Enhanced service resource creation with a comprehensive validation system.</p> HTTP Response UX Improvements<p>Introduced a new user experience for creating HTTP responses, including support for selecting status code response types and defining custom headers.</p> IDE &amp; Extension Stability<p>Refactored artifacts management and navigation. Resolved extension startup and activation issues for reliable performance. Improved state management and project handling in multi-root workspaces.</p>"},{"location":"wso2-integrator-bi-release-notes/#fixed-issues_4","title":"Fixed issues","text":"<ul> <li>WSO2 Integrator: BI Issues</li> </ul>"},{"location":"deploy/deploy-to-devant/","title":"Deploy to Devant","text":"<p>Devant is a powerful IPaaS with first-class AI support. Incorporate AI agents into the integrations you build in low-code and pro-code, and move away from siloed systems to intelligent digital experiences with Devant by WSO2\u2014the AI iPaaS that your AI Agents can call 'home'. BI provides a seamless integration experience with Devant. You can deploy your integrations to Devant with just a few clicks.</p>"},{"location":"deploy/deploy-to-devant/#step-1-login-and-create-a-project-in-devant","title":"Step 1: Login and Create a project in Devant","text":"<ul> <li> <p>Navigate to the Visualizer view by clicking on the BI icon on the sidebar.</p> </li> <li> <p>On the right hand side, under <code>Deployment Options</code> select <code>Deploy to Devant</code>.</p> </li> </ul> <p></p> <ul> <li>Login to Devant organization and create a project</li> </ul> <p> </p>"},{"location":"deploy/deploy-to-devant/#step-2-initialize-the-source-as-a-git-repository-and-push-the-changes","title":"Step 2: Initialize the source as a Git repository and push the changes","text":"<p>Go to the <code>Source Control</code> view and follow the steps to create a Git repository and commit your changes.</p> <p> </p>"},{"location":"deploy/deploy-to-devant/#step-3-create-the-integration-on-devant","title":"Step 3: Create the integration on Devant","text":"<p>Once the source is pushed to GitHub, you can configure the build details and create the integration in Devant.</p> <p></p>"},{"location":"deploy/deploy-to-devant/#step-4-view-the-integration-on-devant","title":"Step 4: View the integration on Devant","text":"<p>Once the integration is deployed to Devant, the <code>Deployment Options</code> panel displays a <code>View in Devant</code> link. This opens the Devant overview page, where you can view logs, update configurations, test the integration, promote it to higher environments, and perform many other management tasks.</p> <p></p>"},{"location":"deploy/managing-configurations/","title":"Managing Configurations","text":"<p>Configurability in WSO2 Integrator: BI allows users to modify integration behavior using external inputs without changing the source code. It is powered by Ballerina\u2019s built-in support for configurable variables, enabling runtime configuration of module-level values.</p>"},{"location":"deploy/managing-configurations/#configuring-a-single-integration-package","title":"Configuring a single integration package","text":"<p>Consider the following step-by-step guide to configure a Ballerina package that contains an HTTP service.</p>"},{"location":"deploy/managing-configurations/#step-1-create-an-http-service-using-the-default-configurations","title":"Step 1: Create an HTTP service using the default configurations","text":"<p>Start with a basic HTTP service using default configurations.</p>"},{"location":"deploy/managing-configurations/#step-2-create-required-types-and-configurable-variables","title":"Step 2: Create required types and configurable variables","text":"<ul> <li> <p>Create a type <code>Greeting</code> that holds the greeting information.</p> </li> <li> <p>Create a configurable variable to hold the greeting to be sent when invoking the API endpoint. This can be done by adding a <code>Configuration</code> in <code>WSO2 Integrator: BI</code> design view.</p> </li> </ul> <p></p>"},{"location":"deploy/managing-configurations/#step-3-run-the-integration","title":"Step 3: Run the integration","text":"<ul> <li>You'll be prompted to create a <code>Config.toml</code>. This file can contain the greeting information. This allows configuring the values externally during the runtime.</li> </ul> <p>This concept of configurables can be used to hold environment-specific variables that need to be updated at the time of execution.</p>"},{"location":"deploy/managing-configurations/#configuring-a-consolidated-package","title":"Configuring a consolidated package","text":"<p>For scenarios involving multiple packages, consolidated packages allow you to manage configurations across services in a single deployment unit. The following example shows two services: a Courses service and an Assessment service, each with their own configuration files.</p> <p>Consider the following step-by-step guide to configure a consolidated package that contains two packages. </p>"},{"location":"deploy/managing-configurations/#initial-service-configurations","title":"Initial service configurations","text":"<p>Courses service (<code>Config.toml</code>)</p> <pre><code>[sampleorg.courses]\napp_port = 8081\n</code></pre> <p>Assessment service (<code>Config.toml</code>)</p> <pre><code>[sampleorg.assessments]\napp_port = 8082\n</code></pre> <p>These packages are published to the local repository for this example.</p>"},{"location":"deploy/managing-configurations/#step-1-pack-and-publish-artifacts","title":"Step 1: Pack and publish artifacts","text":"<p>Use the following commands to prepare packages for consolidation.</p> <pre><code>bal pack\nbal push --repository local\n</code></pre>"},{"location":"deploy/managing-configurations/#step-2-create-a-consolidated-package","title":"Step 2: Create a consolidated package","text":"<p>Consolidate multiple packages into a single deployment unit.</p> <pre><code>bal consolidate-packages new --package-path lms sampleorg/assessments:0.1.0,sampleorg/courses:0.1.0 --repository=local\n</code></pre> <p>This creates a new consolidated package named <code>lms</code> containing both services.</p>"},{"location":"deploy/managing-configurations/#step-3-configure-the-consolidated-package","title":"Step 3: Configure the consolidated package","text":"<p>You can provide configuration values through either configuration files or CLI arguments.</p>"},{"location":"deploy/managing-configurations/#via-configuration-file","title":"Via configuration file","text":"<p>Create a <code>Config.toml</code> file using the following format to add configuration values.</p> <pre><code>[org-name.module-name]\nvariable-name = \"value\"\n</code></pre> <p>Example configuration for both services:</p> <pre><code>[sampleorg.assessments]\napp_port = 9091\n\n[sampleorg.courses]\napp_port = 9092\n</code></pre> Note<p>The configuration file is not required to reside within the package directory and can be split across multiple files. Specify file paths using the <code>BAL_CONFIG_FILES</code> environment variable.</p> <p>For Windows: <pre><code>set BAL_CONFIG_FILES=&lt;path-to-config1.toml&gt;;&lt;path-to-config2.toml&gt;\n</code></pre></p> <p>For Linux/macOS: <pre><code>export BAL_CONFIG_FILES=&lt;path-to-config1.toml&gt;;&lt;path-to-config2.toml&gt;\n</code></pre></p>"},{"location":"deploy/managing-configurations/#via-cli-arguments","title":"Via CLI arguments","text":"<p>Pass configuration values directly when running the consolidated package using the <code>-C</code> flag.</p> <p>For example: Running the consolidated package with configuration</p> <pre><code>bal run lms -- -Csampleorg.courses.app_port=9092 -Csampleorg.assessments.app_port=9091\n</code></pre> <p>For detailed configuration options, refer to Provide values to configurable variables in the Ballerina documentation.</p>"},{"location":"deploy/overview/","title":"Overview","text":""},{"location":"deploy/overview/#deployment-options","title":"Deployment Options","text":"<p>WSO2 Integrator: BI supports flexible deployment models that can be grouped into two main categories:</p> <ol> <li> <p>Environment-Based Deployment</p> <ul> <li> <p>Local Deployment:  Ideal for development and testing, this mode allows you to run integrations directly on your local machine using the built-in runtime. It offers quick feedback loops, easier debugging, and is often used in early stages of integration development.</p> </li> <li> <p>Cloud Deployment:  Designed for scalable, production-grade environments, this option allows BI built integrations to be deployed in private or public cloud infrastructures. It integrates seamlessly with cloud-native tools for monitoring, auto-scaling, load balancing, and resilience.</p> </li> </ul> </li> <li> <p>Infrastructure-Based Deployment</p> <ul> <li> <p>VM-Based Deployment:  Suited for on-premises or tightly controlled environments, BI built integrations can be deployed on virtual machines using traditional infrastructure provisioning. This model provides full control over the runtime environment but may require more manual effort in scaling and management.</p> </li> <li> <p>Containerized Deployment:  Best for modern, automated environments, BI built integrations run in Docker containers or on Kubernetes clusters. This mode enables improved portability, orchestration, and tight integration with CI/CD pipelines for continuous delivery and infrastructure automation.</p> </li> </ul> </li> </ol> Note<p>Use local and VM-based deployments for early-stage development, PoCs, or controlled environments. Move to containerized or cloud deployments for scalability, high availability, and production readiness. Each option can be adapted to meet your performance, availability, and operational needs.</p>"},{"location":"deploy/overview/#deployment-patterns","title":"Deployment Patterns","text":"<p>To address different architectural and operational requirements, WSO2 Integrator: BI supports both centralized and decentralized deployment patterns:</p> <ul> <li> <p>Centralized Deployment: Consolidates multiple BI artifacts into a single deployable unit. This pattern simplifies deployment, reduces resource consumption, and is ideal for tightly coupled integration solutions.</p> </li> <li> <p>Decentralized Deployment: Each BI component is packaged and deployed independently. This allows teams to iterate and release components separately, improving agility and scalability in microservice-oriented environments.</p> </li> </ul> <p>You can choose a pattern based on your team's workflows, size of the integration solution, and deployment control requirements.</p>"},{"location":"deploy/overview/#hot-deployment-strategies","title":"Hot Deployment Strategies","text":"<p>Hot deployments refer to the process of updating or redeploying software components with zero downtime and maintaining high availability in production systems.</p> <p>Here the hot deployment strategy works by orchestrating multiple service instances through a NGINX load balancer, allowing you to update and restart services without interrupting user traffic. The load balancer automatically routes requests away from instances undergoing updates and back to them once they are healthy again.</p>"},{"location":"deploy/overview/#common-load-balancing-strategies","title":"Common Load Balancing Strategies:","text":""},{"location":"deploy/overview/#1-active-active","title":"1. Active-Active","text":"<p>All instances actively serve traffic simultaneously. NGINX uses passive health monitoring through <code>max_fails</code> and <code>fail_timeout</code> directives. When an instance fails to respond successfully <code>max_fails</code> times within the <code>fail_timeout</code> window, NGINX temporarily removes it from the load balancing pool.</p> <p>This passive approach relies on actual client requests to detect server failures, meaning the load balancer only discovers problems when real traffic encounters them. Passive monitoring is reactive and depends on the natural flow of requests to identify unhealthy servers. The default load balancing method is round-robin, distributing requests evenly across all available servers, though this can be changed to other algorithms like least connections or IP hash based on application requirements.</p> <p>Failed requests are automatically retried on other available instances, as a fault tolerance mechanism.</p>"},{"location":"deploy/overview/#nginx-configuration","title":"NGINX configuration","text":"<pre><code>events {}\n\nhttp {\nupstream backend {\nserver 127.0.0.1 max_fails=3 fail_timeout=30s;\nserver 127.0.0.2 max_fails=3 fail_timeout=30s;\n}\n\nserver {\nlocation / {\nproxy_pass http://backend;\n}\n}\n}\n</code></pre>"},{"location":"deploy/overview/#2-active-active-with-health-checks","title":"2. Active-Active (With health checks)","text":"<p>This configuration requires NGINX Plus, which supports active health checks. NGINX proactively polls a specified health endpoint (e.g., /health) on each instance to determine availability.</p> <p>Unlike passive health checks that only detect failures when client requests fail, active health checks continuously monitor server health in the background, providing faster failure detection and more reliable service availability. This proactive approach allows NGINX to remove unhealthy servers from the pool before they impact user requests, significantly reducing the mean time to detection and improving overall system reliability.</p>"},{"location":"deploy/overview/#nginx-configuration_1","title":"NGINX configuration","text":"<pre><code>events {}\n\nhttp {\nupstream backend {\nserver 127.0.0.1 max_fails=3 fail_timeout=30s;\nserver 127.0.0.2 max_fails=3 fail_timeout=30s;\n}\n\nserver {\nlisten 80;\nlocation / {\nproxy_pass http://backend;\nhealth_check uri=/health interval=5s;\n}\n}\n}\n</code></pre>"},{"location":"deploy/overview/#3-active-passive","title":"3. Active-Passive","text":"<p>Primary server handles all traffic, backup only activates on failure. The backup server remains idle until the primary fails, ensuring you always have a failover target.</p> <p>When the primary server fails to send a response, the load balancer immediately redirects the request to backup server. This failover process is automatic and transparent to the client, occurring within milliseconds of detecting the failure. The backup server must be pre-configured with identical application code and dependencies.</p> <p>Nginx tracks failed requests against <code>max_fails</code> threshold and after reaching threshold, server is marked as unavailable for <code>fail_timeout</code> duration. And then keep sending request to one of the backup servers. Once a server is marked as unavailable, Nginx will not attempt to send requests to it until the <code>fail_timeout</code> period expires, ensuring that the backup server handles all incoming traffic consistently. If multiple backup servers are configured, Nginx will select the first available backup server in the order they are defined, maintaining the single-active-server principle of active-passive architecture.</p> <p>After <code>fail_timeout</code> period, Nginx attempts to route traffic back to primary server. If successful, primary server resumes active role and backup servers return to standby mode. This recovery process is gradual and intelligent - Nginx sends a small number of test requests to the recovered primary server before fully transitioning traffic back. If the primary server successfully handles these test requests without errors, it regains its active status and the backup server automatically transitions back to standby mode. However, if the primary server continues to fail during the recovery attempt, it remains marked as unavailable for another <code>fail_timeout</code> period, and the backup server continues to handle all traffic until the next recovery cycle.</p>"},{"location":"deploy/overview/#nginx-configuration_2","title":"NGINX configuration","text":"<pre><code>events {}\n\nhttp {\nupstream backend {\nserver 127.0.0.1 max_fails=3 fail_timeout=30s;\nserver 127.0.0.2 max_fails=3 fail_timeout=30s;\n}\n\nserver {\nlisten 80;\n\nlocation / {\nproxy_pass http://backend;\n}\n}\n}\n</code></pre> Best Practice<p>Ensure identical configurations across all instances and automate deployments for consistency.</p> <p>You can visit the following sections to get an understanding on the possible deployment and configuration options.</p> <ul> <li>Deploy to Devant</li> <li>Containerized Deployment</li> <li>VM-based Deployment</li> <li>Managing Configurations</li> </ul>"},{"location":"deploy/capacity-planning/overview/","title":"Capacity Planning for WSO2 Integrator: BI","text":"<p>This section provides an overview of the performance characteristics and capacity planning considerations for WSO2 Integrator: BI when deployed in a cloud environment. The performance reports linked below detail the results of various integration scenarios under specific load conditions.</p> <p>The tests were conducted on the following AWS EC2 instance configuration:</p> Description Value EC2 Instance type c5.large CPUs 2 Thread(s) per core 2 Core(s) per socket 1 Socket(s) 1 Processor model name Intel(R) Xeon(R) Platinum 8124M CPU @ 3.00GHz System Memory 4 GiB Storage 8G nvme0n1 Operating System Ubuntu 18.04.6 LTS Kernel version 5.4.0-1094-aws"},{"location":"deploy/capacity-planning/overview/#performance-reports","title":"Performance reports","text":"<p>Explore the detailed performance reports for various integration types:</p> <ul> <li>Performance report for REST Services (HTTP and HTTPS) </li> </ul>"},{"location":"deploy/capacity-planning/performance-report-for-rest-services/","title":"Performance report for REST services (HTTP and HTTPS)","text":""},{"location":"deploy/capacity-planning/performance-report-for-rest-services/#overview","title":"Overview","text":"Test Scenarios Description Passthrough HTTP service (h1c -&gt; h1c) An HTTP Service, which forwards all requests to an HTTP back-end service. Passthrough HTTPS service (h1 -&gt; h1) An HTTPS Service, which forwards all requests to an HTTPS back-end service. JSON to XML transformation HTTP service An HTTP Service, which transforms JSON requests to XML and then forwards all requests to an HTTP back-end service. JSON to XML transformation HTTPS service An HTTPS Service, which transforms JSON requests to XML and then forwards all requests to an HTTPS back-end service. HTTP/2 client and server downgrade service (h2 -&gt; h2) An HTTP/2(with TLS) server accepts requests from an HTTP/1.1(with TLS) client and the HTTP/2(with TLS) client sends requests to an HTTP/1.1(with TLS) back-end service. Both the upstream and downstream connections are downgraded to HTTP/1.1(with TLS). <p>Our test client is Apache JMeter. We test each scenario for a fixed duration of time. We split the test results into warm-up and measurement parts and use the measurement part to compute the performance metrics.</p> <p>A majority of test scenarios use a Netty based back-end service, which echoes back any request posted to it after a specified period of time.</p> <p>Note</p> <p>The code and instructions for running these tests are provided here.</p>"},{"location":"deploy/capacity-planning/performance-report-for-rest-services/#performance-metrics","title":"Performance metrics","text":"<p>We run the performance tests under different numbers of concurrent users, message sizes (payloads), and back-end service delays.</p> <p>The main performance metrics:</p> <ol> <li>Throughput: The number of requests that the Ballerina service processes during a specific time interval (e.g., per second).</li> <li>Response Time: The end-to-end latency for an operation of invoking a Ballerina service. The complete distribution of response times was recorded.</li> </ol> <p>In addition to the above metrics, we measure the load average and several memory-related metrics.</p>"},{"location":"deploy/capacity-planning/performance-report-for-rest-services/#test-parameters","title":"Test parameters","text":"<p>The following are the test parameters.</p> Test Parameter Description Values Scenario Name The name of the test scenario. Refer to the above table. Heap Size The amount of memory allocated to the application 2G Concurrent Users The number of users accessing the application at the same time. 100, 200, 500, 1000 Message Size (Bytes) The request payload size in Bytes. 500, 1000, 10000 Back-end Delay (ms) The delay added by the back-end service. 0 <p>The duration of each test is 1200 seconds. The warm-up period is 600 seconds. The measurement results are collected after the warm-up period.</p> <p>A c5.large Amazon EC2 instance was used to install Ballerina.</p> <p>The following are the measurements collected from each performance test conducted for a given combination of test parameters.</p> Measurement Description Error % Percentage of requests with errors Average Response Time (ms) The average response time of a set of results Standard Deviation of Response Time (ms) The \u201cStandard Deviation\u201d of the response time. 99th Percentile of Response Time (ms) 99% of the requests took no more than this time. The remaining samples took at least as long as this Throughput (Requests/sec) The throughput measured in requests per second. Average Memory Footprint After Full GC (M) The average memory consumed by the application after a full garbage collection event."},{"location":"deploy/capacity-planning/performance-report-for-rest-services/#test-results","title":"Test results","text":"<p>The following is the summary of performance test results collected for the measurement period.</p> Scenario Name Concurrent Users Message Size (Bytes) Back-end Service Delay (ms) Error % Throughput (Requests/sec) Average Response Time (ms) Standard Deviation of Response Time (ms) 99th Percentile of Response Time (ms) Passthrough HTTP service (h1c -&gt; h1c) 100 500 0 0 8935.69 11.13 4.78 26 Passthrough HTTP service (h1c -&gt; h1c) 100 1000 0 0 9341.58 10.65 4.97 26 Passthrough HTTP service (h1c -&gt; h1c) 100 10000 0 0 6358.34 15.66 6.52 36 Passthrough HTTP service (h1c -&gt; h1c) 200 500 0 0 10719.1 18.59 6.93 37 Passthrough HTTP service (h1c -&gt; h1c) 200 1000 0 0 9108.9 21.89 7.25 43 Passthrough HTTP service (h1c -&gt; h1c) 200 10000 0 0.01 6348.51 31.42 134.77 78 Passthrough HTTP service (h1c -&gt; h1c) 500 500 0 0 8706.5 57.35 15.55 96 Passthrough HTTP service (h1c -&gt; h1c) 500 1000 0 0 8946.24 55.81 14.4 92 Passthrough HTTP service (h1c -&gt; h1c) 500 10000 0 0.01 6393.06 78.11 25.5 145 Passthrough HTTP service (h1c -&gt; h1c) 1000 500 0 0 8665.7 115.29 24.02 172 Passthrough HTTP service (h1c -&gt; h1c) 1000 1000 0 0 8944.47 111.69 19.91 165 Passthrough HTTP service (h1c -&gt; h1c) 1000 10000 0 0.01 6296.27 158.68 41.14 263 JSON to XML transformation HTTP service 100 500 0 0 1590.82 62.8 252.15 138 JSON to XML transformation HTTP service 100 1000 0 0 998.79 100.05 379 188 JSON to XML transformation HTTP service 100 10000 0 0 84.2 1186.62 521.64 1903 JSON to XML transformation HTTP service 200 500 0 0 1498.83 133.35 180.62 295 JSON to XML transformation HTTP service 200 1000 0 0.02 1001.07 199.77 693.66 397 JSON to XML transformation HTTP service 200 10000 0 0 70.63 2823.67 1185.64 4479 JSON to XML transformation HTTP service 500 500 0 0 1478.51 338.23 237.25 747 JSON to XML transformation HTTP service 500 1000 0 0 905.56 552.17 340.06 1175 JSON to XML transformation HTTP service 500 10000 0 99.16 15.89 29911.92 1485.55 31103 JSON to XML transformation HTTP service 1000 500 0 0 1445.42 691.67 373.82 1551 JSON to XML transformation HTTP service 1000 1000 0 0 885.2 1128.63 1467.77 2239 JSON to XML transformation HTTP service 1000 10000 0 0 59.63 16479.55 5309.9 24959 Passthrough HTTPS service (h1 -&gt; h1) 100 500 0 0 8282.45 12.02 5.67 28 Passthrough HTTPS service (h1 -&gt; h1) 100 1000 0 0 7676.52 12.97 6.39 31 Passthrough HTTPS service (h1 -&gt; h1) 100 10000 0 0 4405.6 22.63 38.69 70 Passthrough HTTPS service (h1 -&gt; h1) 200 500 0 0 8298.14 24.04 9.49 53 Passthrough HTTPS service (h1 -&gt; h1) 200 1000 0 0 8185.02 24.37 9.6 53 Passthrough HTTPS service (h1 -&gt; h1) 200 10000 0 0.01 4314.93 46.26 12.9 97 Passthrough HTTPS service (h1 -&gt; h1) 500 500 0 0 7631.44 65.43 18.17 111 Passthrough HTTPS service (h1 -&gt; h1) 500 1000 0 0 7322.7 68.2 18.11 114 Passthrough HTTPS service (h1 -&gt; h1) 500 10000 0 0.01 4187 119.3 29.12 226 Passthrough HTTPS service (h1 -&gt; h1) 1000 500 0 0 7257.55 137.64 30.77 207 Passthrough HTTPS service (h1 -&gt; h1) 1000 1000 0 0 7213.92 138.47 30.72 209 Passthrough HTTPS service (h1 -&gt; h1) 1000 10000 0 0.01 4076.18 245.27 59.83 439 JSON to XML transformation HTTPS service 100 500 0 0 1507.24 66.28 337.13 158 JSON to XML transformation HTTPS service 100 1000 0 0 967.15 103.32 311.01 206 JSON to XML transformation HTTPS service 100 10000 0 2 83.65 1181 4156.47 30079 JSON to XML transformation HTTPS service 200 500 0 0 1380.33 144.81 192.98 349 JSON to XML transformation HTTPS service 200 1000 0 0.04 897.78 222.73 1027.82 501 JSON to XML transformation HTTPS service 200 10000 0 2.27 76.57 2602.91 4886.23 30079 JSON to XML transformation HTTPS service 500 500 0 0 1505.97 332.02 446.28 1663 JSON to XML transformation HTTPS service 500 1000 0 0 881.78 566.83 1394.24 1159 JSON to XML transformation HTTPS service 500 10000 0 100 21.33 22693.11 11309.03 42495 JSON to XML transformation HTTPS service 1000 500 0 0 1424.83 701.47 591.2 3487 JSON to XML transformation HTTPS service 1000 1000 0 0 806.11 1239.02 1167.47 3215 JSON to XML transformation HTTPS service 1000 10000 0 0 61.55 15956.62 4774.51 24319 HTTP/2 client and server downgrade service (h2 -&gt; h2) 100 500 0 0 8289.32 12.01 5.58 28 HTTP/2 client and server downgrade service (h2 -&gt; h2) 100 1000 0 0 7675.18 12.97 6.07 30 HTTP/2 client and server downgrade service (h2 -&gt; h2) 100 10000 0 0 4327.11 23.03 6.94 49 HTTP/2 client and server downgrade service (h2 -&gt; h2) 200 500 0 0 7626.89 26.16 8.86 52 HTTP/2 client and server downgrade service (h2 -&gt; h2) 200 1000 0 0 8012.4 24.89 9.59 54 HTTP/2 client and server downgrade service (h2 -&gt; h2) 200 10000 0 0.04 4206.28 47.46 13.18 100 HTTP/2 client and server downgrade service (h2 -&gt; h2) 500 500 0 0 7562.06 66.03 18.66 113 HTTP/2 client and server downgrade service (h2 -&gt; h2) 500 1000 0 0 7174.09 69.61 17.7 114 HTTP/2 client and server downgrade service (h2 -&gt; h2) 500 10000 0 0 4113.17 121.44 33.72 253 HTTP/2 client and server downgrade service (h2 -&gt; h2) 1000 500 0 0 7462.01 133.86 36.73 218 HTTP/2 client and server downgrade service (h2 -&gt; h2) 1000 1000 0 0 7202.02 138.7 35.71 221 HTTP/2 client and server downgrade service (h2 -&gt; h2) 1000 10000 0 0.01 4017.91 248.82 61.31 447 <p>Note</p> <p>Ballerina Swan Lake 2201.12.4 was used for testing with default configurations. Your results may vary depending on the Ballerina version and configuration used.</p>"},{"location":"deploy/containerized-deployment/deploy-as-docker-image/","title":"Deploy as a Docker Image","text":"<p>This guide explains how to deploy an integration as a Docker image.</p> <ul> <li>Navigate to the Visualizer view by clicking on the BI icon on the sidebar.</li> <li>Click on the Deploy with Docker under the Deployment Options section in the right panel.</li> <li> <p>Click Create Docker Image button.      </p> </li> <li> <p>The integration will be built as a Docker image and the image will be available in the local Docker registry.</p> </li> </ul> <p>Note: You can see the generated Dockerfile in <code>/target/docker/&lt;integration-name&gt;</code>.</p>"},{"location":"deploy/containerized-deployment/deploy-as-docker-image/#execute-the-docker-image","title":"Execute the Docker image","text":"<p>Follow the steps below to execute the Docker image.</p> <ol> <li>Execute the docker images command to verify if the Docker image is generated.</li> </ol> <pre><code>docker images\n</code></pre> <p>The output will be similar to the following:</p> <pre><code>REPOSITORY                                          TAG                    IMAGE ID             CREATED                SIZE\nfake_store_manager                          latest                 e971e4336f71   58 minutes ago   237MB\n</code></pre> <ol> <li> <p>Execute the <code>docker run -d -v &lt;path/to/config&gt;/Config.toml:/home/ballerina/Config.toml -p 8090:8090 fake_store_manager:latest</code> command to run the generated Docker image.</p> </li> <li> <p>Call the <code>http://localhost:8090/store/products</code> API via try-it and see the output.</p> </li> </ol> <p></p>"},{"location":"deploy/containerized-deployment/deploy-on-kubernetes/","title":"Deploy on Kubernetes","text":"<p>This guide explains how to deploy an integration in a Kubernetes cluster.</p>"},{"location":"deploy/containerized-deployment/deploy-on-kubernetes/#step-1-enable-kubernetes-artifact-build","title":"Step 1: Enable Kubernetes artifact build","text":"<pre><code>- Navigate to the Visualizer view by clicking on the BI icon on the sidebar.\n- Go to the `Explorer` view and add the following to `Ballerina.toml` to enable building artifacts for Kubernetes.\n\n```toml\n[build-options]\ncloud = \"k8s\"\n```\n\n- Specify the container image details by creating a `Cloud.toml` file.\n\n```toml\n[container.image]\nrepository=\"wso2inc\" # Docker hub repository name.\nname=\"greeter\" # container name\ntag=\"latest\"\n```\n\n&lt;a href=\"https://wso2.github.io/docs-bi/assets/img/deploy/update-k8s-cnfigs.gif\"&gt;&lt;img src=\"https://wso2.github.io/docs-bi/assets/img/deploy/update-k8s-cnfigs.gif\" alt=\"Update k8s build configurations\" width=\"70%\"&gt;&lt;/a&gt;</code></pre>"},{"location":"deploy/containerized-deployment/deploy-on-kubernetes/#step-2-build-the-artifacts","title":"Step 2: Build the artifacts","text":"<pre><code>Go to the terminal in VSCode and build the executable using `bal build`. You'll get an output as follows.\n\n```\n    Compiling source\n            example/greeter:0.1.0\n\n    Generating executable\n\n    Generating artifacts\n\n            @kubernetes:Service\n            @kubernetes:ConfigMap\n            @kubernetes:Secret\n            @kubernetes:Deployment\n            @kubernetes:HPA\n\n    Building the docker image\n\n    Execute the below command to deploy the Kubernetes artifacts: \n            kubectl apply -f /home/example/greeter/target/kubernetes/greeter\n```\n\n&lt;a href=\"https://wso2.github.io/docs-bi/assets/img/deploy/build-k8s-artifacts.gif\"&gt;&lt;img src=\"https://wso2.github.io/docs-bi/assets/img/deploy/build-k8s-artifacts.gif\" alt=\"Build k8s artifacts\" width=\"70%\"&gt;&lt;/a&gt;\n\n???+ Info\n    This generates the cloud artifacts inside the `target/` directory.</code></pre>"},{"location":"deploy/containerized-deployment/deploy-on-kubernetes/#step-3-push-the-docker-image","title":"Step 3:  Push the Docker image","text":"<pre><code>Execute the command below to push the created Docker image into Docker Hub for the cluster to get access to the previously built container.\n```bash\ndocker push wso2inc/greeter:latest\n```\n\n???+ Note\n    Replace `wso2inc` with your repository name.\n\nYou view the output below.\n\n```\nThe push refers to repository [docker.io/wso2inc/greeter]\nlatest: digest: sha256:c1acf5165848d70c347a970d6b5c32f63669cdbb0d4c1daca2c91cfbe32f61b2 size: 13718\n```</code></pre>"},{"location":"deploy/containerized-deployment/deploy-on-kubernetes/#step-4-deploy-on-kubernetes","title":"Step 4:  Deploy on Kubernetes","text":"<pre><code>Execute the command below to deploy the application into the Kubernetes cluster.\n\n```bash\nkubectl apply -f /home/example/greeter/target/kubernetes/greeter\n```\nYou view the output below.\n\n```ballerina\nservice/greeter-svc created\ndeployment.apps/greeter-deployment created\nhorizontalpodautoscaler.autoscaling/greeter-hpa created\n```</code></pre>"},{"location":"deploy/containerized-deployment/overview/","title":"Introduction to Containerized Deployment","text":"<p>Integrations developed with BI can be deployed using modern containerization technologies, such as Docker and Kubernetes, enabling consistent, scalable, and portable application deployments across environments.</p> <p>Containerized deployment simplifies the process of packaging your integration artifacts along with their dependencies into a lightweight, self-contained image. This image can be run on any platform that supports containers, eliminating environmental inconsistencies and easing the transition from development to production.</p> <p>There are two primary approaches to containerized deployment:</p> <ul> <li> <p>Docker Deployment:   Ideal for local development, testing, or simple production environments. The integration runtime is encapsulated in a Docker image, which can be built, run, and managed using standard Docker tools.</p> </li> <li> <p>Kubernetes Deployment:   Suitable for orchestrating containerized applications at scale. Kubernetes provides advanced capabilities like auto-scaling, service discovery, rolling updates, and self-healing, making it the preferred choice for cloud-native and enterprise-grade deployments.</p> </li> </ul> <p>Containerized deployment ensures:</p> <ul> <li>Environment consistency across development, testing, and production.</li> <li>Faster deployment cycles and simplified updates.</li> <li>Better resource utilization and scalability.</li> <li>Easier integration with CI/CD pipelines and cloud infrastructure.</li> </ul> <p>In the following sections, you will learn how to package and deploy your BI projects using Docker and Kubernetes, along with best practices and configuration options.</p> <ul> <li>Deploy as Docker Image</li> <li>Deploy on Kubernetes</li> </ul>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/","title":"Configuration Best Practices","text":"<p>This guide covers configuration management strategies across deployment models (VM-based, containerized, Kubernetes) and CI/CD pipelines. Core principle: configuration is injected at runtime, never embedded in artifacts.</p>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#vm-based-deployments","title":"VM-based deployments","text":""},{"location":"deploy/deployment-best-practices/configuration-best-practices/#centralized-approach","title":"Centralized approach","text":"<p>All services share a single configuration management point.</p>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#structure","title":"Structure","text":"<ul> <li>Base configuration: <code>/opt/integrations/shared/Config.toml</code></li> <li>Environment overrides: <code>/opt/integrations/environments/{env}/Config.{env}.toml</code></li> <li>Endpoints: <code>/opt/integrations/environments/{env}/endpoints.{env}.toml</code></li> <li>Secrets: <code>/opt/integrations/environments/{env}/secrets.{env}.toml</code></li> </ul>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#loading","title":"Loading","text":"<p>Create a loader script that combines files in precedence order: secrets \u2192 environment-specific \u2192 base.</p>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#management","title":"Management","text":"<p>All services use the same configuration loader mechanism. Update configurations in one place.</p>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#decentralized-approach","title":"Decentralized approach","text":"<p>Each service maintains its own configuration independently.</p>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#structure_1","title":"Structure","text":"<ul> <li>Per service: <code>/opt/integrations/{service-name}/conf/base.toml</code>, <code>production.toml</code>, <code>secrets.production.toml</code></li> </ul>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#loading_1","title":"Loading","text":"<p>Each service's systemd file references its own configuration via <code>BAL_CONFIG_FILES</code>.</p>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#management_1","title":"Management","text":"<p>Services are deployed and configured independently. Good for teams owning individual services.</p>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#containerized-deployments","title":"Containerized deployments","text":"Warning<p>Do NOT include <code>Config.toml</code> in the container image.</p>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#configuration-injection-methods","title":"Configuration injection methods","text":"<ol> <li>Volume mounts: <code>-v /path/to/Config.toml:/home/ballerina/conf/Config.toml</code></li> <li>Environment variables: <code>-e BAL_CONFIG_VAR_PORT=9090</code></li> <li>Docker Compose: Reference external config files via <code>.env</code> files</li> </ol>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#docker-compose-pattern","title":"Docker Compose pattern","text":"<p>Use environment files (<code>.env.development</code>, <code>.env.production</code>) to define environment-specific values. Mount configuration files as read-only volumes. Each environment has a separate config directory.</p>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#docker-swarm","title":"Docker Swarm","text":"<p>Use Docker Secrets for sensitive data, Docker Configs for non-sensitive configuration. Reference them in service definitions.</p>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#kubernetes-deployments","title":"Kubernetes deployments","text":""},{"location":"deploy/deployment-best-practices/configuration-best-practices/#configuration-resources","title":"Configuration resources","text":""},{"location":"deploy/deployment-best-practices/configuration-best-practices/#configmap","title":"ConfigMap","text":"<p>Non-sensitive configuration</p> <ul> <li>Base application config</li> <li>Endpoint definitions  </li> <li>Feature flags</li> </ul>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#secrets","title":"Secrets","text":"<p>Sensitive data</p> <ul> <li>Database passwords</li> <li>API keys</li> <li>JWT secrets</li> </ul>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#configuration-delivery","title":"Configuration delivery","text":"<ol> <li>ConfigMap volumes mount as read-only files at <code>/etc/config/</code></li> <li>Secret volumes mount with restricted permissions at <code>/etc/secrets/</code></li> <li>Individual values are injected as environment variables</li> </ol>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#environment-management-with-kustomize","title":"Environment management with kustomize","text":"<p>Use kustomize overlays for environment progression:</p> <ul> <li><code>base/</code>: Common manifests</li> <li><code>overlays/development/</code>: Dev-specific ConfigMaps and patches</li> <li><code>overlays/staging/</code>: Staging overrides</li> <li><code>overlays/production/</code>: Production overrides</li> </ul> <p>Deploy with: <code>kubectl apply -k overlays/production/</code></p>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#cicd-configuration-handling","title":"CI/CD configuration handling","text":""},{"location":"deploy/deployment-best-practices/configuration-best-practices/#build-process","title":"Build process","text":"<p>Build WITHOUT the configuration files. Create deployment artifacts independent of environment.</p>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#deployment-process","title":"Deployment process","text":"<p>Each stage applies environment-specific configuration before updating the application:</p> <ol> <li>Dev deployment: Apply dev ConfigMaps/Secrets \u2192 Update image</li> <li>Staging deployment: Apply staging ConfigMaps/Secrets \u2192 Update image</li> <li>Production deployment: Apply production ConfigMaps/Secrets \u2192 Update image (with manual approval)</li> </ol>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#configuration-storage","title":"Configuration storage","text":"<ul> <li>Keep config files in repository (<code>config/</code> or <code>k8s/</code> directories)</li> <li>Store secrets in platform secret storage (never in repo)</li> <li>Each environment has separate secrets</li> </ul>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#pattern-across-platforms","title":"Pattern across platforms","text":"<p>GitHub Actions, GitLab CI, and Jenkins follow the same: build once, configure per environment, deploy many times.</p>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#configuration-promotion","title":"Configuration promotion","text":"<p>Configuration flows through environments: development \u2192 staging \u2192 production.</p>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#promotion-workflow","title":"Promotion workflow","text":"<ol> <li>Validate source environment configuration</li> <li>Backup existing target configuration</li> <li>Copy with environment-specific transformations</li> <li>Validate transformed configuration</li> <li>Apply to the target environment</li> <li>Audit log all changes</li> </ol>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#transformations","title":"Transformations","text":""},{"location":"deploy/deployment-best-practices/configuration-best-practices/#dev-to-staging","title":"Dev to staging","text":"<ul> <li>Change: <code>dev.example.com</code> \u2192 <code>staging.example.com</code></li> <li>Keep: Authentication details unchanged</li> <li>Adjust: Resource limits if needed</li> </ul>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#staging-to-production","title":"Staging to production","text":"<ul> <li>Change: <code>staging.example.com</code> \u2192 <code>prod.example.com</code></li> <li>Increase: Log level requirements</li> <li>Enable: Additional monitoring/security</li> </ul>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#automation-tools","title":"Automation tools","text":"<p>Automate promotion scripts that:</p> <ul> <li>Validate before proceeding</li> <li>Log all changes</li> <li>Enable rollback</li> <li>Handle transformation rules</li> </ul>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#endpoint-promotion","title":"Endpoint promotion","text":"<p>External and internal service endpoints change across environments.</p>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#endpoint-configuration-structure","title":"Endpoint configuration structure","text":"<pre><code>[external_services.development]\nauth_service = \"https://auth-dev.example.com\"\n\n[external_services.staging]\nauth_service = \"https://auth-staging.example.com\"\n\n[external_services.production]\nauth_service = \"https://auth.example.com\"\n</code></pre>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#endpoint-resolution","title":"Endpoint resolution","text":"<p>During application startup:</p> <ul> <li>Read <code>endpoints.toml</code></li> <li>Select the correct endpoint block based on the <code>ENVIRONMENT</code> variable</li> <li>Use the selected endpoint for all outbound connections</li> </ul>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#endpoint-promotion_1","title":"Endpoint promotion","text":"<p>Promote independently from the general configuration:</p> <ol> <li>Extract source environment endpoint block</li> <li>Transform domain/URLs to target pattern</li> <li>Apply to the target environment</li> <li>Validate connectivity to new endpoints</li> <li>Document changes in the audit log</li> </ol>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#common-best-practices","title":"Common best practices","text":""},{"location":"deploy/deployment-best-practices/configuration-best-practices/#separation","title":"Separation","text":"<p>Configuration should be separate from code and artifacts.</p>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#precedence-management","title":"Precedence management","text":"<p>Leverage configuration precedence order correctly:</p> <ol> <li>Environment variables (highest priority)</li> <li>Command-line arguments</li> <li>TOML files</li> <li>Embedded defaults (lowest priority)</li> </ol>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#secrets-security","title":"Secrets security","text":"<ul> <li>Never commit secrets to version control</li> <li>Use platform secret management (Vault, K8s Secrets, CI/CD secrets)</li> <li>Rotate secrets regularly</li> <li>Audit all secret access</li> </ul>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#validation","title":"Validation","text":"<p>Validate the configuration at startup.</p> <ul> <li>Check required values present</li> <li>Validate value ranges and formats</li> <li>Fail fast on invalid configuration</li> </ul>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#documentation","title":"Documentation","text":"<p>Maintain the endpoint and the configuration documentation.</p> <ul> <li>Example configuration files (Config.toml.example)</li> <li>Endpoint registry showing all external/internal endpoints</li> <li>Document transformation rules for promotions</li> </ul>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#auditability","title":"Auditability","text":"<p>Log all configuration changes.</p> <ul> <li>Who changed what</li> <li>When changes occurred</li> <li>Source and target environments</li> <li>Rollback actions</li> </ul>"},{"location":"deploy/deployment-best-practices/configuration-best-practices/#references","title":"References","text":"<ul> <li>Managing Configurations</li> <li>Kubernetes ConfigMaps &amp; Secrets</li> <li>Docker Compose Documentation</li> <li>Kustomize</li> <li>HashiCorp Vault</li> </ul>"},{"location":"deploy/deployment-best-practices/network-level-security/","title":"Network Level Security","text":""},{"location":"deploy/deployment-best-practices/network-level-security/#establish-a-failover-setup","title":"Establish a failover setup","text":"<p>Implement high availability (HA) and failover configurations to ensure continuous system operation and minimize downtime.</p> <ul> <li> <p>Cloud-native deployments: Achieve high availability through the container orchestration platform (e.g., Kubernetes).</p> </li> <li> <p>VM-based deployments: Deploy a minimum of two nodes configured for active-active or active-passive failover to maintain service continuity. For critical production environments with strict availability SLAs, consider three or more nodes.</p> </li> </ul> <p>Continuously monitor the health and performance of all nodes within the cluster. Track key metrics such as resource utilization, response time anomalies, and the volume of incoming network connections. Effective monitoring helps you determine when to add failover instances or adjust network routing to prevent service disruptions.</p>"},{"location":"deploy/deployment-best-practices/network-level-security/#maintain-network-level-logging","title":"Maintain network-level logging","text":"<p>Enable and retain logs for all network components, including proxy servers, load balancers, and other critical infrastructure devices. Review these logs regularly to detect abnormal behavior, unauthorized access attempts, or configuration changes.</p>"},{"location":"deploy/deployment-best-practices/network-level-security/#audit-open-ports-and-services","title":"Audit open ports and services","text":"<p>Conduct periodic network scans to identify open ports and active services. Use tools such as nmap, netstat, or ss for port scanning. Ensure that only the ports necessary for your WSO2 products are accessible on both internal and external networks. Disable or monitor any additional open ports that are not explicitly required.</p>"},{"location":"deploy/deployment-best-practices/network-level-security/#enforce-device-level-security","title":"Enforce device-level security","text":"<ul> <li> <p>Regularly inspect and validate the configuration and integrity of all network devices, including routers, switches, and firewalls. Verify routing tables, access control lists, and firewall rules for correctness and consistency.</p> </li> <li> <p>Replace all default device credentials with strong, unique passwords before deploying devices in production.</p> </li> </ul>"},{"location":"deploy/deployment-best-practices/network-level-security/#apply-firmware-updates","title":"Apply firmware updates","text":"<p>Keep network device firmware up to date to mitigate vulnerabilities and maintain optimal performance. Apply updates as recommended by the device vendor after validating them in a non-production environment.</p>"},{"location":"deploy/deployment-best-practices/os-level-security/","title":"OS level Security","text":""},{"location":"deploy/deployment-best-practices/os-level-security/#run-wso2-processes-with-a-dedicated-user","title":"Run WSO2 processes with a dedicated user","text":"<p>Use a dedicated OS-level user account to run WSO2 products. Assign only the minimum permissions necessary for running the product. Avoid using the root or administrator account, as these have full privileges by default and increase the risk of security breaches.</p> <p>Recommended permissions for the dedicated user:</p> <ul> <li>Read and execute access to the application directory</li> <li>Read and write access to the log directory</li> <li>Read access to configuration files</li> <li>No sudo or administrative privileges</li> </ul>"},{"location":"deploy/deployment-best-practices/os-level-security/#minimize-installed-software","title":"Minimize installed software","text":"<p>Install only the software and packages required for your WSO2 product deployment. Unnecessary software can introduce vulnerabilities. Regularly review and monitor installed packages.</p> <p>Refer to the system requirements for details on the minimum required software.</p>"},{"location":"deploy/deployment-best-practices/os-level-security/#enable-the-firewall","title":"Enable the firewall","text":"<p>Enable and configure a host-level firewall (e.g., iptables, UFW, or firewalld) to protect inbound and outbound connections. Only open the ports that are required for product functionality.</p>"},{"location":"deploy/deployment-best-practices/os-level-security/#restrict-access-to-clustering-ports","title":"Restrict access to clustering ports","text":"<p>Apply firewall rules to restrict access to TCP ports used for clustering so that they are accessible only to other nodes within the WSO2 product cluster. Prevent access from unrecognized or external hosts.</p> <p>Note</p> <p>The actual clustering ports depend on your product configuration. Verify the ports used in your deployment before configuring firewall rules.</p>"},{"location":"deploy/deployment-best-practices/os-level-security/#use-secure-shell-ssh","title":"Use secure shell (SSH)","text":"<ul> <li>Always use Secure Shell (SSH) for remote server access and command execution. Follow these best practices when configuring SSH:</li> <li>Change the default SSH port to a non-standard, higher-numbered port.</li> <li>Disable direct root or administrator logins.</li> <li>Enable authentication via SSH keys instead of passwords.</li> <li>Display a legal or security banner before authentication to warn unauthorized users.</li> </ul>"},{"location":"deploy/deployment-best-practices/os-level-security/#keep-the-system-up-to-date","title":"Keep the system up-to-date","text":"<p>Regularly apply security patches and updates for all installed packages. Test updates in a staging environment before deploying them to production.</p>"},{"location":"deploy/deployment-best-practices/os-level-security/#monitor-user-activities","title":"Monitor user activities","text":"<p>Enable OS-level logging and review logs periodically to monitor user actions. Consider using a centralized logging or Security Information and Event Management (SIEM) solution for continuous monitoring.</p>"},{"location":"deploy/deployment-best-practices/os-level-security/#perform-regular-backups","title":"Perform regular backups","text":"<p>Back up all critical files and data regularly, and store them securely.</p> <p>Critical files to include in backups:</p> <ul> <li>Configuration files (e.g., <code>Config.toml</code>, environment-specific configurations)</li> <li>Keystores and truststores</li> <li>Database exports</li> <li>Custom scripts and deployment artifacts</li> </ul> <p>Backup recommendations:</p> <ul> <li>Perform daily backups for production environments</li> <li>Store backups in encrypted, geographically-distributed storage</li> <li>Test backup restoration procedures periodically</li> <li>Implement access controls for backup storage</li> </ul>"},{"location":"deploy/deployment-best-practices/overview/","title":"Best Practices for Production Deployment","text":"<p>This section provides comprehensive security and operational best practices for deploying WSO2 Integrator: BI in production environments. Following these guidelines helps ensure your deployment is secure, resilient, and maintainable.</p>"},{"location":"deploy/deployment-best-practices/overview/#runtime-security","title":"Runtime security","text":"<p>Runtime security focuses on securing your BI application and the services it generates during execution, including application-level configurations, patches, secrets management, TLS/SSL protocols, and logging.</p> <p>Learn more about runtime security \u2192</p>"},{"location":"deploy/deployment-best-practices/overview/#os-level-security","title":"OS level security","text":"<p>Operating system security addresses the hardening of the underlying operating system where BI is deployed, covering user accounts, firewalls, SSH configuration, system updates, and monitoring.</p> <p>Learn more about OS level security \u2192</p>"},{"location":"deploy/deployment-best-practices/overview/#network-level-security","title":"Network level security","text":"<p>Network security covers the infrastructure and network-level protections for your BI deployment, including high availability, failover configurations, network logging, port auditing, and device security.</p> <p>Learn more about network level security \u2192</p>"},{"location":"deploy/deployment-best-practices/overview/#configuration-best-practices","title":"Configuration best practices","text":"<p>Adhering to secure configuration practices is essential for minimizing vulnerabilities in your BI deployment. This includes managing sensitive data, applying security patches, and following secure coding standards.</p> <p>Learn more about configuration best practices \u2192</p>"},{"location":"deploy/deployment-best-practices/overview/#getting-started","title":"Getting started","text":"<p>Review each category above to understand the security considerations for your deployment. Implement these practices based on your specific environment, compliance requirements, and security policies. These guidelines are applicable to both cloud-native (containerized) and traditional VM-based deployments.</p>"},{"location":"deploy/deployment-best-practices/runtime-security/","title":"Runtime Security","text":""},{"location":"deploy/deployment-best-practices/runtime-security/#apply-security-patches-regularly","title":"Apply security patches regularly","text":"<p>Keeping all software components up to date is a critical part of maintaining runtime security. Security patches often address newly discovered vulnerabilities that attackers can exploit if left unpatched.</p>"},{"location":"deploy/deployment-best-practices/runtime-security/#development-environment","title":"Development environment","text":"Category Guidelines Development Tools Always use the latest stable release of Visual Studio Code.Keep WSO2 Integrator: BI and Ballerina extensions updated to ensure compatibility with the latest security and functionality enhancements."},{"location":"deploy/deployment-best-practices/runtime-security/#production-runtime","title":"Production runtime","text":"Category Guidelines Ballerina Distribution Use the latest patch release of the relevant Ballerina distribution to ensure runtime and library-level vulnerabilities are fixed.Follow Ballerina and WSO2 product release notifications to stay informed about new security advisories. Operating System and Dependencies Regularly apply security updates to the host operating system, container base images, and runtime dependencies (e.g., database clients, third-party libraries).If deploying via Docker, track and update base image versions (e.g., <code>ubuntu</code>, <code>alpine</code>, or <code>ballerina</code>) to the latest stable, patched releases. Automation and CI/CD Integration Integrate automated patch verification and dependency vulnerability scanning into CI/CD pipelines.Use dependency management tools (e.g., Dependabot, Renovate) to receive automated pull requests for new patches.Maintain a rollback plan and a staging environment to safely test patches before deploying to production. Community and Security Feeds Follow WSO2 Security Docs for timely notifications of vulnerabilities and fixes."},{"location":"deploy/deployment-best-practices/runtime-security/#use-keystores-and-truststores-correctly","title":"Use keystores and truststores correctly","text":"<ul> <li>Configure BI and the generated Ballerina services to use separate keystores for service certificates and truststores for trusted CAs.  </li> <li>Use strong passwords and store them securely (e.g., as Kubernetes secrets or environment variables).  </li> <li>Always replace the default keystore files shipped with samples.</li> </ul>"},{"location":"deploy/deployment-best-practices/runtime-security/#manage-secrets-securely","title":"Manage secrets securely","text":"<ul> <li>Never hardcode passwords, tokens, or keys in source code, configuration files, or repositories.</li> <li>Use platform-specific secret management systems such as:  <ul> <li>Kubernetes Secrets</li> <li>HashiCorp Vault</li> <li>AWS Secrets Manager or similar cloud stores.</li> </ul> </li> <li>Pass secrets into the BI runtime via configuration values.</li> </ul>"},{"location":"deploy/deployment-best-practices/runtime-security/#change-default-ports-and-credentials","title":"Change default ports and credentials","text":"<ul> <li>Change all default listener ports used by BI components and generated Ballerina services.   Example: modify configurations or <code>Config.toml</code> to run on custom, non-standard ports.   <pre><code>[ballerina.http.listeners]\nport = 9443\n</code></pre></li> <li>Disable unused ports and protocols to minimize the attack surface.</li> <li>Replace any default credentials used by admin or management consoles.</li> </ul>"},{"location":"deploy/deployment-best-practices/runtime-security/#secure-communication-with-external-services","title":"Secure communication with external services","text":"<p>When BI connects to external systems such as user stores, databases, or other APIs:</p> <ul> <li>Always enable TLS/SSL for data-in-transit protection.  </li> <li>Validate external service certificates using the truststore.  </li> <li>Verify hostnames and certificate chains to avoid man-in-the-middle attacks.  </li> <li>Restrict outbound network access to only approved endpoints.</li> </ul>"},{"location":"deploy/deployment-best-practices/runtime-security/#use-least-privilege-credentials-for-dbs-and-user-stores","title":"Use least-privilege credentials for DBs and user stores","text":"<ul> <li>Never connect to databases, LDAP, or user stores using <code>root</code> or administrator credentials.  </li> <li>Create dedicated application-level accounts with only the minimal privileges required:  </li> <li>Read/write on specific schemas or tables.  </li> <li>No administrative permissions (e.g., <code>DROP DATABASE</code>, <code>GRANT ALL</code>).  </li> <li>Rotate credentials periodically and disable accounts no longer in use.</li> </ul>"},{"location":"deploy/deployment-best-practices/runtime-security/#strengthen-tls-security","title":"Strengthen TLS security","text":"<ul> <li>Enforce TLS 1.2 or TLS 1.3 for all HTTPS and secure socket communications.  </li> <li>Disable older or insecure protocol versions (e.g., TLS 1.0/1.1, SSLv3).  </li> <li>Require strong cipher suites only (Refer to Use cipher suites).</li> </ul>"},{"location":"deploy/deployment-best-practices/runtime-security/#use-cipher-suites","title":"Use cipher suites","text":"<ul> <li>Configure Ballerina to use secure cipher suites. Refer to Ballerina Crypto for more details.</li> <li>Periodically review cipher configurations against current security standards (NIST, OWASP).</li> </ul>"},{"location":"deploy/deployment-best-practices/runtime-security/#logging-and-monitoring","title":"Logging and monitoring","text":"<ul> <li>Comprehensive logs and telemetry, when correlated with access controls and alerting, enhance the ability to identify unauthorized usage or data exfiltration attempts in production environments.</li> <li>Integrate with standardized observability tools (e.g., Prometheus, Jaeger, ELK Stack) so that you can unify your security-monitoring posture across BI deployment models.</li> </ul> <p>Follow the below guides to configure logging and observability.</p> <ul> <li>Configure Logging</li> <li>Observability in BI</li> </ul>"},{"location":"deploy/deployment-best-practices/runtime-security/#prevent-log-forging","title":"Prevent log forging","text":"<ul> <li>Sanitize all user-provided data before writing to logs.  </li> <li>Configure the logging framework to escape newline and control characters.  </li> <li>Use structured logging where possible to make parsing safer.  </li> <li>Restrict log file write permissions to the BI runtime user only.</li> </ul>"},{"location":"deploy/deployment-best-practices/runtime-security/#set-secure-jvm-parameters","title":"Set secure JVM parameters","text":"<p>Since Ballerina runs on the JVM, tune the JVM for security and stability:</p> <ul> <li>Use a supported JDK version with the latest security patches.  </li> <li>Limit heap size and enable garbage-collection logs for troubleshooting.  </li> <li>Run BI under a non-root user with limited filesystem and network permissions.</li> </ul>"},{"location":"deploy/deployment-best-practices/runtime-security/#additional-hardening-recommendations","title":"Additional hardening recommendations","text":"<ul> <li>Run as Non-Root: Configure containers or services to run as a non-root OS user.  </li> <li>File Permissions: Restrict access to configuration files, keystores, and logs (<code>chmod 600</code>).  </li> <li>Network Segmentation: Place BI and databases on private networks/VPCs.  </li> <li>Audit and Compliance: Periodically audit configurations and review access logs.  </li> <li>Backup and Recovery: Encrypt and test backups regularly.  </li> <li>Validate the code with scan tool: Use Ballerina scan tool to identify potential issues such as code smells, bugs, and vulnerabilities.</li> </ul>"},{"location":"deploy/vm-based-deployment/centralized-deployment/","title":"Centralized Deployment","text":"<p>Managing a large number of BI artifacts across different environments can become complex over time. Each integration flow or service, if deployed independently, can lead to higher operational overhead and increased resource consumption. Centralized deployment simplifies this by bundling all related integration artifacts into a single deployable unit, enabling more efficient resource utilization and streamlined deployments. This approach is ideal when multiple integration solutions need to be deployed and managed together across environments.</p> <p>Centralized deployment typically involves two repositories:</p>"},{"location":"deploy/vm-based-deployment/centralized-deployment/#source-repository-ci","title":"Source repository (CI)","text":"<p>Typically, a single integration can consist of multiple components, each implemented as a separate BI project. These components can represent distinct functionalities or services that collectively form the complete integration solution. By organizing the integration into multiple projects, the source repository ensures modularity, reusability, and easier maintenance. Each project can be developed, tested, and published independently, allowing teams to work on different components in parallel while maintaining a clear separation of concerns.</p> <p>The source repository is responsible for the continuous integration (CI) process, which includes:</p>"},{"location":"deploy/vm-based-deployment/centralized-deployment/#steps-in-the-ci-process","title":"Steps in the CI process:","text":""},{"location":"deploy/vm-based-deployment/centralized-deployment/#step-1-prepare-server-environment","title":"Step 1: Prepare server environment","text":"<ul> <li>Provision the VM or Bare-Metal Server.</li> <li>Ensure the server meets the hardware requirements for your application (CPU, memory, disk space, etc.).</li> <li>Configure the server OS (Linux is recommended for production).</li> </ul>"},{"location":"deploy/vm-based-deployment/centralized-deployment/#step-2-install-prerequisites","title":"Step 2: Install prerequisites","text":"<ul> <li>Visual Studio Code: Install Visual Studio Code if you don't have it already.</li> <li>WSO2 Integrator: BI Extension: Install WSO2 Integrator: BI.</li> </ul>"},{"location":"deploy/vm-based-deployment/centralized-deployment/#step-3-create-and-implement-bi-projects","title":"Step 3: Create and implement BI projects","text":"<ul> <li>Create a new integration project using the BI VS Code extension.</li> <li>Add a meaningful description to the README.</li> </ul> Note<p>The README is required when you publish a package to a repository. You can edit the content to add a meaningful description about the integration project.</p> <ul> <li>Implement business logic using the drag-and-drop designer or by writing Ballerina/DSL code.</li> </ul> Tip<p>Use shared modules or libraries for common logic and avoid duplication.</p>"},{"location":"deploy/vm-based-deployment/centralized-deployment/#step-4-add-tests-optional","title":"Step 4: Add tests (optional)","text":"<ul> <li>Use the <code>Test Explorer</code> to create integration tests for services and connectors.</li> </ul>"},{"location":"deploy/vm-based-deployment/centralized-deployment/#step-5-build-or-pack-the-artifacts","title":"Step 5: Build or pack the artifacts","text":"<p>Build the project and create a self-contained executable and deployable <code>.jar</code> artifact.</p> <p></p> <p>Alternatively, use the Ballerina CLI to pack the artifacts for use in a consolidated package.</p> <pre><code>bal pack\n</code></pre>"},{"location":"deploy/vm-based-deployment/centralized-deployment/#step-6-publish-artifacts","title":"Step 6: Publish artifacts","text":"<p>After packing the project, publish the generated artifacts to a shared artifact repository (e.g., Ballerina Central, GitHub Packages, Nexus, or internal registry).</p> <pre><code>bal push\n</code></pre>"},{"location":"deploy/vm-based-deployment/centralized-deployment/#publish-to-ballerina-central","title":"Publish to Ballerina Central","text":"<ol> <li>Create an account on Ballerina Central</li> <li>Navigate to the Dashboard and acquire an access token.</li> <li> <p>Download and place the <code>Settings.toml</code> file in your home repository (<code>&lt;USER_HOME&gt;/.ballerina/</code>). If you already have a <code>Settings.toml</code> file configured in your home repository, follow the other option and copy the access token into the <code>Settings.toml</code> as follows.</p> <pre><code>[central]\naccesstoken=\"&lt;token&gt;\"\n</code></pre> Note<p>If you are working in a context where it is not possible to save the <code>Settings.toml</code> file (e.g., within a CI/CD pipeline) you can set the access token via the <code>BALLERINA_CENTRAL_ACCESS_TOKEN</code> environment variable. <pre><code>export BALLERINA_CENTRAL_ACCESS_TOKEN=\"&lt;token&gt;\"\n</code></pre></p> </li> <li> <p>Publish the package</p> <pre><code>bal push\n</code></pre> </li> </ol> <p>Refer to Publish to Ballerina Central repository for more configuration and information.</p>"},{"location":"deploy/vm-based-deployment/centralized-deployment/#publish-to-local-repository","title":"Publish to local repository","text":"<pre><code>bal push --repository local\n</code></pre> <p>See Use custom repositories for packages management for more information.</p>"},{"location":"deploy/vm-based-deployment/centralized-deployment/#publish-to-custom-repositories","title":"Publish to custom repositories","text":"<p>BI supports Maven repositories such as Nexus, Artifactory and GitHub packages. </p> <p>Follow Using custom repositories for package management to learn more about setting up custom repositories to publish packages.</p> Tip<p>Automate the above CI steps using GitHub Actions or your preferred CI tool.</p>"},{"location":"deploy/vm-based-deployment/centralized-deployment/#deployment-repository-cd","title":"Deployment repository (CD)","text":"<p>The deployment repository acts as the central hub for production-ready integration artifacts. It collects and consolidates the required applications from one or more source repositories, enabling centralized configuration and deployment. This repository streamlines the deployment process by orchestrating the integration of these applications and preparing them for deployment to the target environment. By centralizing deployment management, it simplifies configuration, enhances maintainability, and ensures consistency across environments.</p>"},{"location":"deploy/vm-based-deployment/centralized-deployment/#steps-in-the-cd-process","title":"Steps in the CD process:","text":""},{"location":"deploy/vm-based-deployment/centralized-deployment/#step-1-prepare-the-runtime-environment","title":"Step 1: Prepare the runtime environment","text":"<ul> <li>Provision a server or containerized environment (e.g., Kubernetes, Docker).</li> <li>Install WSO2 Integrator runtime.</li> <li>Ensure external dependencies (databases, message brokers, etc.) are configured.</li> </ul>"},{"location":"deploy/vm-based-deployment/centralized-deployment/#step-2-fetch-and-consolidate-artifacts","title":"Step 2: Fetch and consolidate artifacts","text":"<ul> <li> <p>Go to the terminal on VS Code and install the <code>consolidate-packages</code> tool</p> <pre><code>bal tool pull consolidate-packages\n</code></pre> </li> <li> <p>Go to the terminal on VS Code and install the <code>consolidate-packages</code> tool</p> <pre><code>bal tool pull consolidate-packages\n</code></pre> </li> <li> <p>Pull integration artifacts from the source/artifact repositories to create a consolidated project</p> <pre><code>bal consolidate-packages new --package-path &lt;consolidated-project-path&gt; &lt;comma-separated-list-of-package-names&gt;\n</code></pre> </li> </ul> Note<p>Use consolidated-packages with exact package versions and <code>--repository=local</code> option to create a new consolidated package with the packages published in local repository. For example: <pre><code>bal consolidate-packages new --package-path lms_service sampleorg/lms_assessments:0.1.0,sampleorg/lms_courses:0.1.0 --repository=local\n</code></pre></p> Tip<p>Visit the Consolidate-packages tool for more information on how to consolidate Ballerina packages.</p>"},{"location":"deploy/vm-based-deployment/centralized-deployment/#step-3-add-integration-tests-to-the-consolidated-project-optional","title":"Step 3: Add integration tests to the consolidated project (Optional)","text":"<ul> <li>Write and execute tests for the consolidated project.</li> </ul>"},{"location":"deploy/vm-based-deployment/centralized-deployment/#step-4-create-the-executable-jar-for-the-project","title":"Step 4: Create the executable JAR for the project","text":"<ul> <li>Use the <code>bal build</code> command to build the consolidated project.</li> <li>The integration will be built as an executable JAR, and the JAR file will be available in the <code>target/bin</code> directory of the project.</li> </ul> <p>The generated Ballerina artifact can be deployed to the target environment, configuring necessary environment variables and system settings.</p>"},{"location":"deploy/vm-based-deployment/de-centralized-deployment/","title":"De-centralized Deployment","text":"<p>The de-centralized deployment offers a straightforward approach, ideal for simpler applications or when direct control over individual deployments is preferred. In this method, BI artifacts are developed and published to a registry (a storage location for deployable components). The deployment process retrieves these artifacts and deploys them to the target environment, ensuring all necessary dependencies and configurations are included.</p>"},{"location":"deploy/vm-based-deployment/de-centralized-deployment/#continuous-integration-ci","title":"Continuous integration (CI)","text":"<p>Continuous Integration (CI) in de-centralized deployment streamlines development by automating the building, testing, and publishing of individual BI artifacts, ensuring faster feedback and fewer integration issues.</p> <p>The following steps outline the CI process of the de-centralized deployment:</p>"},{"location":"deploy/vm-based-deployment/de-centralized-deployment/#step-1-prepare-the-server-environment","title":"Step 1: Prepare the server environment","text":"<ul> <li>Provision the VM or Bare-metal Server.</li> <li>Ensure the server meets the hardware requirements for your application (CPU, memory, disk space, etc.).</li> <li>Configure the server OS (Linux is recommended for production).</li> </ul>"},{"location":"deploy/vm-based-deployment/de-centralized-deployment/#step-2-install-prerequisites","title":"Step 2: Install prerequisites","text":"<ul> <li>Visual Studio Code: Install Visual Studio Code if you don't have it already.</li> <li>WSO2 Integrator: BI Extension: Install the WSO2 Integrator: BI extension. Refer to Install WSO2 Integrator: BI for detailed instructions.</li> </ul>"},{"location":"deploy/vm-based-deployment/de-centralized-deployment/#step-3-create-and-implement-bi-projects","title":"Step 3: Create and implement BI projects","text":"<ul> <li>Create a new integration project using the BI VS Code extension.</li> <li>Implement business logic using the drag-and-drop designer or by writing Ballerina/DSL code.</li> </ul> Tip<p>Use shared modules or libraries for common logic and avoid duplication.</p>"},{"location":"deploy/vm-based-deployment/de-centralized-deployment/#step-4-add-integration-tests-optional","title":"Step 4: Add integration tests (Optional)","text":"<ul> <li>Use the <code>Test Explorer</code> of BI to write and execute tests for the project.    </li> </ul>"},{"location":"deploy/vm-based-deployment/de-centralized-deployment/#step-5-create-the-executable-jar-for-the-project","title":"Step 5: Create the executable JAR for the project","text":"<ul> <li>Navigate to the Visualizer view by clicking on the BI icon on the sidebar.</li> <li>Click on the Deploy on VM under the Deployment Options section in the right panel.</li> <li>Click Create Executable button.      </li> <li>The integration will be built as an executable JAR and the JAR file will be available in the <code>target\\bin</code> directory of the project.</li> </ul>"},{"location":"deploy/vm-based-deployment/de-centralized-deployment/#step-6-publish-the-artifacts-to-the-registry","title":"Step 6: Publish the artifacts to the registry.","text":""},{"location":"deploy/vm-based-deployment/de-centralized-deployment/#continuous-deployment-cd","title":"Continuous deployment (CD)","text":"<p>The Continuous Deployment (CD) process in a de-centralized setup involves automating the deployment of Ballerina artifacts to the target environment. This typically involves using a deployment workflow or pipeline to retrieve the built artifacts from the registry, configure the target environment, deploy the application, and verify its successful deployment.</p>"},{"location":"deploy/vm-based-deployment/deploy-on-vm-as-executable-jar/","title":"Deploy on a VM as an Executable JAR","text":"<p>This guide explains how to deploy an integration as an executable JAR file.</p> <ul> <li>Navigate to the Visualizer view by clicking on the BI icon on the sidebar.</li> <li>Click on the Deploy on VM under the Deployment Options section in the right panel.</li> <li> <p>Click Create Executable button.      </p> </li> <li> <p>The integration will be built as an executable JAR and the JAR file will be available in the <code>target\\bin</code> directory of the project.</p> </li> </ul>"},{"location":"deploy/vm-based-deployment/github-action-for-cicd/","title":"GitHub Action for CI/CD integration","text":"<p>The Ballerina GitHub Action enables seamless automation of CI/CD workflows for WSO2 Integrator: BI projects hosted on GitHub. This action can be used to build and push Ballerina packages that serve as integration artifacts in WSO2 Integrator: BI.</p> <p>The following sample GitHub Actions workflow demonstrates how to automate the build and publish process for a Ballerina-based integration using WSO2 Integrator: BI. The workflow uses the Ballerina GitHub Action to build the integration and publish it to Ballerina Central.</p> <pre><code>name: Ballerina publish example\n\non: [workflow_dispatch]\n\njobs:\nbuild:\n\nruns-on: ubuntu-latest\n\nsteps:\n- name: Checkout\nuses: actions/checkout@v1\n\n- name: Ballerina Build\nuses: ballerina-platform/ballerina-action@master\nwith:\nargs: pack\n\n- name: Ballerina Push\nuses: ballerina-platform/ballerina-action@master\nwith:\nargs: push env: BALLERINA_CENTRAL_ACCESS_TOKEN: ${{ secrets.BallerinaToken }}\n</code></pre>"},{"location":"deploy/vm-based-deployment/overview/","title":"Introduction to VM-based Deployment","text":"<p>Integrations developed with BI can be deployed on virtual machines (VMs), offering a flexible and familiar option for organizations operating in traditional IT environments or requiring fine-grained control over their infrastructure.</p> <p>VM-based deployment involves provisioning virtualized compute resources\u2014either on-premises or in the cloud\u2014and manually configuring the environment to host the BI runtime and its dependencies. This approach is well-suited for setups where containerization is not feasible or where integration with existing VM-based infrastructure is required.</p> <p>There are two common scenarios for VM-based deployment:</p> <ul> <li> <p>On-Premises VMs:   Ideal for environments with strict data residency, security, or compliance requirements. Organizations can deploy and manage BI on VMs running in a controlled internal network.</p> </li> <li> <p>Cloud-hosted VMs:   Suitable for leveraging cloud scalability while maintaining control over the OS and runtime environment. Popular platforms like AWS EC2, Azure Virtual Machines, or Google Compute Engine allow BI to run within customized VM instances.</p> </li> </ul> <p>VM-based deployment offers:</p> <ul> <li>Greater control over operating system, networking, and runtime configurations.</li> <li>Compatibility with legacy systems and hybrid infrastructure models.</li> <li>Easier adoption for teams already using VM-based workflows.</li> <li>A viable alternative where containerization is restricted or unsupported.</li> </ul> <p>In the following sections, you will learn how to set up, configure, and manage BI deployments on virtual machines, including environment preparation, runtime installation, and deployment automation strategies.</p> <ul> <li>Centralized Deployment</li> <li>De-centralized Deployment</li> <li>Deploy on VM as Executable Jar</li> <li>GitHub Action for CICD</li> </ul>"},{"location":"developer-guides/create-a-project/","title":"Create a Project","text":"<p>A project in WSO2 Integrator: BI is the foundational workspace where you define, organize, and manage all your integration artifacts\u2014such as services, data mappings, and connections.</p> <p>Follow the steps below to create your first integration project using WSO2 Integrator: BI.</p>"},{"location":"developer-guides/create-a-project/#steps-to-create-an-integration-project","title":"Steps to Create an Integration Project","text":"<ol> <li> <p>Launch Visual Studio Code with the WSO2 Integrator: BI extension enabled.</p> <p>Info</p> <p>If you have not installed the extension, follow the Install WSO2 Integrator: BI guide.</p> </li> <li> <p>Open the BI Extension</p> <p>Click the BI icon on the Activity Bar of the VS Code editor. This opens the WSO2 Integrator: BI view.</p> <p></p> </li> <li> <p>Create a New Integration Project</p> <p>Click on Create New Integration in the BI view.</p> <p></p> </li> <li> <p>Name and Choose Location for Your Project</p> <p>Enter a suitable name for your integration project and select a location to save it.</p> <p></p> <p>Expected outcome: A new integration project folder is created at your chosen location.</p> </li> <li> <p>Access the BI Home Page</p> <p>Once the project is created, the BI home page will open automatically in VS Code.</p> <p></p> <p>Expected outcome: You should see the BI home page, ready for you to start developing integration artifacts.</p> </li> </ol> <p>Now, you can start creating your integration by developing artifacts. See the WSO2 Integrator: BI Artifacts to learn about the integration artifacts.</p> <p>Additionally, you can enhance your experience by incorporating AI-powered assistance with BI Copilot.</p>"},{"location":"developer-guides/data-mapping/","title":"Data Mapping","text":"<p>This guide shows how to build an integration that transforms a JSON payload into a different JSON structure using WSO2 Integrator: BI Data Mapper. You will create an HTTP service with a single resource (<code>transform</code>) to receive a JSON payload and return the transformed result.</p>"},{"location":"developer-guides/data-mapping/#step-1-create-a-new-integration-project","title":"Step 1: Create a new integration project","text":"<ol> <li>Click on the BI icon on the sidebar.</li> <li>Click on the Create New Integration button.</li> <li>Enter the integration name as <code>Transformer</code>.</li> <li>Select integration directory location by clicking on the Select Path button.</li> <li> <p>Click on the Create Integration button to create the integration project.  </p> <p> </p> </li> </ol>"},{"location":"developer-guides/data-mapping/#step-2-create-an-http-service","title":"Step 2: Create an HTTP service","text":"<ol> <li>In the design view, click on the Add Artifact button.</li> <li>Select HTTP Service under the Integration as API category.</li> <li> <p>Click on the Create button to create the new service with the default configurations.</p> <p> </p> </li> </ol>"},{"location":"developer-guides/data-mapping/#step-3-add-the-resource-method","title":"Step 3: Add the resource method","text":"<ol> <li>Click Add Resource and select POST method.</li> <li>Set the resource path as <code>transform</code>.</li> <li> <p>Configure the request payload by pasting the JSON sample below. Set the type name as <code>Input</code> and the variable name to <code>input</code>.</p> <pre><code>{\n\"user\": {\n\"firstName\": \"John\",\n\"lastName\": \"Doe\",\n\"email\": \"john.doe@example.com\",\n\"address\": {\n\"street\": \"123 Elm St\",\n\"city\": \"San Francisco\",\n\"state\": \"CA\",\n\"postalCode\": 94107\n},\n\"phoneNumbers\": [\"123-456-7890\", \"098-765-4321\"]\n},\n\"account\": {\n\"accountNumber\": \"A123456789\",\n\"balance\": 2500,\n\"lastTransaction\": \"2023-10-15T14:30:00Z\"\n}\n}\n</code></pre> <p> </p> </li> <li> <p>Change the response body schema of the <code>201</code> response to <code>Output</code>.</p> </li> <li>To create the type named <code>Output</code>, click Create New Type within the Message Body Type editor.</li> <li> <p>Switch to Import mode and provide the following JSON to create the BI type named <code>Output</code>.</p> <pre><code>{\n\"fullName\": \"John Doe\",\n\"contactDetails\": {\n\"email\": \"john.doe@example.com\",\n\"primaryPhone\": \"123-456-7890\"\n},\n\"location\": {\n\"city\": \"San Francisco\",\n\"state\": \"CA\",\n\"zipCode\": \"94107\"\n},\n\"accountInfo\": {\n\"accountNumber\": \"A123456789\",\n\"balance\": 2500\n},\n\"transactionDate\":  \"2023-10-15T14:30:00Z\"\n}\n</code></pre> <p> </p> </li> <li> <p>Click Save to apply the response configuration.</p> </li> <li>Finally, click Save to update the resource with the specified configurations. </li> </ol> <p>Resource Method</p> <p>To learn more about resources, see Ballerina Resources.</p> <p>Reusable vs inline data mappers</p> <p>There are two ways to create data mappers in WSO2 Integrator: BI. You can create reusable data mappers that can be used anywhere within the integration project, or inline data mappers that are specific to a particular resource or function. In this guide, Step 5 demonstrates how to create a reusable data mapper, and Step 6 (optional) demonstrates how to create an inline data mapper.</p>"},{"location":"developer-guides/data-mapping/#step-5-add-a-reusable-data-mapper","title":"Step 5: Add a reusable data mapper","text":"<ol> <li>Click on the <code>transform</code> resource to navigate to the resource implementation designer view.</li> <li>Hover over the arrow after start and click the \u2795 button to add a new action to the resource.</li> <li>Select Map Data from the node panel and click the Create Data Mapper button. </li> <li> <p>Fill in the required fields with the values below and click the <code>Create</code> button to create the data mapper.</p> Field Value Data Mapper Name <code>transformed</code> Inputs <code>Input input</code> Output <code>Output</code> <p></p> </li> <li> <p>Your newly created data mapper appears under Current Integration. Click it to add it to the sequence.</p> </li> <li>Set <code>payload</code> as the Input and <code>outputResult</code> as the Result, then save.</li> <li> <p>Click the kebab menu on the Map Data node and choose View to open the visual data mapper.</p> <p></p> </li> </ol>"},{"location":"developer-guides/data-mapping/#step-6-optional-add-an-inline-data-mapper","title":"Step 6 (Optional): Add an inline data mapper","text":"<ol> <li>Click on the <code>transform</code> resource to navigate to the resource implementation designer view.</li> <li>Hover over the arrow after start and click the \u2795 button to add a new action to the resource.</li> <li>Select Declare Variable from the node panel.</li> <li>Provide <code>output</code> as the Name and select <code>Output</code> as the Type.</li> <li> <p>Once you select the type, the Open in Data Mapper button appears. Click it to open the visual data mapper.</p> <p></p> <p></p> </li> </ol>"},{"location":"developer-guides/data-mapping/#step-7-create-mappings","title":"Step 7: Create mappings","text":"<p>Click a source field, then click the desired target field to create a mapping.</p>"},{"location":"developer-guides/data-mapping/#create-simple-mapping","title":"Create simple mapping","text":""},{"location":"developer-guides/data-mapping/#auto-mapping","title":"Auto mapping","text":"<p>Click the Auto Map button to automatically create mappings. The BI Copilot panel opens to assist you.</p> <p></p>"},{"location":"developer-guides/data-mapping/#many-to-one-mapping","title":"Many-to-one mapping","text":"<p>You can map multiple source fields to a single target field. For example, create the <code>fullName</code> field in the output by combining the <code>firstName</code> and <code>lastName</code> fields from the input.</p> <p></p>"},{"location":"developer-guides/data-mapping/#edit-mapping-expression","title":"Edit mapping expression","text":"<p>Click the <code>&lt;&gt;</code> button on any link, or use the context menu on any output field, to edit the mapping expression.</p> <p></p>"},{"location":"developer-guides/data-mapping/#array-mapping","title":"Array mapping","text":"<p>BI Data Mapper offers several ways to map arrays. You can map entire arrays, specific elements, or use functions to manipulate array data. To map the first phone number from the <code>phoneNumbers</code> array in the input to the <code>primaryPhone</code> field in the output, create a mapping from <code>phoneNumbers</code> to <code>primaryPhone</code> and pick \"Extract Single Element From Array\" to get the first element. </p> <p></p>"},{"location":"developer-guides/data-mapping/#step-8-return-the-transformed-payload","title":"Step 8: Return the transformed payload","text":"<ol> <li>After you complete the mappings, click the Go Back button to return to the resource designer view.</li> <li>Hover over the arrow after the Map Data node in the flow diagram and click the \u2795 button.</li> <li> <p>Select Return from the node panel. </p> <p></p> </li> <li> <p>Provide <code>outputResult</code> as the return expression.</p> </li> <li> <p>The final code looks like this. The source view can be accessed by clicking the <code>&lt;/&gt;</code> button in the top right corner. </p> <pre><code>import ballerina/http;\n\nlistener http:Listener httpDefaultListener = http:getDefaultListener();\n\nservice / on httpDefaultListener {\n    resource function post transform(@http:Payload Input payload) returns error|json|http:InternalServerError {\n        do {\n            Output outputResult = transform(payload);\n            return outputResult;\n        } on fail error err {\n            // handle error\n            return error(\"unhandled error\", err);\n        }\n    }\n}\n</code></pre> </li> </ol>"},{"location":"developer-guides/data-mapping/#step-9-run-the-integration","title":"Step 9: Run the integration","text":"<ol> <li>Click the Run button in the top-right corner to run the integration.</li> <li>Confirm the Test with Try it Client prompt. The integration starts running and serving at http://localhost:9090/transform.</li> <li> <p>Verify the integration by sending a POST request to the <code>/transform</code> endpoint with the following JSON payload.</p> <pre><code>curl -X POST \"http://localhost:9090/transform\" -H \"Content-Type: application/json\" -d '{\n    \"user\": {\n        \"firstName\": \"John\",\n        \"lastName\": \"Doe\",\n        \"email\": \"john.doe@example.com\",\n        \"address\": {\n            \"street\": \"123 Elm St\",\n            \"city\": \"San Francisco\",\n            \"state\": \"CA\",\n            \"postalCode\": 94107\n        },\n        \"phoneNumbers\": [\"123-456-7890\", \"098-765-4321\"]\n    },\n    \"account\": {\n        \"accountNumber\": \"A123456789\",\n        \"balance\": 2500,\n        \"lastTransaction\": \"2023-10-15T14:30:00Z\"\n    } \n}'\n</code></pre> </li> <li> <p>The response will be the transformed JSON payload. <pre><code>{\n\"fullName\": \"John Doe\",\n\"contactDetails\": {\n\"email\": \"john.doe@example.com\",\n\"primaryPhone\": \"123-456-7890\"\n},\n\"location\": {\n\"city\": \"San Francisco\",\n\"state\": \"CA\",\n\"zipCode\": \"94107\"\n},\n\"accountInfo\": {\n\"accountNumber\": \"A123456789\",\n\"balance\": 2500\n},\n\"transactionDate\":  \"2023-10-15T14:30:00Z\"\n}\n</code></pre></p> </li> </ol>"},{"location":"developer-guides/design-the-integrations/","title":"Design the Integrations","text":"<p>WSO2 Integrator: BI provides intuitive tools to design, analyze, and manage integration flows. Whether you're creating new integrations or reviewing existing ones, the visualizer offers a clear and structured view of system behavior to streamline development and maintenance.</p>"},{"location":"developer-guides/design-the-integrations/#design-view","title":"Design view","text":"<p>The Design View in BI provides an intuitive, visual interface for developing integration projects. It helps you model, understand, and manage integration flows without needing to write or view code directly.</p> <p>Design View helps you:</p> <ul> <li>Visually model complex integration logic.</li> <li>Quickly understand the structure and behavior of your integrations.</li> <li>Accelerate development with drag-and-drop simplicity and AI-powered assistance.</li> <li>Manage deployment and documentation from a unified interface.</li> </ul> <p></p> <p>Key areas of the design view are as follows. </p>"},{"location":"developer-guides/design-the-integrations/#project-explorer","title":"Project explorer","text":"<p>The left sidebar displays the structure of your integration project. It organizes key elements, including:</p> <ul> <li>Entry Points \u2013 Define how your integration is triggered (for example, HTTP services or events).</li> <li>Listeners \u2013 Define the underlying protocols or transports used to receive incoming requests or events.</li> <li>Connections \u2013 Represent external systems that the integration interacts with, such as APIs or databases.</li> <li>Types, Functions, and Data Mappers \u2013 Contain reusable definitions and logic for handling data.</li> <li>Configurations \u2013 Hold externalized values like API keys or secrets.</li> <li>Local Connectors \u2013 Include reusable custom components.</li> </ul>"},{"location":"developer-guides/design-the-integrations/#canvas","title":"Canvas","text":"<p>The central design panel provides a visual representation of the integration flow. Each element is represented as a node, showing how services, listeners, and connections interact with one another. You can view service endpoints and how data moves through the integration.</p>"},{"location":"developer-guides/design-the-integrations/#toolbar","title":"Toolbar","text":"<p>Located at the top of the canvas, the toolbar offers quick actions to:</p> <ul> <li>Generate parts of the integration using AI assistance.</li> <li>Add an Artifact to insert services, events, or other integration elements.</li> </ul>"},{"location":"developer-guides/design-the-integrations/#deployment-options","title":"Deployment options","text":"<p>The panel on the right shows available deployment methods and the status of current deployments. You can choose to:</p> <ul> <li>Deploy to WSO2 Devant.</li> <li>Deploy using Docker or on a virtual machine (VM).</li> <li>Enable the Integration Control Plane (ICP) to monitor and manage deployments.</li> </ul>"},{"location":"developer-guides/design-the-integrations/#readme-panel","title":"README panel","text":"<p>This panel provides contextual documentation about the integration. It\u2019s used to describe the integration\u2019s purpose, features, usage instructions, and external references such as GitHub repositories.</p>"},{"location":"developer-guides/design-the-integrations/#functionautomation-logic-view","title":"Function/Automation logic view","text":"<p>When working with Functions and Automations in BI, you can explore and edit the internal logic using two interactive visual modes\u2014Flow and Sequence. These modes provide complementary perspectives to help understand and manage the behavior of your logic components effectively.</p>"},{"location":"developer-guides/design-the-integrations/#flow-diagram","title":"Flow diagram","text":"<p>The Flow mode presents a high-level, graphical layout of the execution path. It emphasizes clarity by organizing actions vertically in the order in which they are executed.</p> <ul> <li>Shows each step, such as service calls, variable declarations, conditionals, and returns.</li> <li>Includes a visual Error Handler block for defining error management logic.</li> <li>Highlights the end-to-end logic in a simplified, linear format.</li> <li>Helps you quickly understand and edit the logic.</li> </ul> <p></p>"},{"location":"developer-guides/design-the-integrations/#editing-capabilities-in-the-flow-diagram","title":"Editing capabilities in the flow diagram","text":"<p>The Flow mode in BI provides an intuitive, interactive interface for visually editing integration logic. It allows you to design and refine integration flows by interacting directly with the diagram.</p> <p>Interaction Options on Hover</p> <p>When you hover over a connector line between two nodes, the following options become available:</p> <ul> <li>Use AI Assistance: Enter a prompt to generate the next set of nodes using AI assistance. This helps accelerate integration development with contextual suggestions.</li> <li>Add a Comment: Attach comments to document your flow design or explain decisions.</li> <li>Insert Artifacts: Add new artifacts (e.g., functions, conditions, connectors) from the artifact panel between connected nodes.</li> </ul> <p>Node-Level actions</p> <p>Each node in the flow diagram provides a menu with the following actions:</p> <ul> <li>Edit: Modify the operation or configuration of the node.</li> <li>Delete: Remove the node from the diagram.</li> <li>Add Breakpoint: Insert a breakpoint to pause execution at runtime for debugging.</li> <li>View Source: Open and inspect the corresponding source code of the node.</li> </ul> <p>These features make it easy to build, understand, and troubleshoot integrations in a highly visual way.</p> <p></p>"},{"location":"developer-guides/design-the-integrations/#sequence-mode","title":"Sequence mode","text":"<p>The Sequence mode offers a more structured, detailed view closer to traditional sequence diagrams and helps to understand integration behaviours. It emphasizes the sequence of message exchanges across different components in the system.</p> <ul> <li>Displays the flow of invocations between services, clients, and functions.</li> <li>Clearly represents input/output calls and the order of execution.</li> <li>Useful for analyzing integration logic from an operational or interaction standpoint.</li> </ul> <p></p>"},{"location":"developer-guides/design-the-integrations/#types-diagram","title":"Types diagram","text":"<p>The Types Diagram shows a clear visual of how types are defined and connected in the application, allowing for visually adding and editing types. It makes it easier to design data models and understand nested structures by showing their relationships graphically. You can open the Types diagram by clicking on a type in the left panel. </p> <p></p>"},{"location":"developer-guides/design-the-integrations/#source-code-view","title":"Source code view","text":"<p>In addition to the visual views, BI provides a source code view for directly editing the underlying Ballerina code.</p> <p>You can switch to this view by clicking the <code>&lt;/&gt;</code> icon located in the top-right corner of the editor interface. This opens the full source code corresponding to your integration logic.</p> Generate with AI<p>The underlying code is generated in Ballerina, a cloud-native programming language designed for integration. This view is especially useful for users who prefer text-based editing or need fine-grained control over the implementation.</p> <p>Changes made in the Design View or the Source Code View stay in sync, allowing for a seamless switch between visual and code-based development.</p> <p></p>"},{"location":"developer-guides/test-the-integrations/","title":"Testing","text":"<p>BI has a built-in robust test framework, which allows you to ensure that your applications are reliable. The Ballerina powered testframework provides support for assertions, data providers, mocking, and code coverage features, which enable programmers to write comprehensive tests.</p>"},{"location":"developer-guides/test-the-integrations/#test-a-simple-function","title":"Test a Simple Function","text":"<p>Follow the steps below to get started with testing in your BI project.</p> <p>1. Create a BI Project    Open WSO2 BI and create a new project.    Add an artifact of type <code>automation</code> to your project.</p> <p>2. Add a Sample Function    In the generated <code>main.bal</code> file, add the following function:</p> <pre><code>public function intAdd(int a, int b) returns int {\n    return a + b;\n}\n</code></pre> <p>3. Create a Unit Test    Navigate to the Testing extension from the left-hand navigation panel.    Create a new unit test and include an assertion to validate the output of the <code>intAdd</code> function.</p> <p>4. Run the Tests    Use the Run Test option to execute the newly added test case, or choose to run all the tests under the <code>DEFAULT_GROUP</code>.</p> <p></p> <p>For further details on the Ballerina testframework you can refer to Ballerina Testing Guide.</p>"},{"location":"developer-guides/try-the-integration/","title":"Try the Integration","text":"<p>Once you have completed building your integration with the WSO2 Integrator: BI, you can quickly test it right from the design interface. This section walks you through how to run and try your integration project using the built-in tooling.</p>"},{"location":"developer-guides/try-the-integration/#run-the-integration","title":"Run the integration","text":"<ol> <li> <p>Click Run on the top-right title bar of the editor.</p> <p></p> </li> <li> <p>If the required configuration values are missing, a prompt will appear as shown below, indicating that the <code>Config.toml</code> file is missing.</p> <p>You\u2019ll be given three options:</p> <ul> <li>Create Config.toml \u2013 Recommended to generate and populate the required configuration file.</li> <li>Run Anyway \u2013 Proceed without the config file (not recommended for integrations requiring config values).</li> <li>Cancel \u2013 Abort the run operation.</li> </ul> <p>Make sure to choose Create Config.toml and fill in the necessary values before continuing.</p> <p></p> </li> <li> <p>This will launch the integration terminal.</p> </li> </ol>"},{"location":"developer-guides/try-the-integration/#try-the-services","title":"Try the services","text":"<p>The Try It window on the right side of the BI interface provides a built-in way to test your HTTP services without leaving the development environment. </p> <p></p>"},{"location":"developer-guides/wso2-integrator-bi-artifacts/","title":"WSO2 Integrator: BI Artifacts","text":"<p>WSO2 Integrator: BI supports a range of artifact types that enable developers to build powerful, event-driven, API-based, and file-based integration solutions. Each artifact type defines how an integration is triggered and how it behaves in various runtime environments.</p> <p></p> <p>Below is an overview of the available artifact types in the BI.</p>"},{"location":"developer-guides/wso2-integrator-bi-artifacts/#automation","title":"Automation","text":"<p>Create an automation that can be triggered manually or scheduled to run periodically. Automations are ideal for time-based or on-demand processes such as data synchronization, report generation, or cleanup jobs.</p>"},{"location":"developer-guides/wso2-integrator-bi-artifacts/#ai-integration","title":"AI Integration","text":"<p>Create an integration that connects your system with AI capabilities.</p> <p>Available AI Integration types:</p> <ul> <li>AI Chat Agent - Create an intelligent agent that can be accessed via chat or exposed as an API. AI Chat Agents are useful when you want to embed LLM-backed reasoning or decision-making capabilities into your integration workflows.</li> <li>MCP Service - Create an integration that exposes capabilities through the Model Context Protocol (MCP), enabling AI models to interact with your systems and data sources.</li> </ul>"},{"location":"developer-guides/wso2-integrator-bi-artifacts/#integration-as-api","title":"Integration as API","text":"<p>Create an integration that exposes services over various protocols such as HTTP, GraphQL, or TCP. This artifact type is used when building services that must interact with external systems through standard APIs.</p>"},{"location":"developer-guides/wso2-integrator-bi-artifacts/#event-integration","title":"Event Integration","text":"<p>Create an event-driven integration that is triggered by external events. These can include message brokers, third-party services, or cloud-based event sources.</p> <p>Supported event sources:</p> <ul> <li>Kafka</li> <li>RabbitMQ</li> <li>MQTT</li> <li>Azure Service Bus</li> <li>Salesforce</li> <li>GitHub</li> <li>Solace</li> </ul>"},{"location":"developer-guides/wso2-integrator-bi-artifacts/#file-integration","title":"File Integration","text":"<p>Create a file-based integration that reacts to the availability or changes in files within a file system or over FTP. This artifact type is useful for legacy systems or industries that rely on batch file exchanges.</p> <p>Supported file triggers:</p> <ul> <li>FTP services</li> <li>Directory services (local or mounted volumes)</li> </ul>"},{"location":"developer-guides/wso2-integrator-bi-artifacts/#other-artifacts","title":"Other Artifacts","text":"<p>Create supportive artifacts for your integration.</p> <p>Available artifact types:</p> <ul> <li>Function - Reusable code blocks that can be invoked within integrations</li> <li>Data Mapper - Transform data between different formats and structures</li> <li>Type - Define custom data types for use across integrations</li> <li>Connection - Configure reusable connection configurations to external systems</li> <li>Configuration - Manage environment-specific settings and parameters</li> </ul> <p>Each artifact type is designed to simplify the creation of integrations suited for a specific kind of use case or trigger. You can combine multiple artifacts within a single solution to cover a wide range of integration needs.</p>"},{"location":"developer-guides/ai-for-integration/build-an-http-service-with-wso2-copilot/","title":"Build an HTTP Service With BI Copilot","text":"<p>In this tutorial, you\u2019ll create an HTTP service to add key-value pairs to a Redis database. The integrated AI-assistant will help you generate the integration flow.</p>"},{"location":"developer-guides/ai-for-integration/build-an-http-service-with-wso2-copilot/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed on your machine.</li> </ul>"},{"location":"developer-guides/ai-for-integration/build-an-http-service-with-wso2-copilot/#step-1-create-a-new-integration-project","title":"Step 1: Create a new integration project","text":"<ol> <li>Click on the BI icon on the sidebar.</li> <li>Click on the Create New Integration button.</li> <li>Enter the project name as <code>RedisService</code>.</li> <li>Select project directory location by clicking on the Select Location button.</li> <li>Click on the Create New Integration button to create the integration project.</li> </ol>"},{"location":"developer-guides/ai-for-integration/build-an-http-service-with-wso2-copilot/#step-2-create-a-new-integration","title":"Step 2: Create a new integration","text":"<ol> <li>In the design view click on the Generate with AI button.</li> <li> <p>Enter the following prompt and press <code>Enter</code>:    <pre><code> Create an integration service with a base path of /cache and a POST resource at /add that accepts key-value pairs and adds them to Redis.\n</code></pre></p> <p></p> </li> <li> <p>Click on + Add to Integration button to add the generated integration to the project.</p> </li> <li> <p>The generated integration will look like below:  </p> <p></p> </li> </ol>"},{"location":"developer-guides/ai-for-integration/build-an-http-service-with-wso2-copilot/#step-3-add-a-resource-to-get-value","title":"Step 3: Add a resource to get value","text":"<ol> <li>Add the following prompt and press <code>Enter</code>:    <pre><code> Add a resource to get the value of a key from Redis.\n</code></pre></li> <li>Click on + Add to Integration button to add the generated integration to the project.</li> <li> <p>The generated integration will look like below:  </p> <p></p> </li> </ol>"},{"location":"developer-guides/ai-for-integration/build-an-http-service-with-wso2-copilot/#step-4-start-the-redis-server","title":"Step 4: Start the Redis server","text":"<ol> <li>Start the Redis server by running the following command:    <pre><code>docker run --name some-redis -d -p 6379:6379 redis\n</code></pre></li> <li> <p>The redis server will start on port <code>6379</code> without password protection. </p> <p></p> </li> </ol>"},{"location":"developer-guides/ai-for-integration/build-an-http-service-with-wso2-copilot/#step-6-configure-the-redis-client","title":"Step 6: Configure the Redis client","text":"<ol> <li>In the <code>Design View</code>, click on the Configure button on the top-right side.</li> <li>Set <code>redisHost</code> value to <code>localhost</code>.</li> <li>Set <code>redisPort</code> value to <code>6379</code>.   </li> </ol> <p>Note: No need to set the above values if the configurable variables are generated with default values.</p> <pre><code>  &lt;a href=\"https://wso2.github.io/docs-bi/assets/img/developer-guides/ai-for-integration/http-service-with-copilot/configuration.gif\"&gt;&lt;img src=\"https://wso2.github.io/docs-bi/assets/img/developer-guides/ai-for-integration/http-service-with-copilot/configuration.gif\" alt=\"Configurations\" width=\"70%\"&gt;&lt;/a&gt;</code></pre>"},{"location":"developer-guides/ai-for-integration/build-an-http-service-with-wso2-copilot/#step-5-generate-the-curl-commands","title":"Step 5: Generate the curl commands","text":"<ol> <li> <p>Add the following prompt and press <code>Enter</code> to generate the curl command to add key-value pairs to the Redis server.:    <pre><code> Generate a curl command to add key-value pairs to the Redis server.\n</code></pre></p> <p></p> </li> <li> <p>Add the following prompt and press <code>Enter</code> to generate the curl command to get the value of a key from the Redis server.:    <pre><code> Generate a curl command to get the value of a key from the Redis server.\n</code></pre></p> </li> </ol>"},{"location":"developer-guides/ai-for-integration/build-an-http-service-with-wso2-copilot/#step-6-test-the-integration","title":"Step 6: Test the integration","text":"<ol> <li>Click on the Run button to start the integration.</li> <li>Execute the generated <code>curl</code> commands to add a key-value pair.    <pre><code>   curl -X POST http://localhost:8080/cache/add \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"key\": \"BI\", \"value\": \"BI is an AI-assisted integration platform.\"}' \n</code></pre></li> <li>Execute the generated <code>curl</code> command to get the value of the key.    <pre><code>   curl http://localhost:8080/cache/get?key=BI\n</code></pre></li> <li>The response will be the value of the key <code>BI</code> stored in the Redis server.    <pre><code>BI is an AI-assisted integration platform.%\n</code></pre></li> </ol>"},{"location":"developer-guides/ai-for-integration/build-an-http-service-with-wso2-copilot/#step-7-stop-the-integration","title":"Step 7: Stop the integration","text":"<ol> <li>Click on the Stop button to stop the integration.</li> <li>Stop the Redis server by running the following command:    <pre><code>docker stop some-redis\n</code></pre></li> </ol>"},{"location":"developer-guides/debugging-and-troubleshooting/capturing-strand-dumps/","title":"Capturing Strand Dumps","text":"<p>The BI runtime can have unexpected behaviors due to user code errors, bugs, or issues with the running environment. These will result in memory leaks, CPU spinning, runtime hangs, performance degradation or crashing with various errors. This tool provides the capability to dump the status of currently running strands.</p>"},{"location":"developer-guides/debugging-and-troubleshooting/capturing-strand-dumps/#strand-dump","title":"Strand dump","text":"<p>Ballerina strand dump provides information on the available strands and strand groups during the execution of  a BI program. This can be used to:</p> <ul> <li> <p>troubleshoot runtime errors</p> </li> <li> <p>find data races, race conditions, livelocks, and deadlocks</p> </li> <li> <p>inspect strand and strand group status</p> </li> </ul> Note<p>Currently, this ability is only available in the operating systems in which the <code>SIGTRAP</code> POSIX signal is supported (<code>SIGTRAP</code> is not available on Windows).</p>"},{"location":"developer-guides/debugging-and-troubleshooting/capturing-strand-dumps/#get-the-strand-dump","title":"Get the strand dump","text":"<p>To get the strand dump when a BI program is running, you need to know the process ID (PID) of the BI  program. For that, you can use the <code>jps</code> tool. Then, you need to send the <code>SIGTRAP</code> signal to the process. The strand  dump will be produced to the standard output stream in the text format.</p> <p>For example, create a BI integration and copy the following content to the <code>main.bal</code> file via the file explorer view on VSCode.</p> <pre><code>import ballerina/lang.runtime;\nimport ballerina/io;\n\npublic function main() {\n    future&lt;int&gt; addResult = start addnum(1, 2);\n    int|error addition = wait addResult;\n    io:println(addition);\n}\n\nfunction addnum(int num1, int num2) returns int {\n\n    worker sender {\n        runtime:sleep(1000);\n        num1 -&gt; receiver;\n    }\n\n    worker receiver returns int {\n        int firstNum = &lt;- sender;\n        return num2 + firstNum;\n    }\n\n    int intResult = wait receiver;\n    return intResult;\n}\n</code></pre> <p>Run this BI project using <code>run</code> option in BI.</p> <pre><code>Compiling source\n    demo/strandDump:0.1.0\n\nRunning executable\n</code></pre> <p>Obtain its PID while the program is running. <pre><code>$ jps\n3408 Main\n28851 Jps\n28845 $_init\n</code></pre></p> <p>You get the PID for this program as 28845 because <code>$_init</code> is the main class of the Ballerina program.</p> Note<p>If you run the tests in a Ballerina package or a file using the <code>bal test</code> command, you need to get the PID of the process denoted by the <code>BTestMain</code> classname.</p> <p>To get the strand dump, send the <code>SIGTRAP</code> signal to that process. You can use the following CLI command. <pre><code>$ kill -SIGTRAP 28845\n</code></pre> or <pre><code>$ kill -5 28845\n</code></pre></p> <p>Then, the dump of the runtime strands will be emitted to the standard output stream of the Ballerina program.  For example, see the sample below. <pre><code>Ballerina Strand Dump [2022/10/12 12:08:02]\n===========================================\n\nTotal strand group count        :       5\nTotal strand count              :       5\nActive strand group count       :       2\nActive strand count             :       4\n\ngroup 4 [QUEUED]: [1]\n        strand 2 \"main\" [demo.strandDump.0:main] [WAITING]:\n                at      demo.strandDump.0.1.0:main(main.bal:6)\n\ngroup 5 [QUEUED]: [3]\n        strand 3 \"addResult\" [demo.strandDump.0:main][2] [WAITING]:\n                at      demo.strandDump.0.1.0:addnum(main.bal:22)\n\n        strand 4 \"sender\" [demo.strandDump.0:addnum][3] [BLOCKED]:\n                at      ballerina.lang.runtime.0.0.0:sleep(runtime.bal:61)\n                        demo.strandDump.0.1.0:$lambda$_0(main.bal:13)\n\n        strand 5 \"receiver\" [demo.strandDump.0:addnum][3] [BLOCKED ON WORKER MESSAGE RECEIVE]:\n                at      demo.strandDump.0.1.0:$lambda$_1(main.bal:18)\n\n===========================================\n</code></pre></p>"},{"location":"developer-guides/debugging-and-troubleshooting/capturing-strand-dumps/#output-format-and-available-details","title":"Output format and available details","text":"<p>The strand dump contains the following information.</p> <ul> <li> <p>the date and the time when the strand dump was obtained</p> </li> <li> <p>the total number of strand groups and strands created in the program</p> </li> <li> <p>the active number of strand groups and strands in the program</p> </li> </ul> <p>The details on the active strand groups and strands are given in the following format.</p> <p></p> Label Description Strand group ID A unique ID given to a particular strand group. A strand group comprises a set of strands that run on the same thread. Strand group state Current state of the strand group. For the available states, see Strand group states. The current number of strands in the strand group A strand group consists of one or more strands. Only one of them runs on a thread at a time. Strand ID A unique ID given to a particular strand. Strand name Name of the strand associated with the strand ID. This is optional and will be omitted if not available. Strand initiated module Name of the module, which created the strand. Strand initiated function Name of the function, which created the strand. Parent strand ID ID of the parent strand. This will be omitted if there is no parent strand. Strand state Current state of the strand. For the available states, see Strand states. Strand yielded location stack trace The stack trace, which points to the location where the strand is blocked (yielded). This is omitted if the state is <code>RUNNABLE</code> or <code>DONE</code>. A line in the stack trace is given by the format: <code>module name:function name(filename:line number)</code>"},{"location":"developer-guides/debugging-and-troubleshooting/capturing-strand-dumps/#strand-group-states","title":"Strand group states","text":"State Description RUNNABLE Strand group is ready to run or is currently running. QUEUED Strand group execution is blocked or completed or it comprises a new set of strands that are not yet scheduled to run."},{"location":"developer-guides/debugging-and-troubleshooting/capturing-strand-dumps/#strand-states","title":"Strand states","text":"State Description WAITING FOR LOCK Strand is waiting to acquire a lock. BLOCKED ON WORKER MESSAGE SEND Strand is blocked due to the <code>sync send</code> action. BLOCKED ON WORKER MESSAGE RECEIVE Strand is blocked due to the <code>receive</code> action. BLOCKED ON WORKER MESSAGE FLUSH Strand is blocked due to the <code>flush</code> action. WAITING Strand is blocked due to the <code>wait</code> action. BLOCKED Strand is blocked due to any other reason than the above. E.g., sleep, external function call, etc. RUNNABLE Strand is ready to run or is currently running. DONE Strand execution is completed."},{"location":"developer-guides/debugging-and-troubleshooting/debugging-within-the-editor/","title":"Debugging Within the Editor","text":"<p>BI provides multiple options to debug integrations, and the most convenient way is to use the <code>Debug Integration</code> option located in the top right corner of the editor.</p> <p>Follow these steps to quickly start a debug session.</p> <p>1. Open your BI project and navigate to the relevant integration flow.</p> <p>2. Add debug points by clicking on the required nodes in your integration flow.</p> <p>3. Start debugging by clicking the <code>Debug Integration</code> button on the top right corner of the editor.</p> <p>The program will pause at your debug points, allowing you to inspect variables, check the program status, and step through your integration logic interactively.</p> <p>This integrated debugging experience helps you quickly identify and resolve issues directly within your development environment.</p> <p></p> Note<p>For advanced scenarios, use the launch.json configuration file to launch debug sessions with additional settings, such as program arguments and environment variables. For details, see Debug using configurations.</p>"},{"location":"developer-guides/debugging-and-troubleshooting/overview/","title":"Debugging &amp; Troubleshooting","text":"<p>The Ballerina compiler is especially useful in BI development, where large-scale applications with complex logic are common, as it helps detect both syntax and semantic issues early in the coding process. However, it is difficult for the compiler to detect runtime errors like logical errors because they occur during the program execution after a successful compilation. This is where the dedicated debugging tooling support of Ballerina becomes important.</p>"},{"location":"developer-guides/debugging-and-troubleshooting/overview/#debugging-sessions","title":"Debugging sessions","text":"<p>BI offers three types of debugging sessions (i.e., program debugging, test debugging, and remote debugging). Debugging sessions can be initiated using CodeLens or configurations in a <code>launch.json file</code>. It also provides a <code>Debug Console</code> to view the output and perform various debugging scenarios.</p> <p>Refer to the following sections for more information on debugging sessions.</p> <ul> <li>Debugging Within Editor</li> <li>Remotely Debugging Integrations</li> </ul> Info<p>For more information on the debugging sessions and methods, go to Debugging Sessions.</p>"},{"location":"developer-guides/debugging-and-troubleshooting/overview/#debugging-features","title":"Debugging features","text":"<p>BI provides a range of powerful debugging features. You can set breakpoints with conditions and logpoints, pause and continue program execution for precise inspection, evaluate expressions at runtime, and view call stacks and strands. These features enhance the debugging experience, enabling effective troubleshooting and analysis of Ballerina code.</p> <p>You can set break points for a node in BI by  Clicking on the three dots that appear in right hand side of the node and select <code>Add Breakpoint</code> from the menu.</p> Info<p>For detailed information on the feature-rich debugging experience for troubleshooting BI applications, go to Debugging Features.</p>"},{"location":"developer-guides/debugging-and-troubleshooting/overview/#debugging-configurations","title":"Debugging configurations","text":"<p>The debugger allows you to create a <code>launch.json</code> file with default configurations for debugging BI programs. You can generate the file by following a few steps and then, modify the configurations to suit your needs. These configurations have specific attributes that can be edited to customize the debugging process so that you can set program arguments, command options, environment variables, run in a separate terminal, etc. Additionally, you can configure remote debugging by specifying the host address and port number. These configurations provide flexibility and control when debugging Ballerina code in VS Code.</p> Info<p>For more information on the debugging configurations, go to Debugging Configurations.</p>"},{"location":"developer-guides/debugging-and-troubleshooting/profiling-runtime-performance/","title":"Profiling Runtime Performance","text":"<p>Improvements in your BI implementation might be required to increase its runtime performance, minimize code execution times, increase throughput, and reduce latency. To do that, you need to identify performance bottlenecks, implications of concurrent executions, which code segments take longer to execute, and areas to improve performance. This can be done by using a profiler.</p> <p>Profiler is a tool that monitors the BI runtime and its operations such as function calls. It can be used to understand the behavior and troubleshoot the performance issues of a Ballerina program and optimize it.</p> Note<p>Profiler is an experimental feature, which supports only a limited set of functionality. The commands associated with the tool might change in future releases.</p>"},{"location":"developer-guides/debugging-and-troubleshooting/profiling-runtime-performance/#features","title":"Features","text":"<ul> <li> <p>Profile a local session of a BI program through offline instrumentation.</p> </li> <li> <p>Perform CPU profiling, which finds out where the CPU time is going.</p> </li> <li> <p>Generate a flame graph, which shows the function call stack and the execution times of each function call. It provides an interface for viewing runtime performance and the ability to search, diagnose, and find performance bottlenecks.</p> </li> </ul> Note<p>Profiling is a high-powered activity with a heavy overhead and can slow down your application.</p>"},{"location":"developer-guides/debugging-and-troubleshooting/profiling-runtime-performance/#profile-a-bi-program","title":"Profile a BI program","text":"<p>To profile a Ballerina package, you can use the <code>bal profile</code> CLI command inside the root directory of the BI project.</p> <p>Consider the following step-by-step guide to profile a Ballerina package.</p>"},{"location":"developer-guides/debugging-and-troubleshooting/profiling-runtime-performance/#step-1-create-a-bi-project","title":"Step 1: Create a BI project","text":"<ul> <li> <p>Create a BI project named <code>sort</code></p> </li> <li> <p>Go to the file explorer view and replace the contents of the <code>main.bal</code> file with the following Ballerina code, which creates an array of random integers, sorts them, and verifies the output.</p> </li> </ul> <pre><code>import ballerina/io;\nimport ballerina/random;\n\npublic function main() returns error? {\n    int[] arr = check createRandomIntArray(check float:pow(10, 2).cloneWithType(int));\n    int[] sortedArr = bubbleSort(arr);\n    boolean isSorted = isSortedArray(sortedArr);\n    io:println(\"Is the array sorted? \" + isSorted.toString());\n}\n\npublic isolated function bubbleSort(int[] arr) returns int[] {\n    int n = arr.length();\n    int temp = 0;\n    boolean swapped = false;\n    foreach int i in 0 ... n - 2 {\n        foreach int j in 1 ... n - 1 - i {\n            if (arr[j - 1] &gt; arr[j]) {\n                temp = arr[j - 1];\n                arr[j - 1] = arr[j];\n                arr[j] = temp;\n                swapped = true;\n            }\n        }\n        if (!swapped) {\n            break;\n        }\n    }\n    return arr;\n}\n\nisolated function isSortedArray(int[] sortedArr) returns boolean {\n    foreach int i in 0 ..&lt; sortedArr.length() - 1 {\n        if (sortedArr[i] &gt; sortedArr[i + 1]) {\n            return false;\n        }\n    }\n    return true;\n}\n\nisolated function createRandomIntArray(int size) returns int[]|error {\n    int[] array = [];\n    foreach int i in 0 ..&lt; size {\n        array.push(check random:createIntInRange(0, int:MAX_VALUE));\n    }\n    return array;\n}\n</code></pre>"},{"location":"developer-guides/debugging-and-troubleshooting/profiling-runtime-performance/#step-2-run-the-profiler-to-get-the-profile-details","title":"Step 2: Run the profiler to get the profile details","text":"<ul> <li>Run and profile the Ballerina package using the <code>bal profile</code> CLI command via the VSCode terminal.</li> </ul> <pre><code>$ bal profile\n</code></pre> <p>You view the output below.</p> <pre><code>Compiling source\n        profiler_demo/sort:0.1.0\n\nGenerating executable\n        target/bin/sort.jar\n\n================================================================================\nBallerina Profiler: Profiling...\n================================================================================\nNote: This is an experimental feature, which supports only a limited set of functionality.\n[1/6] Initializing...\n[2/6] Copying executable...\n[3/6] Performing analysis...\n[4/6] Instrumenting functions...\n \u25cb Instrumented module count: 31\n \u25cb Instrumented function count: 1016\n[5/6] Running executable...\nIs the array sorted? true\n[6/6] Generating output...\n \u25cb Execution time: 3 seconds \n \u25cb Output: target/bin/ProfilerOutput.html\n-------------------------------------------------------------------------------\n</code></pre>"},{"location":"developer-guides/debugging-and-troubleshooting/profiling-runtime-performance/#step-3-examine-the-profile-details","title":"Step 3: Examine the profile details","text":"<ul> <li>Open the <code>target/bin/ProfilerOutput.html</code> file using a web browser window to examine the Ballerina Profiler output to find the slow-running functions and performance bottlenecks, which can then be addressed to improve the program\u2019s overall performance.</li> </ul>"},{"location":"developer-guides/debugging-and-troubleshooting/profiling-runtime-performance/#ballerina-profiler-output-flame-graph-content-and-features","title":"Ballerina profiler output: Flame graph content and features","text":"<p>The flame graph generated by the Ballerina Profiler contains the following details.</p> <ul> <li> <p>Function call stack</p> </li> <li> <p>Time taken to execute each function and its percentage</p> </li> </ul> <p></p> <p>Flame graph also has the following features.</p> <ul> <li>Search a function.</li> </ul> <p></p> <ul> <li> <p>Clear the search.</p> </li> <li> <p>Click on a function call and zoom the view.</p> </li> </ul> <p></p> <ul> <li>Reset the view.</li> </ul>"},{"location":"developer-guides/debugging-and-troubleshooting/profiling-runtime-performance/#examine-the-output-and-find-performance-bottlenecks","title":"Examine the output and find performance bottlenecks","text":"<p>When you observe the flame graph generated by the Ballerina Profile for the above code, you can see the <code>bubbleSort</code> function has taken the most time to execute.</p> <p></p> <p>Let's try to optimize the sorting function to increase the performance of the Ballerina application.</p>"},{"location":"developer-guides/debugging-and-troubleshooting/profiling-runtime-performance/#optimize-the-ballerina-code","title":"Optimize the Ballerina code","text":"<p>In this example, you will use another sorting algorithm to reduce the time taken to execute the application. Replace the <code>main.bal</code> file with the code below, which uses the merge sort algorithm instead of the bubble sort algorithm for sorting.</p> <pre><code>import ballerina/io;\nimport ballerina/random;\n\npublic function main() returns error? {\n    int[] arr = check createRandomIntArray(check float:pow(10, 2).cloneWithType(int));\n    int[] sortedArr = mergeSort(arr);\n    boolean isSorted = isSortedArray(sortedArr);\n    io:println(\"Is the array sorted? \" + isSorted.toString());\n}\n\npublic isolated function mergeSort(int[] arr) returns int[] {\n    int n = arr.length();\n    int width = 1;\n    while (width &lt; n) {\n        int l = 0;\n        while (l &lt; n) {\n            int r = int:min(l + (width * 2 - 1), n - 1);\n            int m = int:min(l + width - 1, n - 1);\n            merge(arr, l, m, r);\n            l += width * 2;\n        }\n        width *= 2;\n    }\n    return arr;\n}\n\nisolated function merge(int[] a, int l, int m, int r) {\n    int n1 = m - l + 1;\n    int n2 = r - m;\n    int[] L = [];\n    int[] R = [];\n    foreach int i in int:range(0, n1, 1) {\n        L[i] = a[l + i];\n    }\n    foreach int i in int:range(0, n2, 1) {\n        R[i] = a[m + i + 1];\n    }\n    int i = 0;\n    int j = 0;\n    int k = l;\n    while (i &lt; n1 &amp;&amp; j &lt; n2) {\n        if L[i] &lt;= R[j] {\n            a[k] = L[i];\n            i += 1;\n        } else {\n            a[k] = R[j];\n            j += 1;\n        }\n        k += 1;\n    }\n    while (i &lt; n1) {\n        a[k] = L[i];\n        i += 1;\n        k += 1;\n    }\n    while (j &lt; n2) {\n        a[k] = R[j];\n        j += 1;\n        k += 1;\n    }\n}\n\nisolated function isSortedArray(int[] sortedArr) returns boolean {\n    foreach int i in 0 ..&lt; sortedArr.length() - 1 {\n        if (sortedArr[i] &gt; sortedArr[i + 1]) {\n            return false;\n        }\n    }\n    return true;\n}\n\nisolated function createRandomIntArray(int size) returns int[]|error {\n    int[] array = [];\n    foreach int i in 0 ..&lt; size {\n        array.push(check random:createIntInRange(0, int:MAX_VALUE));\n    }\n    return array;\n}\n</code></pre> <p>When you profile the code again, you can see that you have reduced the time taken for sorting substantially and have improved the performance of the application.</p> <p></p>"},{"location":"developer-guides/debugging-and-troubleshooting/profiling-runtime-performance/#profile-a-ballerina-service","title":"Profile a Ballerina service","text":"<p>Consider the following step-by-step guide to profile a BI project that contains an HTTP service.</p> <p>1. Create a new BI project and replace the <code>main.bal</code> file with the below code.</p> <pre><code>import ballerina/http;\n\ntype Country record {\n    string country;\n    int population;\n    string continent;\n    int cases;\n    int deaths;\n};\n\ntype Data record {|\n    string country;\n    string continent;\n    int population;\n    decimal caseFatalityRatio;\n|};\n\nhttp:Client diseaseEp = check new (\"https://disease.sh/v3\");\nfinal Country[] &amp; readonly countries;\n\nfunction init() returns error? {\n    countries = check diseaseEp-&gt;/covid\\-19/countries;\n}\n\nservice /covid19/countries on new http:Listener(8080) {\n    resource function get summary() returns json {\n        Data[] listResult = from var {country, continent, population, cases, deaths} in countries\n            where hasSignificantPopulation(population, deaths)\n            let decimal caseFatalityRatio = &lt;decimal&gt;deaths / &lt;decimal&gt;cases * 100\n            order by caseFatalityRatio descending\n            limit 1000\n            select {country, continent, population, caseFatalityRatio};\n        return getTopSortedValues(listResult);\n    }\n\n    isolated resource function get names() returns json {\n        return from var {country, population, deaths} in countries\n            where hasNonZeroPopulation(population, deaths)\n            select {country};\n    }\n}\n\nisolated function getTopSortedValues(Data[] listResult) returns json[] {\n    json[] sortedArray = listResult.sort(\"ascending\", getKey);\n    return getTopElements(sortedArray);\n}\n\nisolated function getTopElements(json[] sortedArray) returns json[] {\n    return from var i in sortedArray\n        limit 10\n        select i;\n}\n\nisolated function hasSignificantPopulation(int population, int deaths) returns boolean {\n    return population &gt;= 100 &amp;&amp; deaths &gt;= 10;\n}\n\nisolated function hasNonZeroPopulation(int population, int deaths) returns boolean {\n    return population &gt;= 0 &amp;&amp; deaths &gt;= 0;\n}\n\nisolated function getKey(record {|string country; string continent; int population; decimal caseFatalityRatio;|} recordVal) returns string {\n    return recordVal.country;\n}\n</code></pre> <p>2. Run the CLI command <code>bal profile</code> in the root directory to run the BI project and profile it. This will run the service while profiling it.</p> <pre><code>$ bal profile\nCompiling source\n        profiler_demo/covid19_stats:0.1.0\n\nGenerating executable\n        target/bin/covid19_stats.jar\n\n================================================================================\nBallerina Profiler: Profiling...\n================================================================================\nNote: This is an experimental feature, which supports only a limited set of functionality.\n[1/6] Initializing...\n[2/6] Copying executable...\n[3/6] Performing analysis...\n[4/6] Instrumenting functions...\n \u25cb Instrumented module count: 44\n \u25cb Instrumented function count: 2935\n[5/6] Running executable...\n</code></pre> <p>3. Send 10 requests to each resource endpoint using the <code>curl</code> command.</p> <pre><code>$ curl localhost:8080/covid19/countries/names\n</code></pre> <pre><code>$ curl localhost:8080/covid19/countries/summary\n</code></pre> <p>4. Stop the service by sending the <code>SIGINT</code> signal to the program. You can use <code>Ctrl+C</code> in the service running terminal to send the signal to it. This will stop both the  program and profiling it and will generate the profiler output.</p> <pre><code>[6/6] Generating output...\n \u25cb Execution time: 58 seconds \n \u25cb Output: target/bin/ProfilerOutput.html\n--------------------------------------------------------------------------------\n</code></pre> <p>5. Open the <code>target/bin/ProfilerOutput.html</code> file using a web browser to observe the flame graph generated by the profiler.</p> <p></p> <p>By observing the flame graph you can see that the <code>get summary</code> resource function takes more time than the <code>get names</code> resource function. You can use the profiler output data to improve the performance of the Ballerina service.</p>"},{"location":"developer-guides/debugging-and-troubleshooting/remote-debugging-integrations/","title":"Remote Debugging Integrations","text":"<p>You can debug an already running integration by attaching the debugger remotely. Follow the steps below to start a remote debug session:</p> <p>1. Go to the <code>Debug view</code>.</p> <ul> <li>Press Ctrl + Shift + D (or \u2318 + \u21e7 + D on macOS) to open the Debug view.</li> </ul> <p>2. Open the <code>launch.json</code> file and configure the <code>debuggeeHost</code> and <code>debuggeePort</code> attributes under the <code>Ballerina Remote</code> configuration section.</p> <p>3. Select the remote debug configuration.    In the upper left corner, choose <code>Ballerina Remote</code> from the configuration dropdown.</p> <p>4. Start the integration in debug mode.    Open a terminal and run the appropriate command for your scenario:</p> Command Description <code>bal run --debug &lt;DEBUGGEE_PORT&gt; &lt;BAL_FILE_PATH/PACKAGE_PATH&gt;</code> Debug a Ballerina package or file <code>bal run --debug &lt;DEBUGGEE_PORT&gt; &lt;EXECUTABLE_JAR_FILE_PATH&gt;</code> Debug a Ballerina executable JAR <code>bal test --debug &lt;DEBUGGEE_PORT&gt; &lt;PACKAGE_PATH&gt;</code> Debug Ballerina tests <p>Once started, you should see output similar to:</p> <pre><code>Listening for transport dt_socket at address: 5005\n</code></pre> <p>5. Start the debug session.    Click the <code>Start Debugging</code> icon in the upper left of the editor.    You can now debug your running integration, and debug output will appear in the <code>DEBUG CONSOLE</code>.</p>"},{"location":"developer-guides/protocols-and-connectors/build-custom-connectors/","title":"Build Custom Connectors","text":"<p>APIs power many of the digital services we use daily such as notifications, SMS alerts, reminders, and transactions. These services often expose hundreds of operations via their APIs, making manual coding of client logic tedious and error-prone.</p> <p>To simplify this, BI allows you to generate custom connectors automatically from a valid OpenAPI specification. </p>"},{"location":"developer-guides/protocols-and-connectors/build-custom-connectors/#steps-to-generate-a-custom-connector","title":"Steps to generate a custom connector","text":"<ol> <li> <p>Select Add Custom Connector from the left-hand panel</p> <p></p> </li> <li> <p>Click on Generate a connector</p> <p></p> </li> <li> <p>Name and Select OpenAPI Spec file</p> <p></p> </li> </ol>"},{"location":"developer-guides/protocols-and-connectors/overview-of-connectors/","title":"Overview of Connectors","text":"<p>BI supports a wide range of prebuilt connectors to enable seamless integration across databases, messaging systems, cloud services, SaaS platforms, and networking protocols. These connectors are built using the Ballerina language and allow you to interact with external systems with minimal effort, improving development speed and reducing integration complexity.</p> <p>The following categories summarize the available connectors.</p>"},{"location":"developer-guides/protocols-and-connectors/overview-of-connectors/#network-connectors","title":"\ud83c\udf10 Network Connectors","text":"<p>BI provides multiple connectors to interact over common network protocols such as HTTP, TCP, or FTP. Frequently used options include HTTP (<code>ballerina/http</code>), GraphQL (<code>ballerina/graphql</code>), WebSocket (<code>ballerina/websocket</code>), and FTP (<code>ballerina/ftp</code>). These allow you to expose services or connect to remote endpoints using a variety of communication protocols.</p>"},{"location":"developer-guides/protocols-and-connectors/overview-of-connectors/#database-connectors","title":"\ud83d\uddc3\ufe0f Database Connectors","text":"<p>To handle structured and unstructured data, BI offers connectors to popular relational and NoSQL databases. Some examples include MySQL (<code>ballerinax/mysql</code>), MongoDB (<code>ballerinax/mongodb</code>), MS SQL Server (<code>ballerinax/mssql</code>), and Redis (<code>ballerinax/redis</code>). Additional connectors exist for other specialized or cloud-native databases.</p>"},{"location":"developer-guides/protocols-and-connectors/overview-of-connectors/#messaging-connectors","title":"\ud83d\udce9 Messaging Connectors","text":"<p>BI includes support for various message brokers and streaming platforms. Popular messaging connectors include Kafka Consumer and Producer (<code>ballerinax/kafka</code>) and RabbitMQ (<code>ballerinax/rabbitmq</code>). These connectors help implement asynchronous communication patterns for scalable system designs.</p>"},{"location":"developer-guides/protocols-and-connectors/overview-of-connectors/#cloud-connectors","title":"\u2601\ufe0f Cloud Connectors","text":"<p>Connectors are available for accessing storage, queueing, and database services in cloud environments. Common options include Amazon S3 (<code>ballerinax/aws.s3</code>), Gmail (<code>ballerinax/googleapis.gmail</code>), and Google Calendar (<code>ballerinax/googleapis.gcalendar</code>). BI also supports many other connectors for cloud-native services across different providers.</p>"},{"location":"developer-guides/protocols-and-connectors/overview-of-connectors/#saas-connectors","title":"\ud83e\udde9 SaaS Connectors","text":"<p>BI provides connectors for integrating with widely used SaaS platforms. These enable smooth data flow between your integration and business-critical tools like Salesforce (<code>ballerinax/salesforce</code>), Twilio (<code>ballerinax/twilio</code>), GitHub (<code>ballerinax/github</code>), and Stripe (<code>ballerinax/stripe</code>). Many additional SaaS connectors are available to meet your integration needs.</p>"},{"location":"developer-guides/protocols-and-connectors/overview-of-connectors/#ai-connectors","title":"\ud83e\udd16 AI Connectors","text":"<p>BI provides connectors to incorporate large language models (LLMs), embeddings, image generation, vector search, and retrieval-augmented generation (RAG) directly into the integration workflow, such as OpenAI (<code>ballerinax/openai.chat</code>), Pinecone vector database (<code>ballerinax/pinecone.vector</code>), etc. </p>"},{"location":"developer-guides/protocols-and-connectors/overview-of-connectors/#using-connectors-in-bi","title":"Using Connectors in BI","text":"<p>To use a connector in your integration:</p> <ol> <li>Add it from the left panel under Connectors.</li> <li>Configure authentication and connection parameters in the connector setup view.</li> <li>Use drag-and-drop or inline coding to invoke operations exposed by the connector.</li> </ol> <p>Each connector simplifies access to complex services through a well-defined, auto-documented interface.</p>"},{"location":"developer-guides/protocols-and-connectors/supported-protocols/","title":"Supported Protocols","text":"Protocol Description Documentation HTTP HTTP client/server functionalities to produce and consume HTTP APIs <ul> <li> HTTP client </li> <li> HTTP listener </li> </ul> gRPC gRPC client/server functionalities to produce and consume gRPC APIs <ul> <li> gRPC client </li> <li> gRPC listener </li> </ul> WebSocket WebSocket client/server functionalities to produce and consume WebSocket APIs <ul> <li> WebSocket client </li> <li> WebSocket listener </li> </ul> WebSub APIs for the functionalities of the WebSub subscriber <ul><li> WebSub listener</li></ul> WebSubHub APIs for the functionalities of the WebSubHub and WebSub publisher <ul><li> WebSubHub listener </li><li> WebSubHub publsiher client GraphQL GraphQL client/server functionalities to produce and consume GraphQL APIs <ul> <li> GraphQL client </li> <li> GraphQL listener </li> </ul> TCP Send and receive messages to/from another application process (local or remote) over the connection-oriented TCP protocol <ul> <li> TCP client </li> <li> TCP listener </li> </ul> FTP/SFTP FTP/SFTP client/server functionalities to facilitate file handling in a remote file system <ul> <li> FTP client </li> <li> FTP listener </li> </ul> SMTP Send emails via the SMTP protocol using the SMTP client <ul> <li> SMTP client </li> </ul> POP3 Receive emails via the POP3 protocol using both clients and services <ul> <li> POP3 client </li> <li> POP3 listener </li> </ul> IMAP4 Receive emails via the IMAP4 protocol using both clients and services <ul> <li> IMAP4 client </li> <li> IMAP4 listener </li> </ul> JMS Send and receive messages by connecting to a JMS provider <ul> <li> Message producer </li> <li> Message consumer </li> <li> Message listener </li> </ul> AMQP Send and receive messages by connecting to the RabbitMQ server <ul> <li> RabbitMQ client </li> <li> RabbitMQ listener </li> </ul> AWS SQS Perform operations related to queues and messages by connecting to AWS SQS <ul> <li> AWS SQS client </li> </ul> MQTT Publish and subscribe messages by connecting to an MQTT server <ul> <li> MQTT client </li> <li> MQTT listener </li> </ul> SOAP Send and receive messages by connecting to a SOAP service <ul> <li> SOAP client </li> </ul>"},{"location":"developer-guides/tools/integration-tools/edi-tool/","title":"EDI Tool","text":"<p>The EDI tool provides the below set of command line tools to work with EDI files in BI.</p>"},{"location":"developer-guides/tools/integration-tools/edi-tool/#install-the-tool","title":"Install the tool","text":"<p>Execute the command in VSCode terminal below to pull the EDI tool from Ballerina Central.</p> <pre><code>$ bal tool pull edi\n</code></pre>"},{"location":"developer-guides/tools/integration-tools/edi-tool/#usage","title":"Usage","text":"<p>The tool supports three main usages, as follows:</p> <ul> <li>Code generation: Generate Ballerina records and parser functions for a given EDI schema.</li> <li>Package generation: Generates Ballerina records, parser functions, utility methods, and a REST connector for a given collection of EDI schemas and organizes those as a Ballerina package.</li> <li>Schema conversion: Convert various EDI schema formats to Ballerina EDI schema format.</li> </ul>"},{"location":"developer-guides/tools/integration-tools/edi-tool/#define-the-edi-schema","title":"Define the EDI schema","text":"<p>Prior to utilizing the EDI tools, it is crucial to define the structure of the EDI data meant for import. Developers have the option to utilize the Ballerina EDI Schema Specification for guidance. This specification outlines the essential components required to describe an EDI schema, encompassing attributes such as name, delimiters, segments, field definitions, components, sub-components, and additional configuration options.</p> <p>As an illustrative example, consider the following EDI schema definition for a <code>simple order</code>, assumed to be stored as <code>schema.json</code>:</p> <pre><code>{\n\"name\": \"SimpleOrder\",\n\"delimiters\" : {\"segment\" : \"~\", \"field\" : \"*\", \"component\": \":\", \"repetition\": \"^\"},\n\"segments\" : [\n{\n\"code\": \"HDR\",\n\"tag\" : \"header\",\n\"minOccurances\": 1,\n\"fields\" : [{\"tag\": \"code\"}, {\"tag\" : \"orderId\"}, {\"tag\" : \"organization\"}, {\"tag\" : \"date\"}]\n},\n{\n\"code\": \"ITM\",\n\"tag\" : \"items\",\n\"maxOccurances\" : -1,\n\"fields\" : [{\"tag\": \"code\"}, {\"tag\" : \"item\"}, {\"tag\" : \"quantity\", \"dataType\" : \"int\"}]\n}\n]\n}\n</code></pre> <p>This schema can be employed to parse EDI documents featuring one HDR segment, mapped to the <code>header</code>, and any number of ITM segments, mapped to <code>items</code>. The HDR segment incorporates three <code>fields</code>, corresponding to orderId, organization, and date. Each ITM segment comprises two fields, mapped to item and quantity.</p> <p>Below is an example of an EDI document that can be parsed using the aforementioned schema. Let's assume that the following EDI information is saved in a file named <code>sample.edi</code>:</p> <p><pre><code>HDR*ORDER_1201*ABC_Store*2008-01-01~\nITM*A-250*12~\nITM*A-45*100~\nITM*D-10*58~\nITM*K-80*250~\nITM*T-46*28~\n</code></pre> If you already have an existing X12, EDIFACT, or ESL schema file, you can convert it to the Ballerina EDI schema using the EDI tool's schema-conversion capabilities.</p>"},{"location":"developer-guides/tools/integration-tools/edi-tool/#code-generation","title":"Code generation","text":"<p>The below command can be used to generate typed Ballerina records and parser functions for a given EDI schema.</p> <pre><code>$ bal edi codegen -i &lt;input schema path&gt; -o &lt;output path&gt;\n</code></pre> <p>The above command generates all Ballerina records and parser functions required for working with data in the given EDI schema and writes those into the file specified in the <code>output path</code>. The generated parser function (i.e., <code>fromEdiString(...)</code>) can read EDI text files into generated records, which can be accessed from the Ballerina code, similar to accessing any other Ballerina record. Similarly, the generated serialization function (i.e., <code>toEdiString(...)</code>) can serialize generated Ballerina records into EDI text.</p>"},{"location":"developer-guides/tools/integration-tools/edi-tool/#command-options-for-codegen","title":"Command options for <code>codegen</code>","text":"Command option Description Mandatory/Optional <code>-i, --input</code> Path to the EDI schema file. Mandatory <code>-o, --output</code> Path to the output file. Mandatory"},{"location":"developer-guides/tools/integration-tools/edi-tool/#code-generation-example","title":"Code generation example","text":"<p>1. Create a new BI project.</p> <p>2. Create a new folder named resources in the root of the project and copy the <code>schema.json</code> and <code>sample.edi</code> files into it. </p> <p>3. Ballerina records for the EDI schema in <code>resources/schema.json</code> can be generated as follows (generated Ballerina records will be saved in <code>orders.bal</code>).</p> <p>Run the below command from the project root directory to generate the Ballerina parser for the above schema.</p> <pre><code>$ bal edi codegen -i resources/schema.json -o orders.bal\n</code></pre> <p>Generated Ballerina records for the above schema are shown below:</p> <pre><code>public type Header_Type record {|\n   string code = \"HDR\";\n   string orderId?;\n   string organization?;\n   string date?;\n|};\n\npublic type Items_Type record {|\n   string code = \"ITM\";\n   string item?;\n   int? quantity?;\n|};\n\npublic type SimpleOrder record {|\n   Header_Type header;\n   Items_Type[] items = [];\n|};\n</code></pre>"},{"location":"developer-guides/tools/integration-tools/edi-tool/#reading-edi-files","title":"Reading EDI files","text":"<p>The generated <code>fromEdiString</code> function can be used to read EDI text files into the generated Ballerina record, as shown below. Note that any data item in the EDI can be accessed using the record's fields, as shown in the example code.</p> <pre><code>import ballerina/io;\n\npublic function main() returns error? {\n    string ediText = check io:fileReadString(\"resources/sample.edi\");\n    SimpleOrder sample_order = check fromEdiString(ediText);\n    io:println(sample_order.header.date);\n}\n</code></pre>"},{"location":"developer-guides/tools/integration-tools/edi-tool/#writing-edi-files","title":"Writing EDI files","text":"<p>The generated <code>toEdiString</code> function can be used to serialize <code>SimpleOrder</code> records into EDI text, as shown below:</p> <pre><code>import ballerina/io;\n\npublic function main() returns error? {\n    SimpleOrder simpleOrder = {header: {code: \"HDR\", orderId: \"ORDER_200\", organization: \"ABC_Store\", date: \"17-05-2024\"}};\n    simpleOrder.items.push({code: \"ITM\", item: \"A680\", quantity: 15}); \n    simpleOrder.items.push({code: \"ITM\", item: \"A530\", quantity: 2}); \n    simpleOrder.items.push({code: \"ITM\", item: \"A500\", quantity: 4});\n    string ediText = check toEdiString(simpleOrder);\n    io:println(ediText);\n}\n</code></pre>"},{"location":"developer-guides/tools/integration-tools/edi-tool/#package-generation","title":"Package generation","text":"<p>Usually, organizations have to work with many EDI formats, and integration developers need to have a convenient way to work on EDI data with minimum effort. Ballerina EDI packages facilitate this by allowing organizations to pack all EDI processing codes for their EDI collections into an importable package. Therefore, integration developers can simply import those packages and convert EDI messages into Ballerina records in a single line of code.</p> <p>The below command can be used to generate Ballerina records, parser and util functions, and a REST connector for a given collection of EDI schemas organized into a Ballerina package:</p> <pre><code>$ bal edi libgen -p &lt;organization-name/package-name&gt; -i &lt;input schema folder&gt; -o &lt;output folder&gt;\n</code></pre> <p>The Ballerina package will be generated in the output folder. This package can be built and published by issuing <code>bal pack</code> and <code>bal push</code> commands from the output folder. Then, the generated package can be imported into any BI project, and the generated utility functions of the package can be invoked to parse EDI messages into Ballerina records. </p>"},{"location":"developer-guides/tools/integration-tools/edi-tool/#command-options-for-libgen","title":"Command options for <code>libgen</code>","text":"Command option Description Mandatory/Optional <code>-p, --package</code> Package name (organization-name/package-name). Mandatory <code>-i, --input</code> Path to the folder containing EDI schemas. Mandatory <code>-o, --output</code> Path to the folder where packages will be generated. Mandatory"},{"location":"developer-guides/tools/integration-tools/edi-tool/#package-generation-example","title":"Package generation example","text":"<p>Let's assume that an organization named \"CityMart\" needs to work with X12 850, 810, 820, and 855 to handle purchase orders. CityMart's integration developers can put schemas of those X12 specifications into a folder as follows:</p> <pre><code>|-- CityMart\n    |--lib\n    |--schemas\n       |--850.json\n       |--810.json\n       |--820.json\n       |--855.json\n</code></pre> <p>Then, the <code>libgen</code> command can be used to generate a Ballerina package as shown below:</p> <pre><code>$ bal edi libgen -p citymart/porder -i CityMart/schemas -o CityMart/lib\n</code></pre> <p>The generated Ballerina package will look like below:</p> <pre><code>|-- CityMart\n    |--lib  \n    |--porder\n    |     |--modules\n    |     |   |--m850\n    |     |   |  |--G_850.bal\n    |     |   |  |--transformer.bal\n    |     |   |--m810\n    |     |   |  |--G_810.bal\n    |     |   |  |--transformer.bal\n    |     |   |--m820\n    |     |   |  |--G_820.bal\n    |     |   |  |--transformer.bal\n    |     |   |--m855\n    |     |     |--G_855.bal\n    |     |     |--transformer.bal\n    |     |--Ballerina.toml\n    |     |--Module.md\n    |     |--Package.md\n    |     |--porder.bal\n    |     |--rest_connector.bal\n    |\n    |--schemas\n       |--850.json\n       |--810.json\n       |--820.json\n       |--855.json\n</code></pre> <p>As seen in the above project structure, code for each EDI schema is generated into a separate module, to prevent possible conflicts. Now it is possible to build the above project using the <code>bal pack</code> command and publish it into the central repository using the <code>bal push</code> command. Then any BI project can import this package and use it to work with purchase order-related EDI files. An example of using this package for reading an 850 file and writing an 855 file is shown below:</p> <pre><code>import ballerina/io;\nimport citymart/porder.m850;\nimport citymart/porder.m855;\n\npublic function main() returns error? {\n    string orderText = check io:fileReadString(\"orders/d15_05_2023/order10.edi\");\n    m850:Purchase_Order purchaseOrder = check m850:fromEdiString(orderText);\n    ...\n    m855:Purchase_Order_Acknowledgement orderAck = {...};\n    string orderAckText = check m855:toEdiString(orderAck);\n    check io:fileWriteString(\"acks/d15_05_2023/ack10.edi\", orderAckText);\n}\n</code></pre> <p>It is quite common for different trading partners to use variations of standard EDI formats. In such cases, it is possible to create partner-specific schemas and generate a partner-specific Ballerina package for processing interactions with the particular partner.</p>"},{"location":"developer-guides/tools/integration-tools/edi-tool/#using-generated-edi-packages-as-standalone-rest-services","title":"Using generated EDI Packages as standalone REST services","text":"<p>EDI packages generated in the previous step can also be compiled into a jar file (using the <code>bal build</code> command) and executed (using the <code>bal run</code> command) as a standalone Ballerina service that processes EDI files via a REST interface. This is useful for microservice environments where the EDI processing functionality can be deployed as a separate microservice.</p> <p>For example, the \"citymart\" package generated in the above step can be built and executed as a jar file. Once executed, it will expose a REST service to work with X12 850, 810, 820, and 855 files. </p>"},{"location":"developer-guides/tools/integration-tools/edi-tool/#converting-of-x12-850-edi-text-to-json-using-the-rest-service","title":"Converting of X12 850 EDI text to JSON using the REST service","text":"<p>The below REST call can be used to convert an X12 850 EDI text to JSON using the REST service generated from the \"citymart\" package:</p> <pre><code>curl --location 'http://localhost:9090/porderParser/edis/850' \\\n--header 'Content-Type: text/plain' \\\n--data-raw 'GS*PO*SENDERID*RECEIVERID*20240802*1705*1*X*004010~\nST*850*0001~\nBEG*00*NE*4500012345**20240802~\nREF*DP*038~\nPER*BD*John Doe*TE*1234567890*EM*john.doe@example.com~\nFOB*CC~\nITD*01*3*2**30**31~\nDTM*002*20240902~\nN1*ST*SHIP TO NAME*92*SHIP TO CODE~\nN3*123 SHIP TO ADDRESS~\nN4*CITY*STATE*12345*US~\nPO1*1*10*EA*15.00**BP*123456789012*VP*9876543210*UP*123456789012~\nPID*F****PRODUCT DESCRIPTION~\nPO4*1*CA*20*LB~\nCTT*1~\nSE*16*0001~\nGE*1*1~\nIEA*1*000000001~'\n</code></pre> <p>The above REST call will return a JSON response like the below:</p> <pre><code>{\n    \"X12_FunctionalGroup\": {\n        \"FunctionalGroupHeader\": {\n            \"code\": \"GS\",\n            \"GS01__FunctionalIdentifierCode\": \"PO\",\n            \"GS02__ApplicationSendersCode\": \"SENDERID\",\n            \"GS03__ApplicationReceiversCode\": \"RECEIVERID\",\n            ... // Other fields\n        }\n        ... // Other fields\n    },\n    \"InterchangeControlTrailer\": {\n        \"code\": \"IEA\",\n        \"IEA01__NumberofIncludedFunctionalGroups\": 1.0,\n        \"IEA02__InterchangeControlNumber\": 1.0\n    }\n}\n</code></pre>"},{"location":"developer-guides/tools/integration-tools/edi-tool/#converting-of-json-to-x12-850-edi-text-using-the-rest-service","title":"Converting of JSON to X12 850 EDI text using the REST service","text":"<p>The below REST call can be used to convert a JSON to X12 850 EDI text using the REST service generated from the \"citymart\" package:</p> <pre><code>curl --location 'http://localhost:9090/ediParser/objects/850' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n    \"X12_FunctionalGroup\": {\n        \"FunctionalGroupHeader\": {\n            \"code\": \"GS\",\n            \"GS01__FunctionalIdentifierCode\": \"PO\",\n            \"GS02__ApplicationSendersCode\": \"SENDERID\",\n            \"GS03__ApplicationReceiversCode\": \"RECEIVERID\",\n            \"GS04__Date\": \"20240802\",\n            \"GS05__Time\": \"1705\",\n            \"GS06__GroupControlNumber\": 1.0,\n            ... // Other fields\n        },\n        ... // Other fields\n    },\n    \"InterchangeControlTrailer\": {\n        \"code\": \"IEA\",\n        \"IEA01__NumberofIncludedFunctionalGroups\": 1.0,\n        \"IEA02__InterchangeControlNumber\": 1.0\n    }\n}'\n</code></pre> <p>The above REST call will return an X12 850 EDI text response like the below:</p> <pre><code>GS*PO*SENDERID*RECEIVERID*20240802*1705*1*X*004010~\nST*850*0001~\nBEG*00*NE*4500012345**20240802~\nREF*DP*038~\nPER*BD*John Doe*TE*1234567890*EM*john.doe@example.com~\nFOB*CC~\nITD*01*3*2**30**31~\nDTM*002*20240902~\nN1*ST*SHIP TO NAME*92*SHIP TO CODE~\nN3*123 SHIP TO ADDRESS~\nN4*CITY*STATE*12345*US~\nPO1*1*10*EA*15.00**BP*123456789012*VP*9876543210*UP*123456789012~\nPID*F****PRODUCT DESCRIPTION~\nPO4*1*CA*20*LB~\nCTT*1~\nSE*16*0001~\nGE*1*1~\nIEA*1*1~\n</code></pre>"},{"location":"developer-guides/tools/integration-tools/edi-tool/#schema-conversion","title":"Schema conversion","text":"<p>Instead of writing the Ballerina EDI schema from scratch, the Ballerina EDI tool also supports converting various EDI schema formats to the Ballerina EDI schema format.</p>"},{"location":"developer-guides/tools/integration-tools/edi-tool/#x12-schema-to-the-ballerina-edi-schema","title":"X12 schema to the Ballerina EDI schema","text":"<p>X12, short for ANSI ASC X12, is a standard for electronic data interchange (EDI) in the United States. It defines the structure and format of business documents such as <code>purchase orders</code>, <code>invoices</code>, and <code>shipping notices</code>, allowing for seamless communication between different computer systems. X12 standards cover a wide range of industries, including healthcare, finance, retail, and manufacturing.</p> <p>The below command can be used to convert the X12 schema to the Ballerina EDI schema:</p> <pre><code>$ bal edi convertX12Schema -H &lt;enable headers mode&gt; -c &lt;enable collection mode &gt; -i &lt;input schema path&gt; -o &lt;output json file/folder path&gt; -d &lt;segment details path&gt;\n</code></pre> <p>Command options for <code>convertX12Schema</code></p> Command option Description Mandatory/Optional <code>-H, --headers</code> Enable headers mode for X12 schema conversion. Optional <code>-c, --collection</code> Enable collection mode for X12 schema conversion. Optional <code>-i, --input</code> Path to the X12 schema file. Mandatory <code>-o, --output</code> Path to the output file or folder. Mandatory <code>-d, --segdet</code> Path to the segment details file for X12 schema conversion. Optional <p>Example: <pre><code>$ bal edi convertX12Schema -i input/schema.xsd -o output/schema.json\n</code></pre></p>"},{"location":"developer-guides/tools/integration-tools/edi-tool/#edifact-schema-to-the-ballerina-edi-schema","title":"EDIFACT schema to the Ballerina EDI schema","text":"<p>EDIFACT, which stands for Electronic Data Interchange For Administration, Commerce, and Transport, is an international EDI standard developed by the United Nations. It's widely used in Europe and many other parts of the world. EDIFACT provides a common syntax for exchanging business documents electronically between trading partners, facilitating global trade and improving efficiency in supply chain management.</p> <p>The below command can be used to convert the EDIFACT schema to the Ballerina EDI schema:</p> <pre><code>$ bal edi convertEdifactSchema -v &lt;EDIFACT version&gt; -t &lt;EDIFACT message type&gt; -o &lt;output folder&gt;\n</code></pre> <p>Command options for <code>convertEdifactSchema</code></p> Command option Description Mandatory/Optional <code>-v, --version</code> EDIFACT version for EDIFACT schema conversion. Mandatory <code>-t, --type</code> EDIFACT message type for EDIFACT schema conversion. Mandatory <code>-o, --output</code> Path to the folder where EDIFACT schemas will be generated. Mandatory <p>Example: <pre><code>$ bal edi convertEdifactSchema -v d03a -t ORDERS -o output/schema.json\n</code></pre></p>"},{"location":"developer-guides/tools/integration-tools/edi-tool/#esl-to-ballerina-edi-schema","title":"ESL to Ballerina EDI schema","text":"<p>ESL, or Electronic Shelf Labeling, is a technology used in retail stores to display product pricing and information electronically. Instead of traditional paper price tags, ESL systems use digital displays that can be updated remotely, allowing retailers to change prices in real-time and automate pricing strategies.</p> <p>The below command can be used to convert the ESL schema to the Ballerina EDI schema:</p> <pre><code>$ bal edi convertESL -b &lt;segment definitions file path&gt; -i &lt;input ESL schema file/folder&gt; -o &lt;output file/folder&gt;\n</code></pre> <p>Command options for <code>convertEdifactSchema</code></p> Command option Description Mandatory/Optional <code>-b, --basedef</code> Path to the segment definitions file for ESL schema conversion. Mandatory <code>-i, --input</code> Path to the ESL schema file or folder. Mandatory <code>-o, --output</code> Path to the output file or folder. Mandatory <p>Example: <pre><code>$ bal edi convertESL -b segment_definitions.yaml -i esl_schema.esl -o output/schema.json\n</code></pre></p>"},{"location":"developer-guides/tools/integration-tools/health-tool/","title":"Health Tool (FHIR/HL7)","text":"<p>The health tool generates Ballerina library packages and Ballerina templates for FHIR APIs for developing healthcare integrations.</p> <p>These artifacts generated by the tool are primarily focused on the Fast Healthcare Interoperability Resources (FHIR) standard and are developed using Ballerina.</p>"},{"location":"developer-guides/tools/integration-tools/health-tool/#prerequisites","title":"Prerequisites","text":"<p>Download a directory containing all FHIR specification definition files. You can download a preferred standard implementation guide from the FHIR Implementation Guide registry.</p> Note<p>It is recommended to use a Standard for Trial Use (STU) or higher release level of the implementation guides. Make sure that the downloaded specification archive has the <code>StructureDefinition</code>, <code>ValueSet</code>, and <code>CodeSystem</code> files for the Implementation Guide (IG) resources when extracted.</p>"},{"location":"developer-guides/tools/integration-tools/health-tool/#install-the-tool","title":"Install the tool","text":"<p>Execute the command below to pull the Health tool from Ballerina Central.</p> <pre><code>$ bal tool pull health\nhealth:2.0.0 pulled successfully.\nhealth:2.0.0 successfully set as the active version.\n</code></pre>"},{"location":"developer-guides/tools/integration-tools/health-tool/#usage","title":"Usage","text":"<p>The Ballerina Health tool has the following usages.</p> <ul> <li>FHIR package generation: generate a Ballerina package from a given FHIR implementation guide.</li> <li>FHIR template generation: generate Ballerina templates for FHIR APIs from a given FHIR implementation guide.</li> <li>CDS template generation: generate Ballerina templates for Clinical Decision Support (CDS) services based on the CDS specification 2.0.</li> </ul> <p>The general usage of the tool is as follows.</p> Note<p>Make sure to give the directory path of the downloaded FHIR definitions as the last argument.</p> <pre><code>$ bal health fhir [-m | --mode] &lt;mode-type&gt;\n            [-o | --output] &lt;output-location&gt;\n            [--package-name] &lt;ballerina-package-name&gt;\n            [--dependent-package] &lt;dependent-ballerina-package-name&gt; \n            [--org-name] &lt;package-or-template-organization-name&gt;\n            [--included-profile] &lt;profile(s)-to-include-in-generation&gt;\n            [--excluded-profile] &lt;profile(s)-to-exclude-in-generation&gt;\n            &lt;fhir-specification-directory-path&gt;\n</code></pre>"},{"location":"developer-guides/tools/integration-tools/health-tool/#command-options","title":"Command options","text":"<p>The parameters that are available with the tool are listed below.</p> Command option Description Usage <code>-m, --mode</code> Mode type can be <code>package</code> or <code>template</code>. The Ballerina package and API templates are generated according to the mode. <ul><li>package generation</li><li>template generation</li></ul> <code>-o, --output</code> The Ballerina artifacts are generated at the same location from which the <code>bal health fhir</code> command is executed. You can point to another directory location by using the <code>(-o\\|--output)</code> flag. <ul><li>package generation</li><li>template generation</li></ul> <code>--package-name</code> Name of the Ballerina package to be generated. <ul><li>package generation</li></ul> <code>--dependent-package</code> Ballerina package name, which contains the IG resources. <ul><li>template generation</li></ul> <code>--org-name</code> Organization name of the Ballerina package or API templates to be generated. <ul><li>package generation</li><li>template generation</li></ul> <code>--included-profile</code> Generate one or more specific FHIR profiles as Ballerina templates for FHIR APIs. <ul><li>template generation</li></ul> <code>--excluded-profile</code> Skip one or more specific FHIR profiles when generating Ballerina templates for FHIR APIs. <ul><li>template generation</li></ul>"},{"location":"developer-guides/tools/integration-tools/health-tool/#fhir-package-generation","title":"FHIR package generation","text":"<p>The FHIR resources in the implementation guide will be represented as Ballerina records including the correct cardinality constraints and metadata. FHIR integration developers can leverage the generated package when transforming custom health data into FHIR format without referring to the specification.</p>"},{"location":"developer-guides/tools/integration-tools/health-tool/#package-generation-usage","title":"Package generation usage","text":"<p>The tool supports the package generation usage as follows.</p> <pre><code>$ bal health fhir [-m | --mode] package\n            [--package-name] &lt;ballerina-package-name&gt;\n            [-o | --output] &lt;output-location&gt;\n            [--org-name] &lt;package-organization-name&gt;\n            &lt;fhir-specification-directory-path&gt;\n</code></pre>"},{"location":"developer-guides/tools/integration-tools/health-tool/#package-generation-command-options","title":"Package generation command options","text":"Command option Description Mandatory/Optional <code>-m, --mode</code> If <code>mode</code> is set to <code>package</code>, a Ballerina package will be generated including all the records and types. Mandatory <code>--package-name</code> Name of the Ballerina package to be generated. The package name can be explicitly set using this argument. Unless specified, the default name of the implementation guide will be taken to construct the name of the package. For more information, see the the <code>name</code> field. Mandatory <code>-o, --output</code> Location of the generated Ballerina artifacts. If this path is not specified, the output will be written to the same directory from which the command is run. Optional <code>--org-name</code> Organization name of the Ballerina package to be generated. For more information, see  the <code>org</code> field. Optional"},{"location":"developer-guides/tools/integration-tools/health-tool/#package-generation-example","title":"Package generation example","text":"<p>Follow the steps below to try out an example package generation use case of the Health tool.</p>"},{"location":"developer-guides/tools/integration-tools/health-tool/#step-1-clone-the-example-project","title":"Step 1: Clone the example project","text":"<ul> <li> <p>Clone the artifacts of the example and extract them to a preferred location.</p> </li> <li> <p>The cloned directory includes the artifacts below that will be required to try out this example (Ballerina project and the FHIR specification files). </p> <ul> <li>The <code>ig-carinbb/definitions</code> directory: includes the definition files of the FHIR specification.</li> <li>The <code>carinbb-patient-service</code> directory: includes the Ballerina project containing the artifacts (i.e., the <code>Ballerina.toml</code>, <code>Dependencies.toml</code>, and <code>service.bal</code> files) to be executed.</li> </ul> </li> </ul>"},{"location":"developer-guides/tools/integration-tools/health-tool/#step-2-generate-the-package","title":"Step 2: Generate the package","text":"<p>Follow the steps below to run the Health tool and create the Ballerina package.</p> <ol> <li> <p>Navigate to the cloned <code>working-with-health-tool/package-generation</code> directory.</p> </li> <li> <p>Run the tool with the required command options to generate the package.</p> Note<p>This example uses the definitions files downloaded from the JSON Definitions ZIP archive of the Carin BB implementation guide to generate the package.</p> <pre><code>$ bal health fhir -m package -o ig-carinbb/gen --org-name healthcare_samples --package-name carinbb_package ig-carinbb/definitions/\n[INFO] Ballerina FHIR package generation completed successfully. Generated package can be found at: /tmp/healthcare-samples/working-with-health-tool/package-generation/ig-carinbb/gen\n</code></pre> <p>The generated folder (i.e., <code>working-with-health-tool/package-generation/ig-carinbb/gen</code>) will contain the following directory structure.</p> <pre><code>\u2514\u2500\u2500 carinbb_package\n    \u251c\u2500\u2500 Ballerina.toml\n    \u251c\u2500\u2500 Package.md\n    \u251c\u2500\u2500 initializer.bal\n    \u251c\u2500\u2500 resource_c4bbcoverage.bal\n    \u251c\u2500\u2500 resource_c4bbexplanation_of_benefit.bal\n    \u251c\u2500\u2500 resource_c4bbexplanation_of_benefit_inpatient_institutional.bal\n    \u251c\u2500\u2500 resource_c4bbexplanation_of_benefit_oral.bal\n    \u251c\u2500\u2500 resource_c4bbexplanation_of_benefit_outpatient_institutional.bal\n    \u251c\u2500\u2500 resource_c4bbexplanation_of_benefit_pharmacy.bal\n    \u251c\u2500\u2500 resource_c4bbexplanation_of_benefit_professional_non_clinician.bal\n    \u251c\u2500\u2500 resource_c4bborganization.bal\n    \u251c\u2500\u2500 resource_c4bbpatient.bal\n    \u251c\u2500\u2500 resource_c4bbpractitioner.bal\n    \u251c\u2500\u2500 resource_c4bbrelated_person.bal\n    \u251c\u2500\u2500 resources\n    \u251c\u2500\u2500 tests\n    \u2514\u2500\u2500 variables.bal\n</code></pre> </li> <li> <p>Build the generated package.</p> <pre><code>$ cd ig-carinbb/gen/carinbb_package\n$ bal pack\nCompiling source\n        healthcare_samples/carinbb_package:0.0.1\n\nCreating bala\n        target/bala/healthcare_samples-carinbb_package-any-0.0.1.bala\n</code></pre> </li> <li> <p>Push it to a repository.</p> Tip<p>You can push either to the local repository or to Ballerina Central, which is a remote repository.</p> <pre><code>$ bal push --repository local\nSuccessfully pushed target/bala/healthcare_samples-carinbb_package-any-0.0.1.bala to 'local' repository.\n</code></pre> </li> </ol>"},{"location":"developer-guides/tools/integration-tools/health-tool/#step-3-use-the-generated-package","title":"Step 3: Use the generated package","text":"<p>Follow the steps below to use the generated package by running the cloned BI project.</p> <ol> <li> <p>Navigate to the cloned <code>working-with-health-tool/package-generation/carinbb-patient-service</code> directory.</p> Info<p>You can change the dependency (name and version) of the generated package in the <code>carinbb-patient-service/Ballerina.toml</code> file of this cloned BI project directory as preferred.</p> </li> <li> <p>Run the cloned BI project and validate the output.</p> <pre><code>Compiling source\n        healthcare_samples/carinbb_ballerina:1.0.0\n\nRunning executable\n</code></pre> </li> <li> <p>Invoke the API to try it out.</p> <pre><code>$ curl http://localhost:9090/Patient/2121\n</code></pre> <p>You can view the response shown below.</p> <pre><code>{\n\"resourceType\": \"Patient\",\n\"gender\": \"male\",\n\"id\": \"2121\",\n\"identifier\": [\n{\n\"system\": \"http://hl7.org/fhir/sid/us-ssn\",\n\"value\": \"2121\"\n}\n],\n\"meta\": {\n\"profile\": [\n\"http://hl7.org/fhir/us/carin-bb/StructureDefinition/C4BB-Patient\"\n]\n},\n\"name\": [\n{\n\"family\": \"Doe\",\n\"given\": [\n\"John\",\n\"Hemish\"\n]\n}\n]\n}\n</code></pre> </li> </ol>"},{"location":"developer-guides/tools/integration-tools/health-tool/#fhir-template-generation","title":"FHIR template generation","text":"<p>The tool can also be used to generate Ballerina templates for FHIR APIs for the FHIR resources in an implementation guide. FHIR integration developers can utilize these API templates by customizing them to align with their specific business logic and subsequently exposing them as standard FHIR APIs.</p>"},{"location":"developer-guides/tools/integration-tools/health-tool/#template-generation-usage","title":"Template generation usage","text":"<p>The tool supports the template generation usage as follows.</p> <pre><code>$ bal health fhir [-m | --mode] template\n            [--dependent-package] &lt;dependent-ballerina-package-name&gt; \n            [-o | --output] &lt;output-location&gt;\n            [--org-name] &lt;template-organization-name&gt;\n            [--included-profile] &lt;profile(s)-to-include-in-generation&gt;\n            [--excluded-profile] &lt;profile(s)-to-exclude-in-generation&gt;\n            &lt;fhir-specification-directory-path&gt;\n</code></pre>"},{"location":"developer-guides/tools/integration-tools/health-tool/#template-generation-command-options","title":"Template generation command options","text":"Command option Description Mandatory/Optional <code>-m, --mode</code> If <code>mode</code> is set to <code>template</code>, Ballerina templates for FHIR APIs can be generated. Mandatory <code>--dependent-package</code> Fully qualified name of the published Ballerina package containing the IG resources (e.g., <code>&lt;org&gt;/&lt;package&gt;</code>). This option can be used to generate Ballerina templates for FHIR APIs specifically for the resources in the given IG. The package name part of this value will be added as a prefix to the template name. Mandatory <code>-o, --output</code> Location of the generated Ballerina artifacts. If this path is not specified, the output will be written to the same directory from which the command is run. Optional <code>--org-name</code> Organization name of the Ballerina templates for FHIR APIs to be generated. For more information, see  the <code>org</code> field. Optional <code>--included-profile</code> If one or more specific FHIR profiles need to be generated as Ballerina templates, specify the profile URL(s) as the value of this parameter. This argument can be used more than once. Optional <code>--excluded-profile</code> If one or more specific FHIR profiles need to be skipped when generating Ballerina templates, specify the profile URL(s) as the value of this parameter. This argument can be used more than once. Optional"},{"location":"developer-guides/tools/integration-tools/health-tool/#template-generation-example","title":"Template generation example","text":"<p>Follow the steps below to try out an example template generation use case of the Health tool.</p>"},{"location":"developer-guides/tools/integration-tools/health-tool/#step-1-clone-the-example-project_1","title":"Step 1: Clone the example project","text":"<ul> <li> <p>Clone the artifacts of the example and extract them to a preferred location.</p> </li> <li> <p>The cloned directory includes the <code>ig-uscore/definitions</code> directory, which includes the definition files of the FHIR specification.</p> </li> </ul>"},{"location":"developer-guides/tools/integration-tools/health-tool/#step-2-generate-the-templates","title":"Step 2: Generate the templates","text":"<p>Follow the steps below to run the Health tool and generate the Ballerina templates for FHIR APIs for the selected package.</p> Note<p>You need to have a Ballerina package containing IG-specific FHIR resource data models to generate FHIR IG templates. You can use the package mode of the Health tool for easy generation of this package. This example uses the <code>health.fhir.r4.uscore501</code> package in Ballerina Central.</p> <ol> <li> <p>Navigate to the cloned <code>working-with-health-tool/template-generation</code> directory.</p> </li> <li> <p>Run the tool with the required command options to generate the Ballerina templates for FHIR APIs.</p> Note<p>This example uses the definitions files downloaded from the JSON Definitions ZIP archive of the US Core implementation guide to generate the templates.</p> <pre><code>$ bal health fhir -m template -o ig-uscore/gen --org-name healthcare_samples --dependent-package ballerinax/health.fhir.r4.uscore501 ig-uscore/definitions\n[INFO] Generating templates for all FHIR profiles...\n[INFO] Ballerina FHIR API templates generation completed successfully. Generated templates can be found at: /tmp/healthcare-samples/working-with-health-tool/template-generation/ig-uscore/gen\n</code></pre> <p>The generated folder (i.e., <code>working-with-health-tool/template-generation/ig-uscore/gen</code>) will contain the following directory structure.</p> <pre><code>.\n|____device\n|   |____api_config.bal\n|   |____service.bal\n|   |____.gitignore\n|   |____Package.md\n|   |____Ballerina.toml\n|____observation\n|   |____api_config.bal\n|   |____service.bal\n|   |____.gitignore\n|   |____Package.md\n|   |____Ballerina.toml\n|____patient\n|   |____api_config.bal\n|   |____service.bal\n|   |____Dependencies.toml\n|   |____.gitignore\n|   |____Package.md\n|   |____Ballerina.toml\n|____practitioner\n|   |____api_config.bal\n|   |____service.bal\n|   |____Dependencies.toml\n|   |____.gitignore\n|   |____Package.md\n|   |____Ballerina.toml\n|____practitionerrole\n|   |____api_config.bal\n|   |____service.bal\n|   |____.gitignore\n|   |____Package.md\n|   |____Ballerina.toml\n|____procedure\n|   |____api_config.bal\n|   |____service.bal\n|   |____.gitignore\n|   |____Package.md\n|   |____Ballerina.toml\n|____relatedperson\n|   |____api_config.bal\n|   |____service.bal\n|   |____.gitignore\n|   |____Package.md\n|   |____Ballerina.toml    \n</code></pre> </li> </ol>"},{"location":"developer-guides/tools/integration-tools/health-tool/#step-3-use-the-generated-templates","title":"Step 3: Use the generated templates","text":"<p>Follow the steps below to use the generated API templates by running the cloned Ballerina project.</p> <ol> <li> <p>Navigate to the generated <code>working-with-health-tool/template-generation/ig-uscore/gen/practitioner/</code> directory.</p> </li> <li> <p>Update the <code>get fhir/r4/Practitioner/[string id]</code> method in the corresponding <code>working-with-health-tool/template-generation/ig-uscore/gen/practitioner/service.bal</code> file with the code below to implement the business logic.</p> <pre><code>isolated resource function get fhir/r4/Practitioner/[string id] (r4:FHIRContext fhirContext) returns Practitioner|r4:OperationOutcome|r4:FHIRError {\n    Practitioner practitioner = {\n        resourceType: \"Practitioner\",\n        id: \"1\",\n        meta: {\n            lastUpdated: \"2021-08-24T10:10:10Z\",\n            profile: [\n                \"http://hl7.org/fhir/us/core/StructureDefinition/us-core-practitioner\"\n            ]\n        },\n        identifier: [\n            {\n                use: \"official\",\n                system: \"http://hl7.org/fhir/sid/us-npi\",\n                value: \"1234567890\"\n            }\n        ],\n        name: [\n            {\n                use: \"official\",\n                family: \"Smith\",\n                given: [\n                    \"John\",\n                    \"Jacob\"\n                ],\n                prefix: [\n                    \"Dr.\"\n                ]\n            }\n        ]\n    };\n    return practitioner;\n}\n</code></pre> </li> <li> <p>Run the service and verify the output response.</p> <pre><code>Compiling source\n        healthcare_samples/health.fhir.r4.uscore501.practitioner:1.0.0\n\nRunning executable\n</code></pre> </li> <li> <p>Invoke the API to try it out.</p> <pre><code>$ curl http://localhost:9090/fhir/r4/Practitioner/1\n</code></pre> <p>You can view the response shown below.</p> <p><code>json { \"resourceType\":\"Practitioner\", \"identifier\":[     {         \"system\":\"http://hl7.org/fhir/sid/us-npi\",         \"use\":\"official\",         \"value\":\"1234567890\"     } ], \"meta\":{     \"lastUpdated\":\"2021-08-24T10:10:10Z\",     \"profile\":[         \"http://hl7.org/fhir/us/core/StructureDefinition/us-core-practitioner\"     ] }, \"name\":[     {         \"given\":[             \"John\",             \"Jacob\"         ],         \"prefix\":[             \"Dr.\"         ],         \"use\":\"official\",         \"family\":\"Smith\"     } ], \"id\":\"1\"    }</code></p> </li> </ol>"},{"location":"developer-guides/tools/integration-tools/health-tool/#cds-template-generation","title":"CDS template generation","text":"<p>A Ballerina service template can be generated from CDS hook definitions. This template will also include basic functionalities such as validation, prefetch, etc., and it will facilitate the developers' implementation of the required connection with the external decision support system to run the CDS server. The generated Ballerina service project will be written into the provided output location.</p> <p>Supported CDS version: 2.0</p>"},{"location":"developer-guides/tools/integration-tools/health-tool/#template-generation-usage_1","title":"Template generation usage","text":"<pre><code>$ bal health cds\n            [--org-name] &lt;template-organization-name&gt;\n            [--package-name] &lt;name-of-the-package&gt;\n            [--package-version] &lt;version-of-the-package&gt;\n            [-o | --output] &lt;output-location&gt;\n            [-i | --input] &lt;cds-hook-definitions-file-path&gt;\n</code></pre>"},{"location":"developer-guides/tools/integration-tools/health-tool/#cds-template-generation-command-options","title":"CDS template generation command options","text":"Command option Description Mandatory/Optional <code>-i, --input</code> The input file with CDS hook definitions which will be used to generate the Ballerina service. Only TOML files are accepted. Mandatory <code>--org-name</code> The organization name to be used for the generated Ballerina template. For more information, see  the <code>org</code> field. Optional <code>--package-name</code> The package name to be used for the generated Ballerina template. If not specified, <code>health.fhir.templates.crd</code> will be used to construct the name of the package. Optional <code>--package-version</code> The version to be used for the generated Ballerina template. Optional <code>-o, --output</code> The location of the generated Ballerina artifacts. If this path is not specified, the output will be written to the same directory from which the command is run. Optional"},{"location":"developer-guides/tools/integration-tools/health-tool/#template-generation-example_1","title":"Template generation example","text":"<p>Follow the steps below to try out a sample CDS template generation use case of the Health tool.</p>"},{"location":"developer-guides/tools/integration-tools/health-tool/#step-1-create-the-cds-definition-file","title":"Step 1: Create the CDS definition file","text":"<p>Create a TOML file with the CDS hook definitions. Refer to CDS specifications for more information about the attributes.</p> <p>Sample <code>cds-definitions.toml</code> file:</p> <pre><code>[[cds_services]]\nid = \"static-patient-greeter\"\nhook = \"patient-view\"\ntitle = \"Static CDS Service Example\"\ndescription = \"An example of a CDS Service that returns a static set of cards\"\nusageRequirements = \"Note: functionality of this CDS Service is degraded without access to a FHIR Restful API as part of CDS recommendation generation.\"\n[cds_services.prefetch]\npatientToGreet = \"Patient/{{context.patientId}}\"\n\n[[cds_services]]\nid = \"book-imaging-center\"\nhook = \"order-dispatch\"\ntitle = \"Book an imaging center\"\ndescription = \"This hook can be used when booking imaging center\"\n</code></pre>"},{"location":"developer-guides/tools/integration-tools/health-tool/#step-2-generate-the-templates_1","title":"Step 2: Generate the templates","text":"<p>Follow the steps below to run the Health tool and generate the Ballerina templates for CDS for the given hook definitions.</p> <ol> <li> <p>Navigate to the working directory that contains the CDS hook definitions file.</p> </li> <li> <p>Run the tool with the required command options to generate the Ballerina template.</p> </li> </ol> <pre><code>$ bal health cds --org-name wso2 --package-name cds_service  --package-version 1.0.0 -i cds-definitions.toml\n[INFO] Ballerina CDS service template generation completed successfully. The generated project can be found at /Users/tom/Desktop/working_directory/generated-template/cds_service\n</code></pre> <p>The generated folder (i.e., working_directory/template-generation/cds_service) will contain the following directory structure.</p> <pre><code>.\n\u251c\u2500\u2500 Ballerina.toml\n\u251c\u2500\u2500 Config.toml\n\u251c\u2500\u2500 Package.md\n\u251c\u2500\u2500 decision_engine_connector.bal\n\u251c\u2500\u2500 interceptor.bal\n\u251c\u2500\u2500 service.bal\n\u2514\u2500\u2500 utils.bal\n</code></pre>"},{"location":"developer-guides/tools/integration-tools/health-tool/#step-3-use-the-generated-templates_1","title":"Step 3: Use the generated templates","text":"<p>Follow the steps below to use the generated CDS service template.</p> <p>1. Navigate to the generated <code>working_directory/template-generation/cds_service/</code> directory.</p> <p>2. Complete the decision system connectivity implementation.\u00a0The <code>decision_engine_connector</code>\u00a0file contains placeholder functions that must be implemented to connect with external decision systems. Please follow the instructions in the file.</p> <pre><code>Sample implementation for a placeholder function:\n\n```\nisolated function connectDecisionSystemForBookImagingCenter(cds:CdsRequest cdsRequest, string hookId) returns cds:CdsResponse|cds:CdsError {\n    cds:Card card1 = {\n        summary: \"Prior authorization\",\n        indicator: \"critical\",\n        'source: {\n            label: \"Static CDS Service Example\",\n            url: \"https://example.com\",\n            icon: \"https://example.com/img/icon-100px.png\"\n        },\n        detail: \"Obtain prior authorization to avoid claim denials and patient financial liability. Contact: For questions, reach out to the insurance provider or billing department.\",\n        suggestions: [{label: \"Kindly get pri-authorization\"}],\n        selectionBehavior: \"at-most-one\",\n        links: [{label: \"Prior-auth\", url: \"https://www.acmehealth.com/policies/lab-coverage\", 'type: cds:ABSOLUTE}]\n    };\n\n    cds:Card card2 = {\n        summary: \"Alternative centers\",\n        indicator: \"info\",\n        'source: {\n            label: \"Static CDS Service Example\",\n            url: \"https://example.com\",\n            icon: \"https://example.com/img/icon-100px.png\"\n        },\n        detail: \"Discuss alternative imaging centers with patients to enhance access and affordability. For assistance, reach out to the facility's scheduling department or insurance provider.\",\n        suggestions: [\n            {label: \"The selected imaging center is far away from your location. Please select nearby one. Suggested: Asiri labs : Col - 3\"}\n        ],\n        selectionBehavior: \"any\"\n    };\n\n    cds:CdsResponse cdsResponse = {\n        cards: [card1, card2],\n        systemActions: []\n    };\n    return cdsResponse;\n}\n```</code></pre> <p>3. Run the service.</p> <pre><code>Compiling source\n        healthcare_samples/health.fhir.r4.uscore501.practitioner:1.0.0\n\nRunning executable\n</code></pre> <p>4. Invoke the API to try it out.</p> <pre><code>curl --location 'http://localhost:8080/cds-services/book-imaging-center' \\\n--header 'Content-Type: application/json' \\\n--data '{\n \"hookInstance\": \"d1577c69-dfbe-44ad-ba6d-3e05e953b2ea\",\n \"hook\": \"order-dispatch\",\n \"context\": {\n     \"patientId\": \"12345\",\n     \"dispatchedOrders\": [\n         \"ServiceRequest/proc002\"\n     ],\n     \"performer\": \"Organization/O12345\",\n     \"fulfillmentTasks\": [\n         {\n             \"resourceType\": \"Task\",\n             \"status\": \"draft\",\n             \"intent\": \"order\",\n             \"code\": {\n                 \"coding\": [\n                     {\n                         \"system\": \"http://hl7.org/fhir/CodeSystem/task-code\",\n                         \"code\": \"fulfill\"\n                     }\n                 ]\n             },\n             \"focus\": {\n                 \"reference\": \"ServiceRequest/proc002\"\n             },\n             \"for\": {\n                 \"reference\": \"Patient/12345\"\n             },\n             \"authoredOn\": \"2016-03-10T22:39:32-04:00\",\n             \"lastModified\": \"2016-03-10T22:39:32-04:00\",\n             \"requester\": {\n                 \"reference\": \"Practitioner/wdwdwd\"\n             },\n             \"owner\": {\n                 \"reference\": \"Organization/some-performer\"\n             }\n         }\n     ]\n }\n}'\n</code></pre> <p>You can view the response shown below.</p> <pre><code>{\n\"resourceType\": \"Practitioner\",\n\"identifier\": [\n{\n\"system\": \"http://hl7.org/fhir/sid/us-npi\",\n\"use\": \"official\",\n\"value\": \"1234567890\"\n}\n],\n\"meta\": {\n\"lastUpdated\": \"2021-08-24T10:10:10Z\",\n\"profile\": [\n\"http://hl7.org/fhir/us/core/StructureDefinition/us-core-practitioner\"\n]\n},\n\"name\": [\n{\n\"given\": [\"John\", \"Jacob\"],\n\"prefix\": [\"Dr.\"],\n\"use\": \"official\",\n\"family\": \"Smith\"\n}\n],\n\"id\": \"1\"\n}\n</code></pre>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/","title":"Azure Logic Apps Migration Tool","text":"<p>This guide explains how to use the migrate-logicapps tool to convert Azure Logic Apps integrations into Ballerina packages compatible with the WSO2 Integrator: BI.</p>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/#tool-overview","title":"Tool overview","text":"<p>The tool accepts either a project directory that contains multiple Logic Apps <code>.json</code> files or a single Logic Apps <code>.json</code>  file as input and produces an equivalent WSO2 Integrator: BI project.</p>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/#supported-logic-apps-versions","title":"Supported Logic Apps versions","text":"<p>The migration tool supports all the NuGet versions of Azure Logic Apps.  It is recommended to use the latest version of the Logic Apps JSON schema for the best results.</p>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/#installation","title":"Installation","text":"<p>To pull the <code>migrate-logicapps</code> tool from Ballerina Central, run the following command: <pre><code>$ bal tool pull migrate-logicapps\n</code></pre></p>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/#parameters","title":"Parameters","text":"<p>Following are parameters that can be used with the <code>migrate-logicapps</code> tool:</p> <ul> <li>source-project-directory-or-file - Required. The path to the directory that contains multiple Logic Apps JSON files   or a single Logic Apps JSON file to be migrated.</li> <li>-o or --out - Optional. The directory where the new Ballerina package will be created. If not provided,<ul> <li>For a project directory input, the new Ballerina package is created inside the source project directory.</li> <li>For a single JSON file, the new Ballerina package is created in the same directory as the source file.</li> </ul> </li> <li>-v or --verbose - Optional. Enable verbose output during conversion.</li> <li>-m or --multi-root - Optional. Treat each child directory as a separate project and convert all of them. The source must be a directory containing multiple Logic Apps JSON files.</li> </ul>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/#implementation","title":"Implementation","text":"<p>Follow the steps below to migrate your Logic Apps integration.</p>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/#step-1-prepare-your-input","title":"Step 1: Prepare your input","text":"<p>You can migrate either a project directory that contains multiple Logic Apps <code>.json</code> files or a single Logic Apps <code>.json</code> file:</p> <ul> <li>For multiple JSON files: Ensure that the project directory only contains Logic Apps <code>.json</code> files.</li> <li>For single JSON files: You can directly use any valid Logic Apps <code>.json</code> file.</li> </ul>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/#step-2-run-the-migration-tool","title":"Step 2: Run the migration tool","text":"<p>Use one of the following commands based on your needs.</p> <ol> <li> <p>To convert a Logic Apps JSON file with the default output location:</p> <pre><code>$ bal migrate-logicapps /path/to/logic-app-control-flow.json\n</code></pre> <p>This will create a Ballerina package in the same directory as the input <code>.json</code> file.</p> </li> <li> <p>To convert a Logic Apps JSON file with a custom output location:</p> <pre><code>$ bal migrate-logicapps /path/to/logic-app-control-flow.json --out /path/to/output-dir\n</code></pre> <p>This will create a Ballerina package at <code>/path/to/output-dir</code>.</p> </li> <li> <p>To convert multiple Logic Apps JSON files with the default output location:</p> <pre><code>$ bal migrate-logicapps /path/to/logic-apps-file-directory --multi-root\n</code></pre> <p>This will create multiple Ballerina packages inside <code>/path/to/logic-apps-file-directory</code> directory for each Logic  Apps file.</p> </li> <li> <p>To convert multiple Logic Apps JSON files with a custom output location:</p> <pre><code>$ bal migrate-logicapps /path/to/logic-apps-file-directory --out /path/to/output-dir --multi-root\n</code></pre> <p>This will create multiple Ballerina packages at <code>/path/to/output-dir</code> for each Logic Apps file.</p> </li> </ol>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/#step-3-review-migration-output","title":"Step 3: Review migration output","text":"<ol> <li> <p>For a directory with multiple Logic Apps JSON files as input:</p> <ul> <li>A new Ballerina package is created for each Logic Apps file with the same name as the input <code>.json</code> file, appended    with a <code>_ballerina</code> suffix.</li> <li>Created Ballerina package contains the WSO2 Integrator: BI file structure.</li> </ul> </li> <li> <p>For a single Logic Apps JSON file input:</p> <ul> <li>A new Ballerina package is created with the same name as the <code>.json</code> file, appended with a <code>_ballerina</code> suffix.</li> <li>Created Ballerina package contains the WSO2 Integrator: BI file structure.</li> </ul> </li> </ol>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/#step-4-address-the-todo-comments-and-manual-adjustments","title":"Step 4: Address the TODO comments and manual adjustments","text":"<p>The generated Ballerina code may contain TODO comments for some Logic Apps actions.  You need to manually review and implement these actions in the Ballerina code.</p> <pre><code>// Function to escape special characters for database operations\npublic function escapeSpecialCharacters(string input) returns string {\n    // Simplified implementation since replace function is not available\n    return input;\n}\n</code></pre> <p>The generated Ballerina code may also contain semantic errors or unsupported features that require manual adjustments.</p> <pre><code>// Initialize HTTP client with timeout configuration\npublic function initializeHttpClient(HttpConfig config) returns http:Client|error {\n    return new (config.baseUrl, {\n        timeout: config.timeout\n    });\n}\n</code></pre>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/#example-converting-a-logic-apps-json-file","title":"Example: Converting a Logic Apps JSON file","text":"<p>Let's walk through an example of migrating a Logic Apps sample <code>.json</code> integration to Ballerina.</p> <p>Here's a sample Logic Apps <code>.json</code> file (<code>weather-forecast.json</code>) that runs every hour to fetch weather data from an external API and store it in a SQL database.</p> <pre><code>{\n\"$schema\": \"https://schema.management.azure.com/providers/Microsoft.Logic/schemas/2016-06-01/workflowdefinition.json#\",\n\"contentVersion\": \"1.0.0.0\",\n\"parameters\": {\n\"sqlConnectionString\": {\n\"type\": \"string\",\n\"metadata\": {\n\"description\": \"SQL Database connection string\"\n}\n},\n\"weatherApiKey\": {\n\"type\": \"string\",\n\"metadata\": {\n\"description\": \"Weather API key for external service\"\n}\n}\n},\n\"triggers\": {\n\"Recurrence\": {\n\"type\": \"Recurrence\",\n\"recurrence\": {\n\"frequency\": \"Hour\",\n\"interval\": 1\n}\n}\n},\n\"actions\": {\n\"Initialize_Location_Variable\": {\n\"type\": \"InitializeVariable\",\n\"inputs\": {\n\"variables\": [\n{\n\"name\": \"location\",\n\"type\": \"string\",\n\"value\": \"Seattle,WA\"\n}\n]\n},\n\"runAfter\": {}\n},\n\"Get_Weather_Data\": {\n\"type\": \"Http\",\n\"inputs\": {\n\"method\": \"GET\",\n\"uri\": \"https://api.openweathermap.org/data/2.5/weather\",\n\"queries\": {\n\"q\": \"@variables('location')\",\n\"appid\": \"@parameters('weatherApiKey')\",\n\"units\": \"metric\"\n}\n},\n\"runAfter\": {\n\"Initialize_Location_Variable\": [\n\"Succeeded\"\n]\n}\n},\n\"Check_Weather_Response\": {\n\"type\": \"If\",\n\"expression\": {\n\"and\": [\n{\n\"not\": {\n\"equals\": [\n\"@outputs('Get_Weather_Data')['statusCode']\",\n200\n]\n}\n}\n]\n},\n\"actions\": {\n\"Terminate_Error\": {\n\"type\": \"Terminate\",\n\"inputs\": {\n\"runStatus\": \"Failed\",\n\"runError\": {\n\"code\": \"WeatherApiError\",\n\"message\": \"Failed to retrieve weather data\"\n}\n}\n}\n},\n\"else\": {\n\"actions\": {\n\"Parse_Weather_JSON\": {\n\"type\": \"ParseJson\",\n\"inputs\": {\n\"content\": \"@body('Get_Weather_Data')\",\n\"schema\": {\n\"type\": \"object\",\n\"properties\": {\n\"main\": {\n\"type\": \"object\",\n\"properties\": {\n\"temp\": {\n\"type\": \"number\"\n},\n\"humidity\": {\n\"type\": \"number\"\n},\n\"pressure\": {\n\"type\": \"number\"\n}\n}\n},\n\"weather\": {\n\"type\": \"array\",\n\"items\": {\n\"type\": \"object\",\n\"properties\": {\n\"main\": {\n\"type\": \"string\"\n},\n\"description\": {\n\"type\": \"string\"\n}\n}\n}\n},\n\"name\": {\n\"type\": \"string\"\n}\n}\n}\n}\n},\n\"Check_Temperature_Range\": {\n\"type\": \"If\",\n\"expression\": {\n\"and\": [\n{\n\"greater\": [\n\"@body('Parse_Weather_JSON')['main']['temp']\",\n-50\n]\n},\n{\n\"less\": [\n\"@body('Parse_Weather_JSON')['main']['temp']\",\n60\n]\n}\n]\n},\n\"actions\": {\n\"Update_Weather_Database\": {\n\"type\": \"ApiConnection\",\n\"inputs\": {\n\"host\": {\n\"connection\": {\n\"name\": \"@parameters('$connections')['sql']['connectionId']\"\n}\n},\n\"method\": \"post\",\n\"path\": \"/v2/datasets/@{encodeURIComponent(encodeURIComponent('default'))},@{encodeURIComponent(encodeURIComponent('default'))}/tables/@{encodeURIComponent(encodeURIComponent('[dbo].[WeatherReadings]'))}/items\",\n\"body\": {\n\"Location\": \"@body('Parse_Weather_JSON')['name']\",\n\"Temperature\": \"@body('Parse_Weather_JSON')['main']['temp']\",\n\"Humidity\": \"@body('Parse_Weather_JSON')['main']['humidity']\",\n\"Pressure\": \"@body('Parse_Weather_JSON')['main']['pressure']\",\n\"Condition\": \"@first(body('Parse_Weather_JSON')['weather'])['main']\",\n\"Description\": \"@first(body('Parse_Weather_JSON')['weather'])['description']\",\n\"Timestamp\": \"@utcNow()\",\n\"IsValid\": true\n}\n}\n},\n\"Log_Success\": {\n\"type\": \"Compose\",\n\"inputs\": {\n\"message\": \"Weather data successfully updated\",\n\"location\": \"@body('Parse_Weather_JSON')['name']\",\n\"temperature\": \"@body('Parse_Weather_JSON')['main']['temp']\",\n\"timestamp\": \"@utcNow()\"\n},\n\"runAfter\": {\n\"Update_Weather_Database\": [\n\"Succeeded\"\n]\n}\n}\n},\n\"else\": {\n\"actions\": {\n\"Log_Invalid_Temperature\": {\n\"type\": \"Compose\",\n\"inputs\": {\n\"error\": \"Invalid temperature reading\",\n\"temperature\": \"@body('Parse_Weather_JSON')['main']['temp']\",\n\"location\": \"@body('Parse_Weather_JSON')['name']\"\n}\n},\n\"Insert_Invalid_Reading\": {\n\"type\": \"ApiConnection\",\n\"inputs\": {\n\"host\": {\n\"connection\": {\n\"name\": \"@parameters('$connections')['sql']['connectionId']\"\n}\n},\n\"method\": \"post\",\n\"path\": \"/v2/datasets/@{encodeURIComponent(encodeURIComponent('default'))},@{encodeURIComponent(encodeURIComponent('default'))}/tables/@{encodeURIComponent(encodeURIComponent('[dbo].[WeatherReadings]'))}/items\",\n\"body\": {\n\"Location\": \"@body('Parse_Weather_JSON')['name']\",\n\"Temperature\": \"@body('Parse_Weather_JSON')['main']['temp']\",\n\"Humidity\": \"@body('Parse_Weather_JSON')['main']['humidity']\",\n\"Pressure\": \"@body('Parse_Weather_JSON')['main']['pressure']\",\n\"Condition\": \"Invalid\",\n\"Description\": \"Temperature out of valid range\",\n\"Timestamp\": \"@utcNow()\",\n\"IsValid\": false\n}\n},\n\"runAfter\": {\n\"Log_Invalid_Temperature\": [\n\"Succeeded\"\n]\n}\n}\n}\n},\n\"runAfter\": {\n\"Parse_Weather_JSON\": [\n\"Succeeded\"\n]\n}\n}\n}\n},\n\"runAfter\": {\n\"Get_Weather_Data\": [\n\"Succeeded\",\n\"Failed\"\n]\n}\n},\n\"Handle_Errors\": {\n\"type\": \"Scope\",\n\"actions\": {\n\"Send_Error_Notification\": {\n\"type\": \"Http\",\n\"inputs\": {\n\"method\": \"POST\",\n\"uri\": \"https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK\",\n\"body\": {\n\"text\": \"Weather Logic App failed: @{workflow()['run']['error']['message']}\"\n}\n}\n}\n},\n\"runAfter\": {\n\"Check_Weather_Response\": [\n\"Failed\",\n\"Skipped\",\n\"TimedOut\"\n]\n}\n}\n},\n\"outputs\": {\n\"result\": {\n\"type\": \"Object\",\n\"value\": {\n\"status\": \"completed\",\n\"location\": \"@variables('location')\",\n\"executionTime\": \"@utcNow()\"\n}\n}\n}\n}\n</code></pre> <p>Following is the flow diagram of the Logic Apps:</p> <p>Logic Apps workflow showing the hourly weather data collection and database storage process</p>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/#run-the-migration-tool","title":"Run the migration tool","text":"<p>To convert the Logic Apps <code>.json</code> file using the <code>migrate-logicapps</code> tool, execute the following command:</p> <pre><code>$ bal migrate-logicapps /path/to/weather-forecast.json\n</code></pre>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/#examine-the-generated-ballerina-code","title":"Examine the generated Ballerina code","text":"<p>The tool generates a Ballerina package named <code>weather-forecast_ballerina</code> inside <code>/path/to</code> with the following structure:</p> <pre><code>weather-forecast_ballerina/\n\u251c\u2500\u2500 agents.bal\n\u251c\u2500\u2500 Ballerina.toml\n\u251c\u2500\u2500 config.bal\n\u251c\u2500\u2500 connections.bal\n\u251c\u2500\u2500 data_mappings.bal\n\u251c\u2500\u2500 functions.bal\n\u251c\u2500\u2500 main.bal\n\u2514\u2500\u2500 types.bal\n</code></pre> <p>The <code>main.bal</code> file contains the main logic of the integration, which includes the scheduled trigger.</p> <p>Please note that the generated Ballerina code may be different in multiple runs since the migration tool uses AI-based  conversion and the output may vary based on the complexity of the Logic Apps application.</p>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/#supported-logic-apps-features","title":"Supported Logic Apps features","text":"<p>The migration tool supports the following Azure Logic Apps features:</p>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/#core-workflow-components","title":"Core workflow components","text":"<ul> <li>Triggers: HTTP requests, scheduled triggers, and event-based triggers</li> <li>Actions: HTTP actions, data operations, and control flow actions</li> <li>Connectors: Common Azure connectors and third-party service integrations</li> <li>Variables: Workflow variables and their transformations</li> <li>Expressions: Logic Apps expressions and functions</li> </ul>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/#control-flow","title":"Control flow","text":"<ul> <li>Conditional Logic: If-else conditions and switch statements</li> <li>Loops: For-each loops and until loops</li> <li>Parallel Branches: Concurrent execution paths</li> <li>Scopes: Grouping actions and error handling</li> </ul>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/#data-operations","title":"Data operations","text":"<ul> <li>Data Transformation: JSON parsing, composition, and manipulation</li> <li>Variable Operations: Initialize, set, increment, and append operations</li> <li>Array Operations: Filtering, mapping, and aggregation</li> <li>String Operations: Concatenation, substring, and formatting</li> </ul>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/#error-handling","title":"Error handling","text":"<ul> <li>Try-Catch Blocks: Exception handling and error propagation</li> <li>Retry Policies: Configurable retry mechanisms</li> <li>Timeout Settings: Action timeout configurations</li> </ul>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/#integration-patterns","title":"Integration patterns","text":"<ul> <li>REST API Calls: HTTP client operations with authentication</li> <li>Message Routing: Content-based routing and message transformation</li> <li>Protocol Translation: Converting between different message formats</li> </ul>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/#limitations","title":"Limitations","text":"<p>While the migration tool provides comprehensive conversion capabilities, there are some limitations to be aware of:</p>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/#platform-specific-features","title":"Platform-specific features","text":"<ul> <li>Azure-specific connectors: Some Azure-native connectors may not have direct Ballerina equivalents.</li> <li>Logic Apps runtime features: Some runtime-specific features may need manual implementation</li> </ul>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/#advanced-scenarios","title":"Advanced scenarios","text":"<ul> <li>Complex Custom Connectors: Custom connectors with complex authentication flows may require manual adaptation</li> <li>Stateful Workflows: Long-running stateful workflows may need additional consideration</li> <li>Large-scale Parallel Processing: Extremely high-concurrency scenarios may require performance tuning</li> </ul>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/#ai-generated-code-considerations","title":"AI-generated code considerations","text":"<ul> <li>Code Review Required: Generated code should be reviewed and tested before production use</li> <li>Performance Optimization: Generated code may require optimization for specific use cases</li> <li>Security Validation: Security configurations and credentials should be validated manually</li> </ul>"},{"location":"developer-guides/tools/migration-tools/logic-apps-migration-tool/#post-migration-requirements","title":"Post-migration requirements","text":"<ul> <li>Testing: Comprehensive testing of converted workflows is recommended</li> <li>Configuration: Environment-specific configurations need to be set up manually</li> <li>Monitoring: Logging and monitoring setup may require additional configuration</li> </ul> Disclaimer<p>Azure Logic Apps: \"Azure Logic Apps\", \"Microsoft Azure\", and \"Logic Apps\" are trademarks of Microsoft Corporation. All product, company names and marks mentioned herein are the property of their respective owners and are mentioned for identification purposes only.</p>"},{"location":"developer-guides/tools/migration-tools/mulesoft-migration-tool/","title":"MuleSoft Migration Tool","text":"<p>This guide explains how to convert existing MuleSoft applications into integrations compatible with WSO2 Integrator: BI.</p>"},{"location":"developer-guides/tools/migration-tools/mulesoft-migration-tool/#overview","title":"Overview","text":"<p>This migration support is directly integrated into WSO2 Integrator: BI, providing a user-friendly wizard interface for converting MuleSoft projects. The tool accepts either a MuleSoft project directory or a standalone Mule <code>.xml</code> configuration file as input and, generates equivalent Ballerina packages that can be opened directly in WSO2 Integrator: BI.</p> <p>The migration wizard provides:</p> <ul> <li>Interactive project selection with file picker support.</li> <li>Real-time migration status with detailed logs.</li> <li>Migration coverage reports showing conversion success rates.</li> <li>Automated project creation with the converted Ballerina code.</li> </ul>"},{"location":"developer-guides/tools/migration-tools/mulesoft-migration-tool/#supported-mule-versions","title":"Supported Mule versions","text":"<p>The migration tool supports both Mule 3.x and Mule 4.x projects.</p>"},{"location":"developer-guides/tools/migration-tools/mulesoft-migration-tool/#usage","title":"Usage","text":"<p>Follow the steps below to migrate your MuleSoft application.</p>"},{"location":"developer-guides/tools/migration-tools/mulesoft-migration-tool/#step-1-prepare-your-input","title":"Step 1: Prepare your input","text":"<p>You can migrate a complete MuleSoft project, a standalone Mule <code>.xml</code> configuration file, or a directory containing multiple MuleSoft projects:</p> <ul> <li>For MuleSoft projects: Ensure your project follows the standard structure with configuration XML files located under:<ul> <li>Mule 3.x: <code>mule-project/src/main/app</code></li> <li>Mule 4.x: <code>mule-project/src/main/mule</code></li> </ul> </li> <li>For standalone XML files: You can directly use any valid Mule XML configuration file.</li> </ul>"},{"location":"developer-guides/tools/migration-tools/mulesoft-migration-tool/#step-2-launch-the-migration-wizard","title":"Step 2: Launch the migration wizard","text":"<ol> <li>Open WSO2 Integrator: BI in VS Code.</li> <li> <p>Access the welcome page - If not automatically displayed, you can access it through the Command Palette (<code>Ctrl+Shift+P</code> / <code>Cmd+Shift+P</code>) and search for \"BI: Open Welcome\".</p> </li> <li> <p>Click \"Import External Integration\" in the \"Import External Integration\" section.</p> </li> </ol>"},{"location":"developer-guides/tools/migration-tools/mulesoft-migration-tool/#step-3-select-source-platform-and-project","title":"Step 3: Select source platform and project","text":"<ol> <li>Choose MuleSoft as your source platform from the available options.</li> <li>Select your project using the file picker:<ul> <li>For MuleSoft projects: Select the project root directory.</li> <li>For standalone XML files: Select the individual <code>.xml</code> file.</li> </ul> </li> <li> <p>Configure MuleSoft settings:</p> <ul> <li>Force Version: Choose \"Auto Detect\" (recommended) or manually specify Mule version (3 or 4).</li> <li>Click \"Start Migration\" to begin the conversion process.</li> </ul> </li> </ol>"},{"location":"developer-guides/tools/migration-tools/mulesoft-migration-tool/#step-4-monitor-migration-progress-and-review-results","title":"Step 4: Monitor migration progress and review results","text":""},{"location":"developer-guides/tools/migration-tools/mulesoft-migration-tool/#during-migration-real-time-progress-monitoring","title":"During Migration: Real-time Progress Monitoring","text":"<p>While the migration is ongoing, you will see:</p> <ul> <li>Real-time migration status updates.</li> <li>Detailed logs of the conversion process.</li> <li>Progress indication showing current migration step.</li> </ul>"},{"location":"developer-guides/tools/migration-tools/mulesoft-migration-tool/#after-migration-coverage-report-and-results","title":"After Migration: Coverage Report and Results","text":"<p>Once the migration process completes, the same page updates to show:</p> <ul> <li>Migration Coverage: Percentage showing successful conversion rate.</li> <li>Total code lines: Number of lines processed.</li> <li>Migratable vs Non-migratable code lines: Breakdown of conversion success.</li> <li>View Full Report: Click this button to view the detailed migration report in your browser.</li> <li>Save Report: Click this button to save the migration report to your local file system for future reference.</li> </ul>"},{"location":"developer-guides/tools/migration-tools/mulesoft-migration-tool/#step-5-create-and-open-the-ballerina-project","title":"Step 5: Create and open the Ballerina project","text":"<ol> <li>Configure your integration project:<ul> <li>Enter an Integration Name.</li> <li>Specify the Package Name for the Ballerina package.</li> <li>Select Integration Path where the project will be created.</li> <li>Choose whether to create a new directory using the package name.</li> </ul> </li> <li>Click \"Create and Open Project\" to generate the Ballerina integration project with the converted code.</li> </ol>"},{"location":"developer-guides/tools/migration-tools/mulesoft-migration-tool/#step-6-review-migration-output","title":"Step 6: Review migration output","text":"<p>The generated Ballerina package follows the standard Ballerina Integration (BI) file structure and includes:</p> <ul> <li>Generated Ballerina code with your converted MuleSoft logic.</li> <li>Configuration files (<code>Config.toml</code>, <code>Ballerina.toml</code>) for the new project.</li> <li>Organized code structure with separate files for connections, functions, types, and main logic.</li> </ul>"},{"location":"developer-guides/tools/migration-tools/mulesoft-migration-tool/#step-7-review-the-migration-summary","title":"Step 7: Review the migration summary","text":"<ul> <li>The migration assessment/summary report provides the following percentages:<ol> <li>Component conversion percentage - Shows the proportion of MuleSoft components successfully converted to Ballerina.</li> <li>DataWeave conversion percentage - Reflects the success rate of converting DataWeave scripts.</li> <li>Overall project conversion percentage \u2013 Combines both component and DataWeave conversion rates to indicate the total migration success.</li> </ol> </li> <li>The report includes a Manual work estimation section, which provides an estimated time required to review the migrated code, address TODOs, and complete the migration process.</li> <li>The report also features sections for Element blocks that require manual conversion and DataWeave expressions that require manual conversion, listing all Mule component blocks and DataWeave scripts unsupported by the current tool version and requiring manual conversion. These items are marked as TODOs in the appropriate locations within the generated Ballerina package.</li> </ul>"},{"location":"developer-guides/tools/migration-tools/mulesoft-migration-tool/#step-8-address-the-todo-items","title":"Step 8: Address the TODO items","text":"<p>During conversion, if there are any unsupported Mule XML tags, they are included in the generated Ballerina code as TODO comments. You may need to do the conversion for them manually.</p> <pre><code>public function endpoint(Context ctx) returns http:Response|error {\n\n    // TODO: UNSUPPORTED MULE BLOCK ENCOUNTERED. MANUAL CONVERSION REQUIRED.\n    // ------------------------------------------------------------------------\n    // &lt;db:select-unsupported config-ref=\"MySQL_Configuration\" xmlns:doc=\"http://www.mulesoft.org/schema/mule/documentation\" doc:name=\"Database\" xmlns:db=\"http://www.mulesoft.org/schema/mule/db\"&gt;\n    //             &lt;db:parameterized-query&gt;&lt;![CDATA[SELECT * from users;]]&gt;&lt;/db:parameterized-query&gt;\n    //         &lt;/db:select-unsupported&gt;\n    // ------------------------------------------------------------------------\n\n    log:printInfo(string `Users details: ${ctx.payload.toString()}`);\n\n    ctx.inboundProperties.response.setPayload(ctx.payload);\n    return ctx.inboundProperties.response;\n}\n</code></pre>"},{"location":"developer-guides/tools/migration-tools/mulesoft-migration-tool/#example-converting-a-standalone-mule-xml-file","title":"Example: Converting a standalone Mule XML file","text":"<p>Let's walk through an example of migrating a MuleSoft standalone sample <code>.xml</code> configuration to Ballerina.</p> <p>Here's a sample MuleSoft XML file (<code>users-database-query.xml</code>) that gets invoked via an HTTP listener and performs a database operation:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n\n&lt;mule xmlns:db=\"http://www.mulesoft.org/schema/mule/db\" xmlns:json=\"http://www.mulesoft.org/schema/mule/json\" xmlns:tracking=\"http://www.mulesoft.org/schema/mule/ee/tracking\" xmlns:http=\"http://www.mulesoft.org/schema/mule/http\" xmlns=\"http://www.mulesoft.org/schema/mule/core\" xmlns:doc=\"http://www.mulesoft.org/schema/mule/documentation\"\nxmlns:spring=\"http://www.springframework.org/schema/beans\"\nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\nxsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-current.xsd\nhttp://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/current/mule.xsd\nhttp://www.mulesoft.org/schema/mule/http http://www.mulesoft.org/schema/mule/http/current/mule-http.xsd\nhttp://www.mulesoft.org/schema/mule/ee/tracking http://www.mulesoft.org/schema/mule/ee/tracking/current/mule-tracking-ee.xsd\nhttp://www.mulesoft.org/schema/mule/db http://www.mulesoft.org/schema/mule/db/current/mule-db.xsd\nhttp://www.mulesoft.org/schema/mule/json http://www.mulesoft.org/schema/mule/json/current/mule-json.xsd\"&gt;\n&lt;http:listener-config name=\"config\" host=\"0.0.0.0\" port=\"8081\"  doc:name=\"HTTP Listener Configuration\" basePath=\"demo\"/&gt;\n&lt;db:mysql-config name=\"MySQL_Configuration\" host=\"localhost\" port=\"3306\" user=\"root\" password=\"admin123\" database=\"test_db\" doc:name=\"MySQL Configuration\"/&gt;\n&lt;flow name=\"demoFlow\"&gt;\n&lt;http:listener config-ref=\"config\" path=\"/users\" allowedMethods=\"GET\" doc:name=\"HTTP\"/&gt;\n&lt;db:select config-ref=\"MySQL_Configuration\" doc:name=\"Database\"&gt;\n&lt;db:parameterized-query&gt;&lt;![CDATA[SELECT * FROM users;]]&gt;&lt;/db:parameterized-query&gt;\n&lt;/db:select&gt;\n&lt;/flow&gt;\n&lt;/mule&gt;\n</code></pre>"},{"location":"developer-guides/tools/migration-tools/mulesoft-migration-tool/#run-the-migration-wizard","title":"Run the migration wizard","text":"<p>To convert the Mule XML file using the integrated migration wizard:</p> <ol> <li>Open WSO2 Integrator: BI in VS Code.</li> <li>Click \"Import External Integration\" on the welcome page.</li> <li>Select \"MuleSoft\" as the source platform.</li> <li>Use the file picker to select <code>/path/to/users-database-query.xml</code>.</li> <li>Set Force Version to \"3\" in the MuleSoft settings.</li> <li>Click \"Start Migration\" to begin the conversion process.</li> </ol>"},{"location":"developer-guides/tools/migration-tools/mulesoft-migration-tool/#examine-the-generated-ballerina-code","title":"Examine the generated Ballerina code","text":"<p>The tool generates a Ballerina package named <code>users-database-query-ballerina</code> inside <code>/path/to</code> with the following structure (Standard BI layout):</p> <pre><code>users-database-query-ballerina/\n\u251c\u2500\u2500 Ballerina.toml\n\u251c\u2500\u2500 Config.toml\n\u251c\u2500\u2500 configs.bal\n\u251c\u2500\u2500 connections.bal\n\u251c\u2500\u2500 functions.bal\n\u251c\u2500\u2500 main.bal\n\u2514\u2500\u2500 types.bal\n</code></pre> <p>The bal file contains the Ballerina translation of the original MuleSoft XML configuration. It sets up an HTTP service that listens on port 8081 and responds to <code>GET</code> <code>/users</code> requests by querying the MySQL database and returning the results as the response payload.</p> <p>For illustration purposes, the combined code from multiple Ballerina files in the package is summarized below.</p> <pre><code>import ballerina/http;\nimport ballerina/sql;\nimport ballerinax/mysql;\nimport ballerinax/mysql.driver as _;\n\npublic type Record record {\n};\n\nmysql:Client MySQL_Configuration = check new (\"localhost\", \"root\", \"admin123\", \"test_db\", 3306);\npublic listener http:Listener config = new (8081);\n\nservice /demo on config {\n    Context ctx;\n\n    function init() {\n        self.ctx = {payload: (), inboundProperties: {response: new, request: new, uriParams: {}}};\n    }\n\n    resource function get users(http:Request request) returns http:Response|error {\n        self.ctx.inboundProperties.request = request;\n        return invokeEndPoint0(self.ctx);\n    }\n}\n\npublic function invokeEndPoint0(Context ctx) returns http:Response|error {\n\n    // database operation\n    sql:ParameterizedQuery dbQuery0 = `SELECT * FROM users;`;\n    stream&lt;Record, sql:Error?&gt; dbStream0 = MySQL_Configuration-&gt;query(dbQuery0);\n    Record[] dbSelect0 = check from Record _iterator_ in dbStream0\n        select _iterator_;\n    ctx.payload = dbSelect0;\n\n    ctx.inboundProperties.response.setPayload(ctx.payload);\n    return ctx.inboundProperties.response;\n}\n</code></pre> <p>You can view the migration report using the \"View Full Report\" button in the migration wizard for an overview of the migration.</p> <p>This example demonstrates how to migrate a MuleSoft application that performs database operations to Ballerina using the migration tool. The migration tool automatically converts the database configuration and SQL query to the equivalent Ballerina code using the <code>ballerinax/mysql</code> module.</p>"},{"location":"developer-guides/tools/migration-tools/mulesoft-migration-tool/#supported-mule-components","title":"Supported Mule components","text":"<p>The migration tool currently supports a wide range of Mule components for both Mule 3.x and Mule 4.x. For a full list of supported components and their mappings, see:</p> <ul> <li>Mule 3.x Components</li> <li>Mule 4.x Components</li> </ul>"},{"location":"developer-guides/tools/migration-tools/mulesoft-migration-tool/#supported-dataweave-transformations","title":"Supported DataWeave transformations","text":"<p>The migration tool supports both DataWeave 1.0 (Mule 3.x) and DataWeave 2.0 (Mule 4.x) transformations. For details and conversion samples, see:</p> <ul> <li>DataWeave 1.0 Mappings</li> <li>DataWeave 2.0 Mappings</li> </ul>"},{"location":"developer-guides/tools/migration-tools/mulesoft-migration-tool/#limitations","title":"Limitations","text":"<ul> <li>Multi-project migration is not currently supported through the VS Code extension UI. For batch migration of multiple MuleSoft projects, use the CLI tool migrate-mule separately.</li> <li>Some moderate to advanced MuleSoft features may require manual adjustments after migration.</li> </ul> Disclaimer<p>MuleSoft: \"MuleSoft\", Mulesoft's \"Anypoint Platform\", and \"DataWeave\" are trademarks of MuleSoft LLC, a Salesforce company. All product, company names and marks mentioned herein are the property of their respective owners and are mentioned for identification purposes only.</p>"},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/","title":"TIBCO BusinessWorks Migration Tool","text":"<p>This guide explains how to use the integrated migration feature in WSO2 Integrator: BI to convert TIBCO BusinessWorks integrations into Ballerina packages.</p>"},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#overview","title":"Overview","text":"<p>This migration support is directly integrated into WSO2 Integrator: BI, providing a user-friendly wizard interface for converting TIBCO BusinessWorks projects. The tool accepts either a BusinessWorks project directory or a standalone process file as input and, generates equivalent Ballerina packages that can be opened directly in WSO2 Integrator: BI.</p> <p>The migration wizard provides:</p> <ul> <li>Interactive project selection with file picker support.</li> <li>Real-time migration status with detailed logs.</li> <li>Migration coverage reports showing conversion success rates.</li> <li>Automated project creation with the converted Ballerina code.</li> </ul>"},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#supported-businessworks-versions","title":"Supported BusinessWorks versions","text":"<p>The migration tool supports both BusinessWorks 5 and BusinessWorks 6 projects.</p>"},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#usage","title":"Usage","text":"<p>Follow the steps below to migrate your TIBCO BusinessWorks application using the integrated migration wizard.</p>"},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#step-1-prepare-your-input","title":"Step 1: Prepare your input","text":"<p>You can migrate a complete TIBCO BusinessWorks project or a standalone process file:</p> <ul> <li>For TIBCO BusinessWorks projects: Ensure your project follows the standard BusinessWorks structure</li> <li>For standalone process files: You can directly use any valid BusinessWorks process file.</li> </ul>"},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#step-2-launch-the-migration-wizard","title":"Step 2: Launch the migration wizard","text":"<ol> <li>Open WSO2 Integrator: BI in VS Code</li> <li> <p>Access the welcome page - If not automatically displayed, you can access it through the Command Palette (<code>Ctrl+Shift+P</code> / <code>Cmd+Shift+P</code>) and search for \"BI: Open Welcome\"</p> </li> <li> <p>Click \"Import External Integration\" in the \"Import External Integration\" section</p> </li> </ol>"},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#step-3-select-source-platform-and-project","title":"Step 3: Select source platform and project","text":"<ol> <li>Choose TIBCO as your source platform from the available options</li> <li>Select your project using the file picker:<ul> <li>For TIBCO BusinessWorks projects: Select the project root directory</li> <li>For standalone process files: Select the individual process file</li> </ul> </li> <li>Configure TIBCO settings if available in the wizard</li> <li>Click \"Start Migration\" to begin the conversion process</li> </ol>"},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#step-4-monitor-migration-progress-and-review-results","title":"Step 4: Monitor migration progress and review results","text":""},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#during-migration-real-time-progress-monitoring","title":"During Migration: Real-time Progress Monitoring","text":"<p>While the migration is ongoing, you will see:</p> <ul> <li>Real-time migration status updates</li> <li>Detailed logs of the conversion process</li> <li>Progress indication showing current migration step</li> </ul>"},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#after-migration-coverage-report-and-results","title":"After Migration: Coverage Report and Results","text":"<p>Once the migration process completes, the same page updates to show:</p> <ul> <li>Migration Coverage: Percentage showing successful conversion rate.</li> <li>Total code lines: Number of lines processed.</li> <li>Migratable vs Non-migratable code lines: Breakdown of conversion success.</li> <li>View Full Report: Click this button to view the detailed migration report in your browser.</li> <li>Save Report: Click this button to save the migration report to your local file system for future reference.</li> </ul>"},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#step-5-create-and-open-the-ballerina-project","title":"Step 5: Create and open the Ballerina project","text":"<ol> <li>Configure your integration project:<ul> <li>Enter an Integration Name.</li> <li>Specify the Package Name for the Ballerina package.</li> <li>Select Integration Path where the project will be created.</li> <li>Choose whether to create a new directory using the package name.</li> </ul> </li> <li>Click \"Create and Open Project\" to generate the Ballerina integration project with the converted code.</li> </ol>"},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#step-6-review-migration-output","title":"Step 6: Review migration output","text":"<p>The generated Ballerina package follows the standard Ballerina Integration (BI) file structure and includes:</p> <ul> <li>Generated Ballerina code with your converted TIBCO BusinessWorks logic.</li> <li>Configuration files (<code>Config.toml</code>, <code>Ballerina.toml</code>) for the new project.</li> <li>Organized code structure with separate files for connections, functions, types, and main logic.</li> </ul> <p>Note: The migration report is no longer automatically saved to the project directory. Instead, use the \"View Full Report\" button during the migration process to view the report, or \"Save Report\" to save it to your desired location.</p>"},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#step-7-review-the-migration-summary","title":"Step 7: Review the migration summary","text":"<p>The migration report provides comprehensive metrics:</p> <ol> <li>Component conversion percentage - Shows the proportion of TIBCO components successfully converted to Ballerina.</li> <li>Overall project conversion percentage \u2013 Indicates the total migration success.</li> <li>Manual work estimation - Estimated time required to review migrated code and address TODOs.</li> <li>Activities requiring manual conversion - Lists unsupported TIBCO activities.</li> </ol>"},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#step-8-address-the-todo-items","title":"Step 8: Address the TODO items","text":"<p>During conversion, if there are any unsupported TIBCO activities or components, they are included in the generated Ballerina code as TODO comments. You may need to do the conversion for them manually.</p>"},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#examples","title":"Examples","text":""},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#migrating-tibco-businessworks-5-process","title":"Migrating TIBCO BusinessWorks 5 process","text":""},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#step-1-prepare-the-migration-files","title":"Step 1: Prepare the migration files","text":"<ol> <li> <p>Create new directory named <code>tibco-hello-world</code> with following two files.</p> helloworld.process<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;pd:ProcessDefinition xmlns:pd=\"http://xmlns.tibco.com/bw/process/2003\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" xmlns:ns=\"http://www.tibco.com/pe/EngineTypes\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\"&gt;\n&lt;pd:name&gt;Processes/simpleResponse&lt;/pd:name&gt;\n&lt;pd:startName&gt;HTTP Receiver&lt;/pd:startName&gt;\n&lt;pd:starter name=\"HTTP Receiver\"&gt;\n&lt;pd:type&gt;com.tibco.plugin.http.HTTPEventSource&lt;/pd:type&gt;\n&lt;pd:resourceType&gt;httppalette.httpEventSource&lt;/pd:resourceType&gt;\n&lt;config&gt;\n&lt;outputMode&gt;String&lt;/outputMode&gt;\n&lt;inputOutputVersion&gt;5.3.0&lt;/inputOutputVersion&gt;\n&lt;sharedChannel&gt;GeneralConnection.sharedhttp&lt;/sharedChannel&gt;\n&lt;parsePostData&gt;true&lt;/parsePostData&gt;\n&lt;Headers/&gt;\n&lt;/config&gt;\n&lt;pd:inputBindings/&gt;\n&lt;/pd:starter&gt;\n&lt;pd:endName&gt;End&lt;/pd:endName&gt;\n&lt;pd:errorSchemas/&gt;\n&lt;pd:processVariables/&gt;\n&lt;pd:targetNamespace&gt;http://xmlns.example.com/simpleResponse&lt;/pd:targetNamespace&gt;\n&lt;pd:activity name=\"HTTP Response\"&gt;\n&lt;pd:type&gt;com.tibco.plugin.http.HTTPResponseActivity&lt;/pd:type&gt;\n&lt;pd:resourceType&gt;httppalette.httpResponseActivity&lt;/pd:resourceType&gt;\n&lt;config&gt;\n&lt;responseHeader&gt;\n&lt;header name=\"Content-Type\"&gt;text/xml; charset=UTF-8&lt;/header&gt;\n&lt;/responseHeader&gt;\n&lt;httpResponseCode&gt;200&lt;/httpResponseCode&gt;\n&lt;/config&gt;\n&lt;pd:inputBindings&gt;\n&lt;ResponseActivityInput&gt;\n&lt;asciiContent&gt;\n&lt;response&gt;hello world&lt;/response&gt;\n&lt;/asciiContent&gt;\n&lt;/ResponseActivityInput&gt;\n&lt;/pd:inputBindings&gt;\n&lt;/pd:activity&gt;\n\n&lt;pd:transition&gt;\n&lt;pd:from&gt;HTTP Receiver&lt;/pd:from&gt;\n&lt;pd:to&gt;HTTP Response&lt;/pd:to&gt;\n&lt;pd:lineType&gt;Default&lt;/pd:lineType&gt;\n&lt;/pd:transition&gt;\n\n&lt;pd:transition&gt;\n&lt;pd:from&gt;HTTP Response&lt;/pd:from&gt;\n&lt;pd:to&gt;End&lt;/pd:to&gt;\n&lt;pd:lineType&gt;Default&lt;/pd:lineType&gt;\n&lt;/pd:transition&gt;\n&lt;/pd:ProcessDefinition&gt;\n</code></pre> GeneralConnection.sharedhttp<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;ns0:httpSharedResource xmlns:ns0=\"www.tibco.com/shared/HTTPConnection\"&gt;\n&lt;config&gt;\n&lt;Host&gt;localhost&lt;/Host&gt;\n&lt;Port&gt;9090&lt;/Port&gt;\n&lt;/config&gt;\n&lt;/ns0:httpSharedResource&gt;\n</code></pre> </li> </ol>"},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#step-2-run-the-migration-wizard","title":"Step 2: Run the migration wizard","text":"<ol> <li>Open WSO2 Integrator: BI in VS Code.</li> <li>Click \"Import External Integration\" on the welcome page.</li> <li>Select \"TIBCO\" as the source platform.</li> <li>Use the file picker to select the <code>tibco-hello-world</code> project directory.</li> <li>Click \"Start Migration\" to begin the conversion process.</li> <li>Configure your integration project details and click \"Create and Open Project\".</li> </ol>"},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#step-3-review-the-generated-code","title":"Step 3: Review the generated code","text":"<p>The migration wizard will create a new Ballerina project with the converted code, which you can immediately start working with in the BI interface.</p>"},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#migrating-tibco-businessworks-6-process","title":"Migrating TIBCO BusinessWorks 6 process","text":""},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#step-1-prepare-the-migration-files_1","title":"Step 1: Prepare the migration files","text":"<ol> <li> <p>Create new directory named <code>tibco-hello-world</code> with following process file.</p> main.bwp<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;bpws:process exitOnStandardFault=\"no\"\nname=\"test.api.MainProcess\" suppressJoinFailure=\"yes\"\ntargetNamespace=\"http://xmlns.example.com/test/api\"\nxmlns:bpws=\"http://docs.oasis-open.org/wsbpel/2.0/process/executable\"\nxmlns:info=\"http://www.tibco.com/bw/process/info\"\nxmlns:ns=\"http://www.tibco.com/pe/EngineTypes\"\nxmlns:ns0=\"http://xmlns.example.com/test/api/wsdl\"\nxmlns:ns1=\"http://xmlns.example.com/test/api\"\nxmlns:sca=\"http://docs.oasis-open.org/ns/opencsa/sca/200912\"\nxmlns:sca-bpel=\"http://docs.oasis-open.org/ns/opencsa/sca-bpel/200801\"\nxmlns:tibex=\"http://www.tibco.com/bpel/2007/extensions\"\nxmlns:tibprop=\"http://ns.tibco.com/bw/property\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\"&gt;\n&lt;tibex:Types&gt;\n&lt;xs:schema attributeFormDefault=\"unqualified\"\nelementFormDefault=\"qualified\"\ntargetNamespace=\"http://www.tibco.com/pe/EngineTypes\"\nxmlns:tns=\"http://www.tibco.com/pe/EngineTypes\" xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"&gt;\n&lt;xs:complexType block=\"extension restriction\"\nfinal=\"extension restriction\" name=\"ProcessContext\"&gt;\n&lt;xs:sequence&gt;\n&lt;xs:element\nblock=\"extension restriction substitution\"\nform=\"unqualified\" name=\"JobId\" type=\"xs:string\"/&gt;\n&lt;xs:element\nblock=\"extension restriction substitution\"\nform=\"unqualified\" name=\"ApplicationName\" type=\"xs:string\"/&gt;\n&lt;xs:element\nblock=\"extension restriction substitution\"\nform=\"unqualified\" name=\"EngineName\" type=\"xs:string\"/&gt;\n&lt;xs:element\nblock=\"extension restriction substitution\"\nform=\"unqualified\" name=\"ProcessInstanceId\" type=\"xs:string\"/&gt;\n&lt;xs:element\nblock=\"extension restriction substitution\"\nform=\"unqualified\" minOccurs=\"0\"\nname=\"CustomJobId\" type=\"xs:string\"/&gt;\n&lt;xs:element\nblock=\"extension restriction substitution\"\nform=\"unqualified\" maxOccurs=\"unbounded\"\nminOccurs=\"0\" name=\"TrackingInfo\" type=\"xs:string\"/&gt;\n&lt;/xs:sequence&gt;\n&lt;/xs:complexType&gt;\n&lt;xs:element block=\"extension restriction substitution\"\nfinal=\"extension restriction\" name=\"ProcessContext\" type=\"tns:ProcessContext\"/&gt;\n&lt;/xs:schema&gt;\n&lt;xs:schema attributeFormDefault=\"unqualified\"\nelementFormDefault=\"qualified\"\ntargetNamespace=\"http://xmlns.example.com/test/api\"\nxmlns:tns=\"http://xmlns.example.com/test/api\" xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"&gt;\n&lt;xs:complexType name=\"TestRequestType\"&gt;\n&lt;xs:sequence&gt;\n&lt;xs:element name=\"request\" type=\"xs:string\"/&gt;\n&lt;/xs:sequence&gt;\n&lt;/xs:complexType&gt;\n&lt;xs:complexType name=\"TestResponseType\"&gt;\n&lt;xs:sequence&gt;\n&lt;xs:element name=\"response\" type=\"xs:string\"/&gt;\n&lt;/xs:sequence&gt;\n&lt;/xs:complexType&gt;\n&lt;xs:element name=\"TestRequest\" type=\"tns:TestRequestType\"/&gt;\n&lt;xs:element name=\"TestResponse\" type=\"tns:TestResponseType\"/&gt;\n&lt;/xs:schema&gt;\n&lt;wsdl:definitions\ntargetNamespace=\"http://xmlns.example.com/test/api/wsdl\"\nxmlns:extns=\"http://tns.tibco.com/bw/REST\"\nxmlns:extns1=\"http://xmlns.example.com/test/api\"\nxmlns:plnk=\"http://docs.oasis-open.org/wsbpel/2.0/plnktype\"\nxmlns:tibex=\"http://www.tibco.com/bpel/2007/extensions\"\nxmlns:tns=\"http://xmlns.example.com/test/api/wsdl\"\nxmlns:vprop=\"http://docs.oasis-open.org/wsbpel/2.0/varprop\"\nxmlns:wsdl=\"http://schemas.xmlsoap.org/wsdl/\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\"&gt;\n&lt;plnk:partnerLinkType name=\"partnerLinkType\"&gt;\n&lt;plnk:role name=\"use\" portType=\"tns:testapi\"/&gt;\n&lt;/plnk:partnerLinkType&gt;\n&lt;wsdl:import namespace=\"http://tns.tibco.com/bw/REST\"/&gt;\n&lt;wsdl:import namespace=\"http://xmlns.example.com/test/api\"/&gt;\n&lt;wsdl:message name=\"postRequest\"&gt;\n&lt;wsdl:part element=\"extns1:TestRequest\"\nname=\"item\" tibex:hasMultipleNamespaces=\"false\"/&gt;\n&lt;wsdl:part element=\"extns:httpHeaders\"\nname=\"httpHeaders\" tibex:source=\"bw.rest\"/&gt;\n&lt;/wsdl:message&gt;\n&lt;wsdl:message name=\"postResponse\"&gt;\n&lt;wsdl:part element=\"extns1:TestResponse\"\nname=\"item\" tibex:hasMultipleNamespaces=\"false\"/&gt;\n&lt;/wsdl:message&gt;\n&lt;wsdl:message name=\"post4XXFaultMessage\"&gt;\n&lt;wsdl:part element=\"extns:client4XXError\" name=\"clientError\"/&gt;\n&lt;/wsdl:message&gt;\n&lt;wsdl:message name=\"post5XXFaultMessage\"&gt;\n&lt;wsdl:part element=\"extns:server5XXError\" name=\"serverError\"/&gt;\n&lt;/wsdl:message&gt;\n&lt;wsdl:portType name=\"testapi\"\ntibex:bw.rest.apipath=\"/test\"\ntibex:bw.rest.basepath=\"TestAPI\"\ntibex:bw.rest.resource=\"Service Descriptors/test.api.MainProcess-TestAPI.json\"\ntibex:bw.rest.resource.source=\"generated\" tibex:source=\"bw.rest.service\"&gt;\n&lt;wsdl:documentation&gt;Simple REST API with test endpoint.&lt;/wsdl:documentation&gt;\n&lt;wsdl:operation name=\"post\"&gt;\n&lt;wsdl:input message=\"tns:postRequest\" name=\"postInput\"/&gt;\n&lt;wsdl:output message=\"tns:postResponse\" name=\"postOutput\"/&gt;\n&lt;wsdl:fault message=\"tns:post4XXFaultMessage\" name=\"clientFault\"/&gt;\n&lt;wsdl:fault message=\"tns:post5XXFaultMessage\" name=\"serverFault\"/&gt;\n&lt;/wsdl:operation&gt;\n&lt;/wsdl:portType&gt;\n&lt;/wsdl:definitions&gt;\n&lt;/tibex:Types&gt;\n&lt;tibex:ProcessInfo callable=\"false\" createdBy=\"heshan\"\ncreatedOn=\"Mon Dec 16 00:00:00 PST 2024\" description=\"\"\nextraErrorVars=\"true\" modifiers=\"public\"\nproductVersion=\"6.5.0 V63 2018-08-08\" scalable=\"true\"\nsingleton=\"true\" stateless=\"true\" type=\"IT\"/&gt;\n&lt;tibex:ProcessInterface context=\"\" input=\"\" output=\"\"/&gt;\n&lt;tibex:ProcessTemplateConfigurations/&gt;\n&lt;tibex:NamespaceRegistry enabled=\"true\"&gt;\n&lt;tibex:namespaceItem\nnamespace=\"http://xmlns.example.com/test/api\" prefix=\"tns\"/&gt;\n&lt;tibex:namespaceItem\nnamespace=\"http://xmlns.example.com/test/api/wsdl\" prefix=\"tns1\"/&gt;\n&lt;/tibex:NamespaceRegistry&gt;\n&lt;bpws:import importType=\"http://www.w3.org/2001/XMLSchema\" namespace=\"http://tns.tibco.com/bw/REST\"/&gt;\n&lt;bpws:import importType=\"http://www.w3.org/2001/XMLSchema\" namespace=\"http://xmlns.example.com/test/api\"/&gt;\n&lt;bpws:partnerLinks&gt;\n&lt;bpws:partnerLink myRole=\"use\" name=\"testapi\"\npartnerLinkType=\"ns0:partnerLinkType\"\nsca-bpel:ignore=\"false\" sca-bpel:service=\"testapi\"/&gt;\n&lt;/bpws:partnerLinks&gt;\n&lt;bpws:variables&gt;\n&lt;bpws:variable element=\"ns:ProcessContext\"\nname=\"_processContext\" sca-bpel:internal=\"true\"/&gt;\n&lt;bpws:variable messageType=\"ns0:postRequest\" name=\"post\" sca-bpel:internal=\"true\"/&gt;\n&lt;bpws:variable messageType=\"ns0:postResponse\"\nname=\"postOut-input\" sca-bpel:internal=\"true\"/&gt;\n&lt;bpws:variable element=\"ns1:TestResponse\" name=\"RenderOutput-output\" sca-bpel:internal=\"true\"/&gt;\n&lt;/bpws:variables&gt;\n&lt;bpws:extensions&gt;\n&lt;bpws:extension mustUnderstand=\"no\" namespace=\"http://www.eclipse.org/gmf/runtime/1.0.2/notation\"/&gt;\n&lt;bpws:extension mustUnderstand=\"no\" namespace=\"http://www.tibco.com/bw/process/info\"/&gt;\n&lt;bpws:extension mustUnderstand=\"no\" namespace=\"http://docs.oasis-open.org/ns/opencsa/sca-bpel/200801\"/&gt;\n&lt;bpws:extension mustUnderstand=\"no\" namespace=\"http://docs.oasis-open.org/ns/opencsa/sca/200912\"/&gt;\n&lt;bpws:extension mustUnderstand=\"no\" namespace=\"http://ns.tibco.com/bw/property\"/&gt;\n&lt;bpws:extension mustUnderstand=\"no\" namespace=\"http://www.tibco.com/bpel/2007/extensions\"/&gt;\n&lt;/bpws:extensions&gt;\n&lt;bpws:scope name=\"scope\"&gt;\n&lt;bpws:flow name=\"flow\"&gt;\n&lt;bpws:links/&gt;\n&lt;bpws:pick createInstance=\"yes\" name=\"pick\"&gt;\n&lt;bpws:onMessage operation=\"post\"\npartnerLink=\"testapi\"\nportType=\"ns0:testapi\"\nvariable=\"post\"&gt;\n&lt;bpws:scope name=\"scope1\"&gt;\n&lt;bpws:flow name=\"flow1\"&gt;\n&lt;bpws:links&gt;\n&lt;bpws:link name=\"JSONPayloadOut\" tibex:linkType=\"SUCCESS\"/&gt;\n&lt;/bpws:links&gt;\n&lt;bpws:extensionActivity&gt;\n&lt;tibex:activityExtension name=\"RenderOutput\" outputVariable=\"RenderOutput\"\nxmlns:tibex=\"http://www.tibco.com/bpel/2007/extensions\"&gt;\n&lt;bpws:targets/&gt;\n&lt;bpws:sources&gt;\n&lt;bpws:source linkName=\"JSONPayloadOut\"/&gt;\n&lt;/bpws:sources&gt;\n&lt;tibex:inputBindings&gt;\n&lt;tibex:inputBinding expression=\"&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&gt;&amp;#xa;&amp;lt;xsl:stylesheet xmlns:xsl=&amp;quot;http://www.w3.org/1999/XSL/Transform&amp;quot; xmlns:tns=&amp;quot;http://xmlns.example.com/test/api&amp;quot; version=&amp;quot;2.0&amp;quot;&gt;&amp;#xa;    &amp;lt;xsl:template name=&amp;quot;RenderOutput-input&amp;quot; match=&amp;quot;/&amp;quot;&gt;&amp;#xa;        &amp;lt;tns:TestResponse&gt;&amp;#xa;            &amp;lt;tns:response&gt;Hello world&amp;lt;/tns:response&gt;&amp;#xa;        &amp;lt;/tns:TestResponse&gt;&amp;#xa;    &amp;lt;/xsl:template&gt;&amp;#xa;&amp;lt;/xsl:stylesheet&gt;\" expressionLanguage=\"urn:oasis:names:tc:wsbpel:2.0:sublang:xslt1.0\"/&gt;\n&lt;/tibex:inputBindings&gt;\n&lt;tibex:config&gt;\n&lt;bwext:BWActivity activityTypeID=\"bw.restjson.JsonRender\"\nxmlns:activityconfig=\"http://tns.tibco.com/bw/model/activityconfig\"\nxmlns:bwext=\"http://tns.tibco.com/bw/model/core/bwext\"\nxmlns:restjson=\"http://ns.tibco.com/bw/palette/restjson\"\nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"&gt;\n&lt;activityConfig&gt;\n&lt;properties name=\"config\" xsi:type=\"activityconfig:EMFProperty\"&gt;\n&lt;type href=\"http://ns.tibco.com/bw/palette/restjson#//JsonRender\"/&gt;\n&lt;value jsonOutputStyle=\"None\" schemaType=\"Xsd\" xsi:type=\"restjson:JsonRender\"&gt;\n&lt;inputEditorElement href=\"Schema.xsd#//TestResponse;XSDElementDeclaration\"/&gt;\n&lt;/value&gt;\n&lt;/properties&gt;\n&lt;/activityConfig&gt;\n&lt;/bwext:BWActivity&gt;\n&lt;/tibex:config&gt;\n&lt;/tibex:activityExtension&gt;\n&lt;/bpws:extensionActivity&gt;\n&lt;bpws:extensionActivity&gt;\n&lt;tibex:activityExtension\ninputVariable=\"RenderOutput\"\nname=\"SendHTTPResponse\"\nxmlns:tibex=\"http://www.tibco.com/bpel/2007/extensions\"&gt;\n&lt;bpws:targets&gt;\n&lt;bpws:target linkName=\"JSONPayloadOut\"/&gt;\n&lt;/bpws:targets&gt;\n&lt;tibex:inputBindings&gt;\n&lt;tibex:inputBinding\nexpression=\"&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&gt;&amp;#xa;&amp;lt;xsl:stylesheet xmlns:xsl=&amp;quot;http://www.w3.org/1999/XSL/Transform&amp;quot; xmlns:tns1=&amp;quot;http://tns.tibco.com/bw/activity/sendhttpresponse/xsd/input+3847aa9b-8275-4b15-9ea8-812816768fa4+ResponseActivityInput&amp;quot; version=&amp;quot;2.0&amp;quot;&gt;&amp;#xa;    &amp;lt;xsl:template name=&amp;quot;SendHTTPResponse-input&amp;quot; match=&amp;quot;/&amp;quot;&gt;&amp;#xa;        &amp;lt;tns1:ResponseActivityInput&gt;&amp;#xa;            &amp;lt;asciiContent&gt;&amp;#xa;                &amp;lt;xsl:value-of select=&amp;quot;/jsonString&amp;quot;/&gt;&amp;#xa;            &amp;lt;/asciiContent&gt;&amp;#xa;            &amp;lt;Headers&gt;&amp;#xa;                &amp;lt;Content-Type&gt;&amp;#xa;                    &amp;lt;xsl:value-of select=&amp;quot;&amp;amp;quot;application/json&amp;amp;quot;&amp;quot;/&gt;&amp;#xa;                &amp;lt;/Content-Type&gt;&amp;#xa;            &amp;lt;/Headers&gt;&amp;#xa;        &amp;lt;/tns1:ResponseActivityInput&gt;&amp;#xa;    &amp;lt;/xsl:template&gt;&amp;#xa;&amp;lt;/xsl:stylesheet&gt;\"\nexpressionLanguage=\"urn:oasis:names:tc:wsbpel:2.0:sublang:xslt1.0\"/&gt;\n&lt;/tibex:inputBindings&gt;\n&lt;tibex:config&gt;\n&lt;bwext:BWActivity\nactivityTypeID=\"bw.http.sendHTTPResponse\"\nversion=\"6.0.0.20132205\"\nxmlns:ResponseActivityInput=\"http://tns.tibco.com/bw/activity/sendhttpresponse/xsd/input+3847aa9b-8275-4b15-9ea8-812816768fa4+ResponseActivityInput\"\nxmlns:activityconfig=\"http://tns.tibco.com/bw/model/activityconfig\"\nxmlns:bwext=\"http://tns.tibco.com/bw/model/core/bwext\"\nxmlns:http=\"http://ns.tibco.com/bw/palette/http\"\nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"&gt;\n&lt;activityConfig&gt;\n&lt;properties name=\"config\" xsi:type=\"activityconfig:EMFProperty\"&gt;\n&lt;type href=\"http://ns.tibco.com/bw/palette/http#//SendHTTPResponse\"/&gt;\n&lt;value closeConnection=\"true\"\ninputHeadersQName=\"ResponseActivityInput:headersType\"\nreplyFor=\"HTTPReceiver\" xsi:type=\"http:SendHTTPResponse\"/&gt;\n&lt;/properties&gt;\n&lt;/activityConfig&gt;\n&lt;/bwext:BWActivity&gt;\n&lt;/tibex:config&gt;\n&lt;/tibex:activityExtension&gt;\n&lt;/bpws:extensionActivity&gt;\n&lt;/bpws:flow&gt;\n&lt;/bpws:scope&gt;\n&lt;/bpws:onMessage&gt;\n&lt;/bpws:pick&gt;\n&lt;/bpws:flow&gt;\n&lt;/bpws:scope&gt;\n&lt;/bpws:process&gt;\n</code></pre> </li> </ol>"},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#step-2-run-the-migration-wizard_1","title":"Step 2: Run the migration wizard","text":"<ol> <li>Open WSO2 Integrator: BI in VS Code.</li> <li>Click \"Import External Integration\" on the welcome page.</li> <li>Select \"TIBCO\" as the source platform.</li> <li>Use the file picker to select the <code>tibco-hello-world</code> project directory.</li> <li>Click \"Start Migration\" to begin the conversion process.</li> <li>Configure your integration project details and click \"Create and Open Project\".</li> </ol>"},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#step-3-review-the-generated-code_1","title":"Step 3: Review the generated code","text":"<p>The migration wizard will create a new Ballerina project with the converted code, which you can immediately start working with in the BI interface.</p>"},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#limitations","title":"Limitations","text":"<ul> <li>Multi-project migration is not currently supported through the VS Code extension UI. For batch migration of multiple TIBCO projects, use the CLI tool migrate-tibco separately.</li> <li>Tool generates code assuming target compiler version is 2201.12.0 or later.</li> </ul>"},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#unhandled-activities","title":"Unhandled activities","text":"<ul> <li> <p>If the tool encounters any activity which it does not know how to convert it will generate a placeholder \"unhandled\" function with a comment containing the relevant part of the process file.</p> <pre><code>function unhandled(map&lt;xml&gt; context) returns xml|error {\n    //FIXME: [ParseError] : Unknown activity\n    //&lt;bpws:empty name=\"OnMessageStart\" xmlns:tibex=\"http://www.tibco.com/bpel/2007/extensions\" tibex:constructor=\"onMessageStart\" tibex:xpdlId=\"c266c167-7a80-40cc-9db2-60739386deeb\" xmlns:bpws=\"http://docs.oasis-open.org/wsbpel/2.0/process/executable\"/&gt;\n\n    //&lt;bpws:empty name=\"OnMessageStart\" xmlns:tibex=\"http://www.tibco.com/bpel/2007/extensions\" tibex:constructor=\"onMessageStart\" tibex:xpdlId=\"c266c167-7a80-40cc-9db2-60739386deeb\" xmlns:bpws=\"http://docs.oasis-open.org/wsbpel/2.0/process/executable\"/&gt;\n    return xml `&lt;root&gt;&lt;/root&gt;`;\n}\n</code></pre> </li> </ul>"},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#partially-supported-activities","title":"Partially supported activities","text":"<ul> <li> <p>In case of activities that are only partially supported you will see a log message with the activity name.     <pre><code>WARNING: Partially supported activity: JMS Send\n</code></pre></p> </li> <li> <p>They will also be listed in the report under heading \"Activities that need manual validation\". For most typical use cases, you can use the converted source as is, but we highly encourage users to check the converted code. There will be comments explaining any limitations/assumptions the tool has made.     <pre><code>    // WARNING: using default destination configuration\n    jms:MessageProducer var4 = check var3.createProducer(destination = {\n        'type: jms:TOPIC,\n        name: \"TOPIC\"\n    });\n</code></pre></p> </li> </ul>"},{"location":"developer-guides/tools/migration-tools/tibco-businessworks-migration-tool/#supported-tibco-businessworks-activities","title":"Supported TIBCO BusinessWorks activities","text":"<ul> <li><code>invoke</code></li> <li><code>pick</code></li> <li><code>empty</code></li> <li><code>reply</code></li> <li><code>throw</code></li> <li><code>assign</code></li> <li><code>forEach</code></li> <li><code>extensionActivity</code></li> <li><code>receiveEvent</code></li> <li><code>activityExtension</code><ul> <li><code>bw.internal.end</code></li> <li><code>bw.http.sendHTTPRequest</code></li> <li><code>bw.restjson.JsonRender</code></li> <li><code>bw.restjson.JsonParser</code></li> <li><code>bw.http.sendHTTPResponse</code></li> <li><code>bw.file.write</code></li> <li><code>bw.generalactivities.log</code></li> <li><code>bw.xml.renderxml</code></li> <li><code>bw.generalactivities.mapper</code></li> <li><code>bw.internal.accumulateend</code></li> </ul> </li> <li><code>extActivity</code></li> <li><code>com.tibco.plugin.mapper.MapperActivity</code></li> <li><code>com.tibco.plugin.http.HTTPEventSource</code></li> <li><code>com.tibco.pe.core.AssignActivity</code></li> <li><code>com.tibco.plugin.http.HTTPResponseActivity</code></li> <li><code>com.tibco.plugin.xml.XMLRendererActivity</code></li> <li><code>com.tibco.plugin.xml.XMLParseActivity</code></li> <li><code>com.tibco.pe.core.LoopGroup</code></li> <li><code>com.tibco.pe.core.WriteToLogActivity</code></li> <li><code>com.tibco.pe.core.CatchActivity</code></li> <li><code>com.tibco.plugin.file.FileReadActivity</code></li> <li><code>com.tibco.plugin.file.FileWriteActivity</code></li> <li><code>com.tibco.plugin.jdbc.JDBCGeneralActivity</code></li> <li><code>com.tibco.plugin.json.activities.RestActivity</code></li> <li><code>com.tibco.pe.core.CallProcessActivity</code></li> <li><code>com.tibco.plugin.soap.SOAPSendReceiveActivity</code></li> <li><code>com.tibco.plugin.json.activities.JSONParserActivity</code></li> <li><code>com.tibco.plugin.json.activities.JSONRenderActivity</code></li> <li><code>com.tibco.plugin.soap.SOAPSendReplyActivity</code></li> <li><code>com.tibco.plugin.jms.JMSQueueEventSource</code></li> <li><code>com.tibco.plugin.jms.JMSQueueSendActivity</code></li> <li><code>com.tibco.plugin.jms.JMSQueueGetMessageActivity</code></li> <li><code>com.tibco.plugin.jms.JMSTopicPublishActivity</code></li> <li><code>com.tibco.pe.core.GenerateErrorActivity</code></li> <li><code>com.tibco.plugin.timer.NullActivity</code></li> <li><code>com.tibco.plugin.timer.SleepActivity</code></li> <li><code>com.tibco.pe.core.GetSharedVariableActivity</code></li> <li><code>com.tibco.pe.core.SetSharedVariableActivity</code></li> <li><code>com.tibco.plugin.file.FileEventSource</code></li> <li><code>com.tibco.pe.core.OnStartupEventSource</code></li> <li><code>com.tibco.plugin.file.ListFilesActivity</code></li> <li><code>com.tibco.plugin.xml.XMLTransformActivity</code></li> </ul> Disclaimer<p>TIBCO: \"TIBCO\", \u201cTIBCO BusinessWorks\u201d, and \u201cTIBCO Flogo\u201d are trademarks, or registered trademarks, of TIBCO Software Inc. a business unit of Cloud Software Group. All product, company names and marks mentioned herein are the property of their respective owners and are mentioned for identification purposes only.</p>"},{"location":"developer-guides/tools/other-tools/scan-tool/","title":"Scan Tool","text":"<p>The scan tool is a static code analysis tool that performs analysis on BI projects and identifies potential code smells, bugs, and vulnerabilities without executing them.</p> Note<p>Ballerina scan is an experimental feature that supports only a limited set of rules.</p>"},{"location":"developer-guides/tools/other-tools/scan-tool/#install-the-tool","title":"Install the tool","text":"<p>Execute the command below to pull the scan tool from Ballerina Central.</p> <pre><code>$ bal tool pull scan\n</code></pre> <p>To learn more about managing Ballerina tools, refer to the Ballerina CLI tool command documentation.</p>"},{"location":"developer-guides/tools/other-tools/scan-tool/#usage-guide-for-the-scan-tool","title":"Usage guide for the scan tool","text":"<p>The scan tool helps you analyze your BI project for potential issues, enforce coding standards, and generate detailed reports. </p> <p>The scan tool supports several command-line options as follows.</p> <pre><code>$ bal scan [--target-dir] &lt;target-dir&gt;\n        [--scan-report] [--list-rules]\n[--include-rules] &lt;id(s)-of-rule(s)-to-include&gt;\n        [--exclude-rules] &lt;id(s)-of-rule(s)-to-exclude&gt;\n        [--platforms] &lt;platform(s)-to-report-results&gt;\n</code></pre> <p>Below are various ways you can use the tool to fit your development workflow.</p>"},{"location":"developer-guides/tools/other-tools/scan-tool/#scan-a-bi-project","title":"Scan a BI project","text":"<p>To run a full analysis across all Ballerina files in your BI project, use the following command in terminal.</p> <pre><code>$ bal scan --scan-report\n</code></pre> <p>This will produce the HTML report and scan results inside the <code>target/report</code> directory.</p> <p>The report includes a summary of the number of code smells, bugs, and vulnerabilities found in each file.</p> <p></p> <p>To investigate further, you can click on a file name to view a detailed breakdown of the issues. This view highlights the exact lines where problems were detected, along with a description, and the severity level.</p> <p></p>"},{"location":"developer-guides/tools/other-tools/scan-tool/#list-all-available-analysis-rules","title":"List all available analysis rules","text":"<p>If you\u2019d like to explore the full set of rules the tool can apply, run:</p> <pre><code>$ bal scan --list-rules\n</code></pre> <p>This will display a comprehensive list of available rules for your project, which you can include or exclude in future scans.</p> <p>The output will look something like this:</p> <p></p> Note<p>The list of displayed rules is specific to the current BI project and is determined based on its dependencies.</p>"},{"location":"developer-guides/tools/other-tools/scan-tool/#run-analysis-for-specific-rules","title":"Run analysis for specific rules","text":"<p>If you want to apply a specific set of rules, list them as a comma-separated string by specifying the rule ID:</p> <pre><code>$ bal scan --include-rules=\"ballerina:1, ballerina/io:2\"\n</code></pre> <p>To ignore a specific set of rules during the analysis, use the following command:</p> <pre><code>$ bal scan --exclude-rules=\"ballerina:1, ballerina/io:2\"\n</code></pre>"},{"location":"developer-guides/tools/other-tools/scan-tool/#publishing-static-code-analysis-reports-to-sonarqube","title":"Publishing static code analysis reports to SonarQube.","text":"<p>To learn how to publish reports to SonarQube, refer to Configuration for Platform Plugins.</p>"},{"location":"get-started/develop-ai-agent/","title":"Develop AI Agent","text":""},{"location":"get-started/develop-ai-agent/#overview","title":"Overview","text":"<p>In this guide, you will: Create a simple AI agent that provides personal assistance. We will define a GraphQL schema with a query that invokes the inline agent to generate dynamic responses based on input parameters. The agent runs within the resolver logic and returns results directly as part of the GraphQL response.</p>"},{"location":"get-started/develop-ai-agent/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have the following:</p> <ul> <li>Visual Studio Code: Install Visual Studio Code if you don't have it already.</li> <li>WSO2 Integrator: BI Extension: Install the WSO2 Integrator: BI extension. Refer to Install WSO2 Integrator: BI for detailed instructions.</li> <li>Get OpenAI key:<ol> <li>Sign up at OpenAI.</li> <li>Get an API key from the API section.</li> </ol> </li> </ul>"},{"location":"get-started/develop-ai-agent/#step-1-create-a-new-integration-project","title":"Step 1: Create a new integration project","text":"<ol> <li>Click on the BI icon in the sidebar.</li> <li>Click on the Create New Integration button.</li> <li>Enter the project name as <code>GraphqlService</code>.</li> <li>Select the project directory by clicking on the Select Location button.</li> <li> <p>Click the Create New Integration button to generate the integration project.</p> <p></p> </li> </ol>"},{"location":"get-started/develop-ai-agent/#step-2-create-a-graphql-service","title":"Step 2: Create a GraphQL service","text":"<ol> <li>Click the + button on the WSO2 Integrator: BI side panel or navigate back to the design screen and click on Add Artifact.</li> <li>Select GraphQL Service under the Integration as API artifacts.</li> <li> <p>Keep the default Listener and Service base path configurations, and click Create.</p> <p></p> </li> </ol>"},{"location":"get-started/develop-ai-agent/#step-3-create-a-graphql-resolver","title":"Step 3: Create a GraphQL resolver","text":"<ol> <li>Click the + Create Operations button in the GraphQL design view.</li> <li>In the side panel, click the + button in the Mutation section to add a mutation operation.</li> <li>Provide <code>task</code> as the value for the Field name.</li> <li>Click the Add Argument button to add a GraphQL input<ul> <li>Provide <code>query</code> for the Argument name.</li> <li>Provide <code>string</code> for the Argument type.</li> <li>Click Add to save the argument.</li> </ul> </li> <li> <p>Provide <code>string|error</code> for the Field type, as this will be used as the return type of the resolver.</p> <p></p> </li> </ol>"},{"location":"get-started/develop-ai-agent/#step-4-implement-the-resolving-logic-with-an-inline-agent","title":"Step 4: Implement the resolving logic with an inline agent","text":"<ol> <li>Click the created <code>task</code> operation in the side panel to navigate to the resolver editor view.</li> <li>Click the + button in the flow to open the side panel.</li> <li>Click Agent under Statement, which will navigate you to the agent creation panel.</li> <li>Update Variable Name to <code>response</code>. This is the variable where the agent's output will be stored.</li> <li>Update the Role and Instructions to configure the agent\u2019s behavior.</li> <li>Provide the query parameter as the input for Query. This will serve as the command that the agent will execute.</li> <li>Click Save.</li> <li>Next, configure the agent\u2019s memory, model, and tools. For guidance, refer to the Chat Agent configuration steps and the Personal Assistant setup guide to make the agent function as a personal assistant.</li> <li>After configuring the agent, click the + button on the flow and select Return under Control from the side panel.</li> <li> <p>For the Expression, provide the <code>response</code> variable as the input.</p> <p></p> </li> </ol> <p>At this point, we've created a GraphQL resolver that takes a user-provided <code>query</code> as input, passes it to an inline agent for processing, and returns the agent\u2019s <code>response</code> as the result of the resolver.</p> <p>Note</p> <p>You must implement a query operation to have a valid GraphQL service. Similar to creating the <code>task</code> operation in Step 3, add an operation named <code>greet</code> by pressing the + button in the Query section, without any input parameters. For the implementation, you can simply return a string literal saying <code>\"welcome\"</code>.</p>"},{"location":"get-started/develop-ai-agent/#step-5-run-the-integration-and-query-the-agent","title":"Step 5: Run the integration and query the agent","text":"<ol> <li>Click on the Run button in the top-right corner to run the integration.</li> <li> <p>Query the agent by sending the mutation request below.     <pre><code>curl -X POST http://localhost:8080/graphql \\\n-H \"Content-Type: application/json\" \\\n-d '{ \"query\": \"mutation Task { task(query: \\\"Summarize latest emails\\\") }\" }'\n</code></pre></p> <p></p> </li> </ol>"},{"location":"get-started/develop-automation/","title":"Develop Automation","text":""},{"location":"get-started/develop-automation/#overview","title":"Overview","text":"<p>In this guide, you will create a simple automation that prints <code>\"Hello World\"</code>.</p>"},{"location":"get-started/develop-automation/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have the following:</p> <ul> <li>Visual Studio Code: Install Visual Studio Code if you don't have it already.</li> <li>WSO2 Integrator: BI Extension: Install the WSO2 Integrator: BI extension. Refer to Install WSO2 Integrator: BI for detailed instructions.</li> </ul>"},{"location":"get-started/develop-automation/#step-1-develop-automation-in-wso2-integrator-bi","title":"Step 1: Develop automation in WSO2 Integrator: BI","text":"<ol> <li>In WSO2 Integrator: BI design view, click Add Artifact.</li> <li>Select Automation from the Constructs menu.</li> <li>Click Create to create an automation. This directs you to the automation diagram view.</li> <li>Click + after the Start node to open the node panel.</li> <li>Select Call Function and select println.</li> <li>Click + Add Another Value, type <code>\"Hello World\"</code> and click Save.</li> </ol>"},{"location":"get-started/develop-automation/#step-2-run-automation-in-wso2-integrator-bi","title":"Step 2: Run automation in WSO2 Integrator: BI","text":"<ol> <li> <p>Click Run in the top right corner to run the automation. This compiles the automation and runs it in the embedded Ballerina runtime.</p> <p></p> </li> </ol>"},{"location":"get-started/develop-event-integration/","title":"Develop Event Integration","text":""},{"location":"get-started/develop-event-integration/#overview","title":"Overview","text":"<p>In this guide, you will build a simple event integration that monitors RabbitMQ for new messages and displays them once they become available.</p>"},{"location":"get-started/develop-event-integration/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have the following:</p> <ul> <li>Visual Studio Code: Install Visual Studio Code if you don't have it already.</li> <li>WSO2 Integrator: BI Extension: Install the WSO2 Integrator: BI extension. Refer to Install WSO2 Integrator: BI for detailed instructions.</li> <li>Set up RabbitMQ:<ol> <li>Use an existing RabbitMQ instance or start a new RabbitMQ instance on a server that can be accessed via the internet.</li> <li>Obtain the <code>host</code>, <code>port</code>, <code>username</code>, and <code>password</code> from the RabbitMQ instance.</li> </ol> </li> </ul>"},{"location":"get-started/develop-event-integration/#step-1-develop-event-integration-in-wso2-integrator-bi","title":"Step 1: Develop Event Integration in WSO2 Integrator: BI","text":"<ol> <li>In WSO2 Integrator: BI design view, click Add Artifact.</li> <li>Select Event Integration from the Constructs menu.</li> <li>Click Create to create an event integration. This directs you to the event integration diagram view.</li> <li> <p>Go to the Design View by clicking the Home icon in the top left corner, click on the Configure button, and add the following configurables.</p> Configurable Type <code>host</code> <code>string</code> <code>port</code> <code>int</code> <code>username</code> <code>string</code> <code>password</code> <code>string</code> <p></p> </li> <li> <p>Go to the Design View by clicking the Home icon on the top left corner and click Add Artifact.</p> </li> <li>Select RabbitMQ Event Integration. Choosing the Event Integration from the Devant console disables the other options.</li> <li>Add <code>Orders</code> as the Queue Name and click Create. If there is no queue named <code>Orders</code> in the RabbitMQ server, this will create a new queue with this name.</li> <li>Select previously defined <code>host</code> and <code>port</code> configuration variables for the Host and Port and click Save.     </li> <li>In the Design view, click the Configure button, which will take you to the Configuration page for the Event Integration.</li> <li>Scroll down and find the fields <code>Username</code> and <code>Password</code> and set the corresponding configurables.</li> <li>Click Save Changes and go back to the Design view.     </li> <li>Click the <code>+ Add Handler</code> button and select <code>onMessage</code>, which will open up the handler configuration form.</li> <li>Click Save, and it will redirect you to the flow diagram view.     </li> <li>Click the plus icon after the Start node to open the node panel.</li> <li> <p>Add a Log Info node with the Msg as <code>message.toString()</code>. </p> <p></p> </li> </ol>"},{"location":"get-started/develop-event-integration/#step-2-run-the-integration-in-wso2-integrator-bi","title":"Step 2: Run the integration in WSO2 Integrator: BI","text":"<ol> <li>Click Run in the top-right corner to run the integration. This compiles the integration and runs it in the embedded Ballerina runtime.</li> </ol>"},{"location":"get-started/develop-file-integration/","title":"Develop File Integration","text":""},{"location":"get-started/develop-file-integration/#overview","title":"Overview","text":"<p>In this guide, you will create a file integration that fetches recent weather data.</p>"},{"location":"get-started/develop-file-integration/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have the following:</p> <ul> <li>Visual Studio Code: Install Visual Studio Code if you don't have it already.</li> <li>WSO2 Integrator: BI Extension: Install the WSO2 Integrator: BI extension. Refer to Install WSO2 Integrator: BI for detailed instructions.</li> </ul>"},{"location":"get-started/develop-file-integration/#step-1-develop-file-integration-in-wso2-integrator-bi","title":"Step 1: Develop File Integration in WSO2 Integrator: BI","text":"<ol> <li>Create an empty integration project.</li> <li>Click on the Add Artifact button, then under File Integration select FTP Service.</li> <li>Click on the Save button to create the ftp service by filling the rquired fields.</li> <li> <p>Click on the Add Handler and select onFileChange handler.</p> <p> </p> </li> </ol>"},{"location":"get-started/develop-file-integration/#step-2-log-add-file-events","title":"Step 2: Log Add File Events","text":"<ol> <li>In the Design view, click the <code>onFileChange</code> function box. It will redirect you to the flow diagram view.</li> <li>Click the plus icon after the Start node to open the node panel.</li> <li> <p>Select Foreach and enter the following values in relevant fields:</p> Field Value Variable Name <code>addedFile</code> Variable Type <code>var</code> Collection <code>event.addedFiles</code> <p> </p> </li> <li> <p>Under the Foreach node, add a Log Info node with the Msg as <code>\"File added:\" + addedFile.name</code>. </p> <p> </p> </li> </ol>"},{"location":"get-started/develop-file-integration/#step-3-run-the-integration-in-wso2-integrator-bi","title":"Step 3: Run the integration in WSO2 Integrator: BI","text":"<ol> <li>Click Run in the top right corner to run the integration. This compiles the integration and runs it in the embedded Ballerina runtime.</li> </ol>"},{"location":"get-started/develop-integration-as-api/","title":"Develop Integration as API","text":""},{"location":"get-started/develop-integration-as-api/#overview","title":"Overview","text":"<p>In this guide, you will create a simple integration as an API that acts as a service that calls a third-party endpoint and returns its response to the client.</p> <p> </p>"},{"location":"get-started/develop-integration-as-api/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have the following:</p> <ul> <li>Visual Studio Code: Install Visual Studio Code if you don't have it already.</li> <li>WSO2 Integrator: BI Extension: Install the WSO2 Integrator: BI extension.  Refer to Install WSO2 Integrator: BI for detailed instructions.</li> </ul>"},{"location":"get-started/develop-integration-as-api/#step-1-create-a-new-integration-project","title":"Step 1: Create a new integration project","text":"<ol> <li>Click on the BI icon on the sidebar.</li> <li>Click on the Create New Integration button.</li> <li>Enter the Integration Name as <code>HelloWorld</code>.</li> <li>Select the project directory by clicking on the Select Path button.</li> <li> <p>Click on the Create Integration button to create the integration project.</p> <p> </p> </li> </ol>"},{"location":"get-started/develop-integration-as-api/#step-2-create-an-integration-service","title":"Step 2: Create an integration service","text":"Generate with AI<p>The integration service can also be generated using the AI-assistant. Click on the Generate with AI button and enter the following prompt, then press Add to Integration to generate the integration service.</p> <p><code>Create an http service that has base path as /hello, and 9090 as the port. Add GET resource on /greeting that invokes https://apis.wso2.com/zvdz/mi-qsg/v1.0 endpoint and forward the response to the caller.</code></p> <ol> <li>In the design view, click on the Add Artifact button.</li> <li>Select HTTP Service under the Integration as API category.</li> <li>Select the Design From Scratch option as the Service Contract.</li> <li>Specify the Service base path as <code>/hello</code>.</li> <li> <p>Click on the Save button to create the new service with the specified configurations.</p> <p> </p> </li> </ol>"},{"location":"get-started/develop-integration-as-api/#step-3-design-the-integration","title":"Step 3: Design the integration","text":"<ol> <li>Click on Add Resource button and select GET HTTP method.</li> <li> <p>Change the resource path to <code>greeting</code> and click the Save button.</p> <p> </p> </li> <li> <p>Click the \u2795 button to add a new action to the resource.</p> </li> <li>Select Add Connection from the node panel. </li> <li>Search for <code>HTTP</code> in the search bar and select HTTP as the connection type.</li> <li>Add the URL <code>\"https://apis.wso2.com\"</code> to the connection URL field.</li> <li>Change the Connection Name to <code>externalEp</code> and click Save.</li> <li>Select Connections -&gt; externalEp -&gt; get from the node panel.  Set <code>/zvdz/mi-qsg/v1.0</code> as Path, <code>epResult</code> as Result, and <code>string</code> as Target Type.</li> <li> <p>Fill in the request details as below and click Save.</p> <p> </p> </li> <li> <p>Click \u2795 button again and select Return from the node panel.  </p> </li> <li> <p>Select the <code>epResponse</code> variable as the Expression from the dropdown and click Save. This step will return the response from the <code>HelloWorld</code> API endpoint.      </p> <p> </p> </li> </ol>"},{"location":"get-started/develop-integration-as-api/#step-4-run-the-integration","title":"Step 4: Run the integration","text":"<ol> <li>Click on the Try It button in the top right corner to run the integration.</li> <li>If the integration is not running, there will be notification messaging asking to run the integration.</li> <li>Click Run Integration if a notification is appeared.</li> <li>Once the integration is started, Try-it will open up on the right side.</li> <li> <p>Click on the Run button to invoke the <code>greeting</code> resource.</p> <p> </p> </li> <li> <p>Additionally, you can test the integration using REST clients like Postman or curl.</p> <pre><code>curl http://localhost:9090/hello/greeting\n{\"message\":\"Hello World!!!\"}%\n</code></pre> </li> <li> <p>Click on the \u23f9\ufe0f button or press <code>Shift + F5</code> shortcut to stop the integration.</p> <p></p> </li> </ol>"},{"location":"get-started/install-wso2-integrator-bi/","title":"Install WSO2 Integrator: BI","text":""},{"location":"get-started/install-wso2-integrator-bi/#step-1-install-visual-studio-code","title":"Step 1: Install Visual Studio Code","text":"<p>Download and install Visual Studio Code.</p>"},{"location":"get-started/install-wso2-integrator-bi/#step-2-install-the-wso2-integrator-bi-extension","title":"Step 2: Install the WSO2 Integrator: BI extension","text":"<ol> <li>Go to the Extensions view by clicking on the extension icon on the sidebar or pressing <code>Ctrl + Shift + X</code> on Windows and Linux, or <code>Shift + \u2318 + X</code> on a Mac. <p>Check system requirements to verify environment compatibility.</p> </li> <li>Search for <code>WSO2 Integrator: BI</code> in the extensions view search box.</li> <li> <p>Click on the Install button to install the <code>WSO2 Integrator: BI</code> extension.</p> <p></p> </li> <li> <p>This will install the WSO2 Integrator: BI and Ballerina extensions on VS Code.</p> </li> </ol>"},{"location":"get-started/install-wso2-integrator-bi/#step-3-set-up-wso2-integrator-bi-for-the-first-time","title":"Step 3: Set up WSO2 Integrator: BI for the first time","text":"<ol> <li> <p>Click on the BI icon on the sidebar.  </p> <p></p> </li> <li> <p>Click on the Set up Ballerina distribution button.</p> </li> <li>The setup wizard will install and configure the Ballerina distribution required for WSO2 Integrator: BI.</li> <li> <p>Click on the Restart VS Code button to complete the setup.</p> <p></p> </li> </ol>"},{"location":"get-started/quick-start-guide/","title":"Quick Start Guide","text":"<p>WSO2 Integrator: BI is a powerful low-code integration platform built on top of the Ballerina programming language. It\u2019s designed to help developers quickly build, deploy, and manage integration solutions with minimal boilerplate and maximum productivity.</p> <p>BI combines a visual design interface, AI-assisted development, and seamless low-code\u2013to\u2013pro-code transitions. With built-in connectors, flexible deployment options, and support for patterns like APIs, events, and automations, BI empowers teams to integrate faster and smarter.</p> <p>Whether you\u2019re modernizing legacy systems or building cloud-native services, BI provides a productive and scalable path to integration, helping teams drive digital transformation with clarity, speed, and confidence.</p> <p></p> <p>This quick start guide introduces five core integration types, each with a dedicated hands-on walkthrough.</p>"},{"location":"get-started/quick-start-guide/#automation","title":"Automation","text":"<p>Create integrations that run on a timer\u2014for example, to sync data, generate reports, or perform routine jobs. Follow Develop your first automation to get started.</p>"},{"location":"get-started/quick-start-guide/#ai-agent","title":"AI agent","text":"<p>Build agents that reason and act using GenAI models. Use them to respond to user input, access tools, or make decisions dynamically. Follow Develop your first AI agent to get started. </p>"},{"location":"get-started/quick-start-guide/#integrations-as-apis","title":"Integrations as APIs","text":"<p>Expose your integration as a real-time API that handles incoming requests and returns results. Follow Develop your first integration as API to get started.</p>"},{"location":"get-started/quick-start-guide/#event-integration","title":"Event Integration","text":"<p>Trigger your integration when messages arrive from sources like Kafka or RabbitMQ, enabling reactive workflows. Follow Develop your first event integration to get started.</p>"},{"location":"get-started/quick-start-guide/#file-integration","title":"File Integration","text":"<p>Run your integration when files appear in a folder or FTP location\u2014ideal for batch uploads or scheduled file processing. Follow Develop your first file integration to get started.</p> <p>Explore one or more quick start guides to experience how fast and flexible integration can be with BI.</p>"},{"location":"integration-guides/ai/agents/","title":"Agents Overview","text":"<p>WSO2 Integrator: BI enables developers to easily create intelligent AI agents powered by large language models (LLMs) and integrated with external APIs and services. These AI agents can automate complex workflows, interact with users through natural language, and seamlessly connect with systems like Gmail, Google Calendar, and more. Designed for low-code development and rapid integration, BI makes it simple to embed AI-driven logic into your applications, services, and business processes.</p> <p>There are two main types of AI agents in BI:</p>"},{"location":"integration-guides/ai/agents/#chat-agents","title":"Chat Agents","text":"<p>Chat agents are exposed through HTTP endpoints as REST APIs and are designed to interact with users or external systems. These agents are ideal when you need a chatbot-like experience, where users can type questions or commands and receive intelligent responses powered by an LLM.</p>"},{"location":"integration-guides/ai/agents/#inline-agents","title":"Inline Agents","text":"<p>Inline agents are embedded within service logic (e.g., REST APIs, GraphQL resolvers) and invoked programmatically as part of a backend workflow. These agents are ideal for automation, enrichment, or dynamic processing tasks within your services or business logic.</p> <p>Both Chat and Inline agents can be extended with tools that connect to real-world systems via BI's built-in connectors. You can easily integrate agents with services like Gmail, Google Calendar, databases, or custom APIs\u2014allowing agents to perform actions beyond reasoning, such as reading emails, sending messages, creating events, or fetching records.</p> <p>To get started with agents, visit the following tutorial examples:</p> <ul> <li>Introduction to Chat Agents</li> <li>Introduction to Inline Agents</li> <li>Integrating Agents with MCP Servers</li> <li>Integrating Agents with External Endpoints</li> </ul>"},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/","title":"Integrating Agents with External Endpoints","text":"<p>In this tutorial, you\u2019ll create an AI-powered personal assistant agent that integrates with Gmail and Google Calendar to help you efficiently manage emails, tasks, and schedules. You'll use the prebuilt WSO2 Integrator: BI connectors for seamless integration by turning their actions into agent tools.</p>"},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#prerequisites","title":"Prerequisites","text":"<p>To get started, you\u2019ll need to configure Google API credentials:</p> <ol> <li>Go to the Google Cloud Console and sign in.</li> <li>Follow this guide to generate your Client ID, Client Secret, and Refresh Token.</li> <li>Make sure the necessary scopes and permissions are enabled for both the Gmail and Calendar APIs.</li> </ol>"},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#create-the-agent","title":"Create the agent","text":"<p>Before adding tools, make sure you\u2019ve set up your agent by completing steps 1 to 5 in the Introduction to Chat Agents guide. For this tutorial, you may use the following role and instructions when configuring the agent's behavior.</p> <p>Role <pre><code>Personal AI Assistant\n</code></pre></p> <p>Instructions <pre><code>You are Nova, a smart AI assistant helping me stay organized and efficient.\n\nYour primary responsibilities include:\n- Calendar Management: Scheduling, updating, and retrieving events from the calendar as per the user's needs.\n- Email Assistance: Reading, summarizing, composing, and sending emails while ensuring clarity and professionalism.\n- Context Awareness: Maintaining a seamless understanding of ongoing tasks and conversations to \n  provide relevant responses.\n- Privacy &amp; Security: Handling user data responsibly, ensuring sensitive information is kept confidential,\n  and confirming actions before executing them.\n\nGuidelines:\n- Respond in a natural, friendly, and professional tone.\n- Always confirm before making changes to the user's calendar or sending emails.\n- Provide concise summaries when retrieving information unless the user requests details.\n- Prioritize clarity, efficiency, and user convenience in all tasks.\n</code></pre></p>"},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#use-connector-actions-as-agent-tools","title":"Use connector actions as agent tools","text":"<p>BI includes prebuilt connectors for many external services like Gmail and Google Calendar. You can directly use their actions as tools for your agent\u2014no need to write custom integration code. This significantly reduces the manual effort typically required when working with external APIs.</p>"},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#add-gmail-tools-to-the-agent","title":"Add Gmail tools to the agent","text":""},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#tool-1-list-unread-emails","title":"Tool 1: List unread emails","text":""},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#step-1-add-the-gmail-connector","title":"Step 1: Add the Gmail connector","text":"<ol> <li>In Agent Flow View, click the + button at the bottom-left of the <code>AI Agent</code> box.</li> <li>Click the + button next to Tools \u2192 Create New Tool.</li> <li>Click Add Connection under the Connections section.</li> <li> <p>Search for and select the Gmail connector.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#step-2-configure-the-gmail-connector","title":"Step 2: Configure the Gmail connector","text":"<ol> <li> <p>In the configuration panel:</p> <ul> <li>Click Config to open the Expression Helper.</li> <li>Under the Construct Record tab, select ConnectionConfig.</li> <li>Set the <code>auth</code> type to OAuth2RefreshTokenGrantType.</li> <li>Fill in your clientId, clientSecret, and refreshToken.</li> </ul> <p>Note</p> <p>Externalize credentials using configurable values to avoid exposing them in your version control system. </p> </li> <li> <p>Save the configuration. You\u2019ll now see the Gmail connection listed under Connections.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#step-3-create-the-tool","title":"Step 3: Create the tool","text":"<ol> <li>Select the Gmail connection \u2192 choose the action List messages in user\u2019s mailbox.</li> <li> <p>Provide the required Tool Name input as <code>listUnreadEmails</code>, and optionally add a meaningful Description to help the LLM better understand the tool's purpose.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#step-4-customize-the-tool","title":"Step 4: Customize the tool","text":"<ol> <li>Click on the circular <code>listUnreadEmails</code> tool node.</li> <li>Click \u22ee &gt; View to open the tool function.</li> <li>Click the Gmail connector action node (the rectangle connected to the Gmail connection) to open the configuration panel for that specific connector action.</li> <li>Update these inputs:<ul> <li>Set userId to <code>me</code>. The value <code>\"me\"</code> represents the authenticated user.</li> <li>Under Advanced Configurations, set the q input to <code>\"is:unread\"</code> to filter unread emails only.</li> </ul> </li> <li> <p>Click Save.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#step-5-clean-up","title":"Step 5: Clean up","text":"<p>Remove the <code>userId</code> parameter from the function as it is no longer used in the tool:</p> <ul> <li>Click Edit in the top-right of the function panel.</li> <li>Click the Trash icon next to <code>userId</code>.</li> <li> <p>Click Save.</p> <p></p> </li> </ul> <p>You\u2019ve now created a tool that lists unread emails in the user\u2019s Gmail inbox.</p>"},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#tool-2-read-a-specific-email","title":"Tool 2: Read a specific email","text":""},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#step-1-create-the-tool","title":"Step 1: Create the tool","text":"<ol> <li>In Agent Flow View, click + under Tools \u2192 Create New Tool.</li> <li>Select the existing gmailClient connection.</li> <li>Choose the action Gets the specified message.</li> <li> <p>Name the tool as <code>readSpecificEmail</code> and optionally add a description.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#step-2-customize-the-tool","title":"Step 2: Customize the tool","text":"<ol> <li>Open the <code>readSpecificEmail</code> tool node \u2192 \u22ee &gt; View.</li> <li>Click the Gmail action node and update inputs:<ul> <li>Set userId to <code>\"me\"</code>. The value <code>\"me\"</code> represents the authenticated user.</li> <li>Under Advanced Configurations, set the format input to <code>full</code> to get the full email message data with the body content parsed.</li> </ul> </li> <li> <p>Click Save.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#step-3-clean-up","title":"Step 3: Clean up","text":"<p>Remove <code>userId</code> from parameters (as done previously) and save the tool.</p> <p></p>"},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#tool-3-send-an-email","title":"Tool 3: Send an email","text":""},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#step-1-create-the-tool_1","title":"Step 1: Create the tool","text":"<ol> <li>Use the existing gmailClient connection.</li> <li>Select the action Sends the specified message to the recipients.</li> <li> <p>Name the tool as <code>sendEmail</code> and optionally add a helpful description.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#step-2-customize-and-clean-up","title":"Step 2: Customize and clean up","text":"<ol> <li>Set <code>userId</code> to <code>\"me\"</code> in the connector action configuration (as done previously) .</li> <li>Remove <code>userId</code> from the parameters.</li> <li> <p>Save your tool.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#add-calendar-tools-to-the-agent","title":"Add calendar tools to the agent","text":""},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#tool-4-list-calendar-events","title":"Tool 4: List calendar events","text":""},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#step-1-add-the-google-calendar-connector","title":"Step 1: Add the google calendar connector","text":"<ol> <li>In Agent Flow View, click the + button at the bottom-left of the <code>AI Agent</code> box.</li> <li>Click the + button next to Tools \u2192 Create New Tool.</li> <li>Click + button of the Connections section.</li> <li> <p>Search for and select the Gcalendar connector.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#step-2-configure-the-google-calendar-connector","title":"Step 2: Configure the google calendar connector","text":"<ol> <li> <p>In the configuration panel:</p> <ul> <li>Click Config to open the Expression Helper.</li> <li>Under the Construct Record tab, select ConnectionConfig.</li> <li>Set the <code>auth</code> type to OAuth2RefreshTokenGrantType.</li> <li>Fill in your clientId, clientSecret, and refreshToken.</li> </ul> <p>Note</p> <p>Externalize credentials using configurable values to avoid exposing them in your version control system. </p> <p></p> </li> <li> <p>Save the configuration. You\u2019ll now see the Google calendar connection listed under Connections.</p> </li> </ol>"},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#step-3-create-the-tool_1","title":"Step 3: Create the tool","text":"<ol> <li>Select the Google calendar connection \u2192 choose the action Returns events on the specified calendar..</li> <li> <p>Provide the required Tool Name input as <code>listCalendarEvents</code>, and optionally add a meaningful Description.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#step-4-customize-the-tool_1","title":"Step 4: Customize the tool","text":"<ol> <li>Click on the circular <code>listCalendarEvents</code> tool node.</li> <li>Click \u22ee &gt; View to open the tool function.</li> <li>Click the Google calendar connector action node (the rectangle connected to the Google calendar connection) to open the configuration panel for that specific connector action.</li> <li>Update the <code>calendarId</code> input to <code>\"primary\"</code>, which allows access to the primary calendar of the authenticated user.</li> <li> <p>Click Save.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#step-5-clean-up_1","title":"Step 5: Clean up","text":"<p>Remove the <code>calendarId</code> parameter from the function as it is no longer used in the tool:</p> <ul> <li>Click Edit in the top-right of the function panel.</li> <li>Click the Trash icon next to <code>calendarId</code>.</li> <li> <p>Click Save.</p> <p></p> </li> </ul>"},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#tool-5-create-calendar-event","title":"Tool 5: Create calendar event","text":""},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#step-1-create-the-tool_2","title":"Step 1: Create the tool","text":"<ol> <li>In Agent Flow View, click + under Tools \u2192 Create New Tool.</li> <li>Select the existing gcalendarClient connection.</li> <li>Choose the action Creates an event.</li> <li> <p>Name the tool as <code>createCalendarEvent</code> and optionally add a helpful description.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#step-2-customize-the-tool_1","title":"Step 2: Customize the tool","text":"<ol> <li>Click on the circular <code>createCalendarEvent</code> tool node.</li> <li>Click \u22ee &gt; View to open the tool function.</li> <li>Click the Google calendar connector action node to open the configuration panel for that specific connector action.</li> <li>Update the <code>calendarId</code> input to <code>\"primary\"</code>.</li> <li> <p>Click Save.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#step-3-clean-up_1","title":"Step 3: Clean up","text":"<p>Remove the <code>calendarId</code> parameter from the function as it is no longer used in the tool:</p> <ul> <li>Click Edit in the top-right of the function panel.</li> <li>Click the Trash icon next to <code>calendarId</code>.</li> <li> <p>Click Save.</p> <p></p> </li> </ul>"},{"location":"integration-guides/ai/agents/integrating-agents-with-external-endpoints/#interact-with-the-agent","title":"Interact with the agent","text":"<p>After completing the above steps, your personal AI assistant agent is now ready to assist you with necessary tasks. WSO2 Integrator: BI provides a built-in chat interface to interact with the agent.</p> <p>To start chatting with the agent:</p> <ol> <li>Click the Chat button located at the top-left corner of the interface.</li> <li>You will be prompted to run the integration. Click Run Integration.</li> <li>If you have added any variables to the project, you\u2019ll be prompted to update their values in the Config.toml file. Configure them to continue with the execution of the agent.</li> <li> <p>Start chatting with your assistant.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/agents/integrating-agents-with-mcp-servers/","title":"Integrating Agents with MCP Servers","text":"<p>This tutorial guides you through creating an AI-powered Weather Assistant that integrates with an MCP server to provide real-time weather information. By the end of this tutorial, you will have a personal assistant capable of delivering current weather conditions and forecast details for any location worldwide.</p>"},{"location":"integration-guides/ai/agents/integrating-agents-with-mcp-servers/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have a running MCP Server connected to a weather service. For this setup, you can set up an MCP Server using the guidelines given here. This server enables effective communication between your AI agent and the weather API, allowing real-time data retrieval.</p>"},{"location":"integration-guides/ai/agents/integrating-agents-with-mcp-servers/#create-the-ai-agent","title":"Create the AI agent","text":"<p>Before integrating MCP capabilities, you must first create an AI agent. Follow Steps 1 to 5 in the Introduction to Chat Agents guide to set up your agent.</p> <p>For this tutorial, you can configure the agent with the following role and instructions:</p> <p>Role:</p> <pre><code>Weather AI Assistant\n</code></pre> <p>Instructions:</p> <pre><code>You are Nova, a smart AI assistant dedicated to providing accurate and timely weather information.\n\nYour primary responsibilities include:\n- Current Weather: Provide detailed and user-friendly current weather information for a given location.\n- Weather Forecast: Share reliable weather forecasts according to user preferences (e.g., hourly, daily).\n\nGuidelines:\n- Always communicate in a natural, friendly, and professional tone.\n- Provide concise summaries unless the user explicitly requests detailed information.\n- Confirm location details if ambiguous and suggest alternatives when data is unavailable.\n</code></pre>"},{"location":"integration-guides/ai/agents/integrating-agents-with-mcp-servers/#add-mcp-server-to-the-agent","title":"Add MCP Server to the agent","text":"<p>By connecting to the Weather MCP server, your AI agent can access and interact with real-time weather data sources. To integrate it, follow the steps below.</p>"},{"location":"integration-guides/ai/agents/integrating-agents-with-mcp-servers/#step-1-add-the-mcp-server","title":"Step 1: Add the MCP server","text":"<p>Provide the MCP server connection details.</p> <ol> <li>In Agent Flow View, click the + button at the bottom-right of the <code>AI Agent</code> box.</li> <li>Under Add Tools section, select Use MCP Server.</li> <li> <p>Provide the necessary configuration details, then click Save Tool.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/agents/integrating-agents-with-mcp-servers/#step-2-customize-the-mcp-server","title":"Step 2: Customize the MCP server","text":"<p>You can further customize the MCP configuration to include additional weather tools to suit your use case.</p> <p></p>"},{"location":"integration-guides/ai/agents/integrating-agents-with-mcp-servers/#interact-with-the-agent","title":"Interact with the agent","text":"<p>After completing the above steps, your personal AI assistant agent is now ready to assist you with necessary tasks. WSO2 Integrator: BI provides a built-in chat interface to interact with the agent.</p> <p>To start chatting with the agent:</p> <ol> <li>Click the Chat button located at the top-left corner of the interface.</li> <li>You will be prompted to run the integration. Click Run Integration.</li> <li> <p>Start chatting with your assistant.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/agents/introduction-to-chat-agents/","title":"Introduction to Chat Agents","text":"<p>In this tutorial, you'll create an AI-powered math tutor assistant capable of handling a variety of mathematical queries. The agent will be equipped with tools to perform fundamental arithmetic operations and intelligently combine and execute these tools to address user questions. By the end of this tutorial, you'll have built an interactive math assistant that can help users solve problems and provide clear, step-by-step explanations.</p> <p>Note</p> <p>This math tutor agent can technically be implemented using just an LLM, without any agent capabilities. However, the purpose of this tutorial is to help you understand the essential concepts required to build an AI agent using WSO2 Integrator: BI. By following this guide, you'll gain hands-on experience with agent creation in WSO2 Integrator: BI, setting the foundation for developing more powerful and tailored AI agents in the future.</p>"},{"location":"integration-guides/ai/agents/introduction-to-chat-agents/#prerequisites","title":"Prerequisites","text":"<ul> <li>Sign up at OpenAI.</li> <li>Get an API key from the API section.</li> </ul>"},{"location":"integration-guides/ai/agents/introduction-to-chat-agents/#step-1-create-a-new-integration-project","title":"Step 1: Create a new integration project","text":"<ol> <li>Click on the BI icon in the sidebar.</li> <li>Click on the Create New Integration button.</li> <li>Enter the project name as <code>MathTutor</code>.</li> <li>Select the project directory location by clicking on the Select Location button.</li> <li> <p>Click the Create New Integration button to generate the integration project.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/agents/introduction-to-chat-agents/#step-2-create-an-agent","title":"Step 2: Create an agent","text":"<ol> <li>Click the + button on the BI side panel or navigate back to the design screen and click on Add Artifact.</li> <li>Select AI Chat Agent under the AI Agent artifacts.</li> <li>Provide a Name for the agent. It will take a moment to create an agent with the default configuration.</li> <li> <p>After creating the agent, you can configure it with a model provider, memory, tools, roles, and instructions.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/agents/introduction-to-chat-agents/#step-3-configure-the-agent-behavior","title":"Step 3: Configure the agent behavior","text":"<ol> <li>Click on the AI Agent box to open the agent configuration settings.</li> <li>Define the agent's Role and provide Instructions in natural language. These instructions will guide the agent\u2019s behavior and tasks.</li> <li> <p>Click Save to finalize and complete the agent behavior configuration.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/agents/introduction-to-chat-agents/#step-4-configure-the-agent-model","title":"Step 4: Configure the agent model","text":"<ol> <li>Locate the circle with OpenAI logo which is connected to the AI Agent box. This circle represents the LLM model used by the agent.</li> <li>Click on the circle to open the model configuration options.</li> <li>In the Select Model Provider dropdown, choose OpenAiProvider. By default, OpenAiProvider is selected.</li> <li> <p>Next, provide the OpenAI API key in the API Key input field.</p> <p>Note</p> <p>Since the API key is sensitive, it\u2019s recommended to externalize it by using a configurable value. This helps prevent accidentally committing it to your version control system and ensures it\u2019s kept secure without being exposed. To learn more, see Configurations.</p> <ul> <li>Click the API Key input field to open the Expression Helper window.  </li> <li>In the top bar, go to the Configurables tab (the third option).  </li> <li>Click + Create New Configurable Variable to define a new configurable.  </li> <li>Set the Name to <code>openAiApiKey</code> and the Type to <code>string</code>.  </li> <li>Click Save to create the configurable.</li> </ul> </li> <li> <p>In the Model Type dropdown, select <code>ai:GPT_40</code>.</p> </li> <li> <p>Click Save to complete the LLM model configuration.    </p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/agents/introduction-to-chat-agents/#step-5-configure-agent-memory","title":"Step 5: Configure agent memory","text":"<ol> <li>By default, the agent comes preconfigured with an in-memory implementation.</li> <li>For this tutorial, we will keep the default memory configuration and not make any changes.</li> <li>If you prefer to run the agent without memory (in a stateless fashion), follow these steps:<ul> <li>Click on the three vertical dots in the Memory box.</li> <li>Select the Delete option to remove the memory.</li> </ul> </li> </ol>"},{"location":"integration-guides/ai/agents/introduction-to-chat-agents/#step-6-add-tools-to-the-agent","title":"Step 6: Add tools to the agent","text":"<p>BI allows you to create tools using existing functions. It also supports automatically generating tools from connector actions or OpenAPI specifications by leveraging BI\u2019s capability to generate local connectors from an OpenAPI spec.</p> <p>However, in this tutorial, we will create simple functions to perform arithmetic operations and use them as tools.</p>"},{"location":"integration-guides/ai/agents/introduction-to-chat-agents/#create-a-function","title":"Create a function","text":"<ol> <li>Click the + button in the BI side panel under the Functions section.</li> <li>Provide the required details to create the function. For this example, use <code>sum</code> as the function name, and specify the parameters and return types.</li> <li>Implement the function logic in the flow node editor that opens.</li> </ol>"},{"location":"integration-guides/ai/agents/introduction-to-chat-agents/#add-the-created-function-as-a-tool","title":"Add the created function as a tool","text":"<ol> <li>Go to the agent flow view.</li> <li>Click the + button at the bottom-right corner of the <code>AI Agent</code> box.</li> <li>Click the + button under the Tools section.</li> <li>Select the created function from the Current Integration list \u2014 in this case, <code>sum</code>.</li> <li>Then provide the Tool Name and Description of the tool</li> </ol> <p>Follow steps 1 to 3 to create functions named subtract, multiply and divide to perform subtraction, multiplication, and division operations respectively. Define the appropriate parameters and return types, and implement the corresponding logic in the flow node editor. Then repeat steps 4 to 8 to add each of these functions as tools in the agent by selecting them from the Current Integration list and providing a relevant tool name and description for each.    </p> <p></p>"},{"location":"integration-guides/ai/agents/introduction-to-chat-agents/#step-7-interact-with-the-agent","title":"Step 7: Interact with the agent","text":"<p>After completing the above steps, your math tutor assistant is now ready to answer questions. BI provides a built-in chat interface to interact with the agent.</p> <p>To start chatting with the agent:</p> <ol> <li>Click the Chat button located at the top-left corner of the interface.</li> <li>You will be prompted to run the integration. Click Run Integration.</li> <li>Since we have created a configurable variable for <code>openAiApiKey</code> in step 4, provide it in the <code>Config.toml</code> file.</li> </ol> <p>Note</p> <p>A temporary OpenAI API key is used in the GIF below to showcase the steps.  </p> <p></p>"},{"location":"integration-guides/ai/agents/introduction-to-inline-agents/","title":"Introduction to Inline Agents","text":"<p>In this tutorial, you'll learn how to connect an AI agent to a GraphQL service, enabling the agent to be invoked directly within a GraphQL resolver. This demonstrates the use of an inline agent\u2014a powerful capability in the WSO2 Integrator: BI.</p> <p>Unlike chat agents, which are exposed as REST APIs for external interaction, inline agents are not tied to an API endpoint. Instead, they can be invoked programmatically from anywhere within your integration logic, just like a regular function call.</p> <p>In this example, we'll define a GraphQL schema with a query that invokes the inline agent to generate dynamic responses based on input parameters. The agent runs within the resolver logic and returns results directly as part of the GraphQL response.</p>"},{"location":"integration-guides/ai/agents/introduction-to-inline-agents/#prerequisites","title":"Prerequisites","text":"<ul> <li>Sign up at OpenAI.</li> <li>Get an API key from the API section.</li> </ul>"},{"location":"integration-guides/ai/agents/introduction-to-inline-agents/#step-1-create-a-new-integration-project","title":"Step 1: Create a new integration project","text":"<ol> <li>Click on the BI icon in the sidebar.</li> <li>Click on the Create New Integration button.</li> <li>Enter the project name as <code>GraphqlService</code>.</li> <li>Select the project directory by clicking on the Select Location button.</li> <li> <p>Click the Create New Integration button to generate the integration project.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/agents/introduction-to-inline-agents/#step-2-create-a-graphql-service","title":"Step 2: Create a GraphQL service","text":"<ol> <li>Click the + button on the WSO2 Integrator: BI side panel or navigate back to the design screen and click on Add Artifact.</li> <li>Select GraphQL Service under the Integration as API artifacts.</li> <li> <p>Keep the default Listener and Service base path configurations, and click Create.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/agents/introduction-to-inline-agents/#step-3-create-a-graphql-resolver","title":"Step 3: Create a GraphQL resolver","text":"<ol> <li>Click the + Create Operations button in the GraphQL design view.</li> <li>In the side panel, click the + button in the Mutation section to add a mutation operation.</li> <li>Provide <code>task</code> as the value for the Field name.</li> <li>Click the Add Argument button to add a GraphQL input<ul> <li>Provide <code>query</code> for the Argument name.</li> <li>Provide <code>string</code> for the Argument type.</li> <li>Click Add to save the argument.</li> </ul> </li> <li> <p>Provide <code>string|error</code> for the Field type, as this will be used as the return type of the resolver.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/agents/introduction-to-inline-agents/#step-4-implement-the-resolving-logic-with-an-inline-agent","title":"Step 4: Implement the resolving logic with an inline agent","text":"<ol> <li>Click the created <code>task</code> operation in the side panel to navigate to the resolver editor view.</li> <li>Click the + button in the flow to open the side panel.</li> <li>Click Agent under Statement, which will navigate you to the agent creation panel.</li> <li>Update Variable Name to <code>response</code>. This is the variable where the agent's output will be stored.</li> <li>Update the Role and Instructions to configure the agent\u2019s behavior.</li> <li>Provide the query parameter as the input for Query. This will serve as the command that the agent will execute.</li> <li>Click Save.</li> <li>Next, configure the agent\u2019s memory, model, and tools. For guidance, refer to the Chat Agent configuration steps and the Personal Assistant setup guide to make the agent function as a personal assistant.</li> <li>After configuring the agent, click the + button on the flow and select Return under Control from the side panel.</li> <li> <p>For the Expression, provide the <code>response</code> variable as the input.</p> <p></p> </li> </ol> <p>At this point, we've created a GraphQL resolver that takes a user-provided <code>query</code> as input, passes it to an inline agent for processing, and returns the agent\u2019s <code>response</code> as the result of the resolver.</p> <p>Note</p> <p>You must implement a query operation to have a valid GraphQL service. Similar to creating the <code>task</code> operation in Step 3, add an operation named <code>greet</code> by pressing the + button in the Query section, without any input parameters. For the implementation, you can simply return a string literal saying <code>\"welcome\"</code>.</p>"},{"location":"integration-guides/ai/agents/introduction-to-inline-agents/#step-5-run-the-integration-and-query-the-agent","title":"Step 5: Run the integration and query the agent","text":"<ol> <li>Click on the Run button in the top-right corner to run the integration.</li> <li> <p>Query the agent by sending the mutation request below.     <pre><code>curl -X POST http://localhost:8080/graphql \\\n-H \"Content-Type: application/json\" \\\n-d '{ \"query\": \"mutation Task { task(query: \\\"Summarize latest emails\\\") }\" }'\n</code></pre></p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/direct-llm-call/direct-llm-invocation-with-ballerina-model-providers/","title":"Direct LLM invocation with Ballerina model providers","text":"<p>In this tutorial, you will create an integration that makes a direct call to a Large Language Model (LLM) using Ballerina\u2019s model providers. Direct LLM calls are designed for simple, stateless interactions where conversational history is not required, giving you fine-grained control over each request. With Ballerina, you can send a prompt along with a type descriptor, instructing the LLM to generate a response that automatically conforms to your desired type-safe format (e.g., JSON, Ballerina records, integers). This eliminates manual parsing and ensures structured, predictable outputs.</p> <p>In this tutorial, you\u2019ll leverage this capability to analyze blog content\u2014prompting the LLM to return a structured review, including a suggested category and a rating, using the default WSO2 model provider.</p>"},{"location":"integration-guides/ai/direct-llm-call/direct-llm-invocation-with-ballerina-model-providers/#implementation","title":"Implementation","text":"<p>Follow the steps below to implement the integration.</p>"},{"location":"integration-guides/ai/direct-llm-call/direct-llm-invocation-with-ballerina-model-providers/#step-1-create-a-new-integration-project","title":"Step 1: Create a new integration project","text":"<ol> <li>Click on the WSO2 Integrator: BI icon on the sidebar.</li> <li>Click on the <code>Create New Integration</code> button.</li> <li>Enter <code>BlogReviewer</code> as the project name.</li> <li>Click the <code>Select Path</code> button to set the Integration Path.</li> <li>Click on the <code>Create New Integration</code> button to create the integration project.</li> </ol>"},{"location":"integration-guides/ai/direct-llm-call/direct-llm-invocation-with-ballerina-model-providers/#step-2-define-types","title":"Step 2: Define types","text":"<ol> <li>Click on the <code>Add Artifacts</code> button and select <code>Type</code> in the <code>Other Artifacts</code> section.</li> <li>Click on <code>+ Add Type</code> to add a new type.</li> <li>Click on <code>Import</code> button in the top right corner of the type editor.</li> <li> <p>Use <code>Blog</code> as the <code>Name</code>. Then select <code>JSON</code> from the dropdown and paste the following JSON payload. Then click the <code>Import</code> button.</p> <pre><code>{\n\"title\": \"Tips for Growing a Beautiful Garden\",\n\"content\": \"Spring is the perfect time to start your garden. Begin by preparing your soil with organic compost and ensure proper drainage. Choose plants suitable for your climate zone, and remember to water them regularly. Don't forget to mulch to retain moisture and prevent weeds.\"\n}\n</code></pre> </li> <li> <p>Add another type with <code>Review</code> as the <code>Name</code> and paste the following JSON payload.</p> <pre><code>{\n\"suggestedCategory\": \"Gardening\",\n\"rating\": 5\n}\n</code></pre> </li> <li> <p>The types are now available in the project. <code>Blog</code> and <code>Review</code> are the types that represent the blog content and review respectively.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/direct-llm-call/direct-llm-invocation-with-ballerina-model-providers/#step-3-create-an-http-service","title":"Step 3: Create an HTTP service","text":"<ol> <li>In the design view, click the <code>Add Artifact</code> button.</li> <li>Select <code>HTTP Service</code> under the <code>Integration as API</code> category.</li> <li>Select the <code>Design from Scratch</code> option as the <code>Service Contract</code> and use <code>/blogs</code> as the <code>Service base path</code>.</li> <li> <p>Click the <code>Create</code> button to create the new service with the specified configurations.</p> <p></p> </li> <li> <p>From the HTTP Service view, click <code>+ Add Resource</code> and select the <code>POST</code> method.</p> </li> <li>Give the resource path as <code>review</code>.</li> <li>Click <code>Define Payload</code>, go to the third tab (<code>Browse Existing Types</code>), search for the type <code>Blog</code>, select it, and click <code>Save</code>.</li> <li>Under Responses, edit the 201 response and change its response body schema to Review using the Advanced Configurations section.</li> <li> <p>Click <code>Save</code> to create the resource with the specified configurations.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/direct-llm-call/direct-llm-invocation-with-ballerina-model-providers/#step-4-implement-the-resource-logic","title":"Step 4: Implement the resource logic","text":"<ol> <li>Once redirected to the <code>review</code> resource implementation designer view, follow these steps to implement the logic:</li> <li>Hover over the arrow after the Start node and click the \u2795 button to add a new action to the resource.</li> <li>Select <code>Model Provider</code> from the node panel.</li> <li>Click <code>+ Add Model Provider</code>.</li> <li>Click <code>Default Model Provider (WSO2)</code>.</li> <li>Enter <code>model</code> as the name of the model provider and <code>ai:Wso2ModelProvider</code> as the result type, then click <code>Save</code>.</li> <li>Click the <code>model</code> variable under the <code>Model Providers</code> node.</li> <li>It will show the list of available APIs from the model provider. Select the <code>generate</code> API from the list.</li> <li> <p>Use the following prompt as the <code>Prompt</code> for the blog review use case. Set the name of the result variable to <code>review</code>, use <code>Review</code> as the return type, and convert it to a nilable type using type operators. Then click <code>Save</code>.</p> <pre><code>You are an expert content reviewer for a blog site that \n    categorizes posts under the following categories: \"Gardening\", \"Sports\", \"Health\", \"Technology\", \"Travel\"\n\n    Your tasks are:\n    1. Suggest a suitable category for the blog from exactly the specified categories. \n       If there is no match, use null.\n\n    2. Rate the blog post on a scale of 1 to 10 based on the following criteria:\n    - **Relevance**: How well the content aligns with the chosen category.\n    - **Depth**: The level of detail and insight in the content.\n    - **Clarity**: How easy it is to read and understand.\n    - **Originality**: Whether the content introduces fresh perspectives or ideas.\n    - **Language Quality**: Grammar, spelling, and overall writing quality.\n\nHere is the blog post content:\n\n    Title: ${payload.title}\n    Content: ${payload.content}\n</code></pre> <p></p> </li> <li> <p>Add a new node after the <code>generate</code> API call and select <code>Return</code> from the node panel.</p> </li> <li> <p>Select the <code>review</code> variable from the dropdown and click <code>Save</code>.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/direct-llm-call/direct-llm-invocation-with-ballerina-model-providers/#step-5-configure-default-wso2-model-provider","title":"Step 5: Configure default WSO2 model provider","text":"<ol> <li>Ballerina supports direct calls to Large Language Models (LLMs) with various providers, such as OpenAI, Azure OpenAI, and Anthropic. This demonstration focuses on using the Default Model Provider (WSO2). To begin, you need to configure its settings:<ul> <li>Press <code>Ctrl/Cmd + Shift + P</code> to open the VS Code command palette.</li> <li>Run the command: <code>Ballerina: Configure default WSO2 model provider</code>.    This will automatically generate the required configuration entries.</li> </ul> </li> </ol>"},{"location":"integration-guides/ai/direct-llm-call/direct-llm-invocation-with-ballerina-model-providers/#step-6-run-the-integration","title":"Step 6: Run the integration","text":"<p>Response May Vary</p> <p>Since this integration involves an LLM (Large Language Model) call, the response values may not always be identical across different executions.</p> <ol> <li>Click on the <code>Run</code> button in the top-right corner to run the integration.</li> <li>The integration will start and the service will be available at <code>http://localhost:9090/blogs</code>.</li> <li>Click on the <code>Try it</code> button to open the embedded HTTP client.</li> <li> <p>Enter the blog content in the request body and click on the \u25b6\ufe0f button to send the request.</p> <pre><code>{\n\"title\": \"The Healthy Maven\",\n\"content\": \"For those who want a 360-degree approach to self-care, with advice for betterment in the workplace, home, gym, and on the go, look no further. The Healthy Maven offers recipes for every type of meal under the sun (salads, sides, soups, and more), DIY tips (you\u2019ll learn how to make your own yoga mat spray), and quick workouts. If you like where all this is going, there\u2019s a supplementary podcast run by blogger Davida with guest wellness experts.\"\n}\n</code></pre> </li> <li> <p>The blog content is analyzed by the LLM to suggest a category and rate it based on predefined criteria.</p> </li> </ol>"},{"location":"integration-guides/ai/natural-functions/natural-functions/","title":"Natural Functions","text":"<p>In this tutorial, you will create and use a natural function using the WSO2 Integrator: BI. A natural function allows the logic of the function to be described in natural language and is executed at runtime with a call to a Large Language Model (LLM), with the natural language instructions as the prompt. The tutorial uses a natural function to analyze blog content to suggest a suitable category and rate it on a scale of 1 to 10 based on specified criteria.</p> Natural Programming<p>To learn more about natural programming and natural functions, see Natural Language is Code: A hybrid approach with Natural Programming.</p>"},{"location":"integration-guides/ai/natural-functions/natural-functions/#implementation","title":"Implementation","text":"<p>Follow the steps below to implement the integration.</p>"},{"location":"integration-guides/ai/natural-functions/natural-functions/#step-1-create-a-new-integration-project","title":"Step 1: Create a new integration project","text":"<ol> <li>Click on the BI icon on the sidebar.</li> <li>Click on the <code>Create New Integration</code> button.</li> <li>Enter <code>BlogReviewer</code> as the project name.</li> <li>Select Project Directory and click on the <code>Select Location</code> button.</li> <li>Click on the <code>Create New Integration</code> button to create the integration project.</li> </ol>"},{"location":"integration-guides/ai/natural-functions/natural-functions/#step-2-define-types","title":"Step 2: Define Types","text":"<ol> <li>Click on the <code>Add Artifacts</code> button and select <code>Type</code> in the <code>Other Artifacts</code> section.</li> <li>Click on <code>+ Add Type</code> to add a new type and switch to the <code>Import</code> section. </li> <li> <p>Enter <code>Blog</code> as the <code>Name</code>, paste the following JSON payload, and then click the <code>Import</code> button.</p> <pre><code>{\n\"title\": \"Tips for Growing a Beautiful Garden\",\n\"content\": \"Spring is the perfect time to start your garden. Begin by preparing your soil with organic compost and ensure proper drainage. Choose plants suitable for your climate zone, and remember to water them regularly. Don't forget to mulch to retain moisture and prevent weeds.\"\n}\n</code></pre> </li> <li> <p>Add another type with <code>Review</code> as the <code>Name</code> and paste the following JSON payload. Then click the <code>Import</code> button.</p> <pre><code>{\n\"suggestedCategory\": \"Gardening\",\n\"rating\": 5\n}\n</code></pre> </li> <li> <p>The types are now available in the project. <code>Blog</code> and <code>Review</code> are the types that represent the blog content and review respectively.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/natural-functions/natural-functions/#step-3-add-a-natural-function","title":"Step 3: Add a Natural Function","text":"<ol> <li>Click on the <code>Add Artifact</code> button and select <code>Natural Function</code> under the <code>Other Artifacts</code> category.</li> <li> <p>Use <code>reviewBlog</code> as the name of the function. Then click the <code>Add Parameter</code> button to add a parameter of type <code>Blog</code> named <code>blog</code>. Use <code>Review</code> as the return type and convert it to nilable type using type operators. Then click on the <code>Create</code> button.</p> <p></p> </li> <li> <p>Click on the <code>Edit</code> button to specify the requirement in natural language (i.e., the prompt).</p> </li> <li> <p>Use the following prompt and click on the <code>Save</code> button. Note how interpolations refer to the <code>blog</code> parameter.</p> <pre><code>You are an expert content reviewer for a blog site that \n    categorizes posts under the following categories: \"Gardening\", \"Sports\", \"Health\", \"Technology\", \"Travel\"\n\n    Your tasks are:\n    1. Suggest a suitable category for the blog from exactly the specified categories. \n       If there is no match, use null.\n\n    2. Rate the blog post on a scale of 1 to 10 based on the following criteria:\n    - **Relevance**: How well the content aligns with the chosen category.\n    - **Depth**: The level of detail and insight in the content.\n    - **Clarity**: How easy it is to read and understand.\n    - **Originality**: Whether the content introduces fresh perspectives or ideas.\n    - **Language Quality**: Grammar, spelling, and overall writing quality.\n\nHere is the blog post content:\n\n    Title: ${blog.title}\n    Content: ${blog.content}\n</code></pre> <p></p> </li> </ol>"},{"location":"integration-guides/ai/natural-functions/natural-functions/#step-4-create-an-http-service","title":"Step 4: Create an HTTP service","text":"<ol> <li>In the design view, click on the <code>Add Artifact</code> button.</li> <li>Select <code>HTTP Service</code> under the <code>Integration as API</code> category.</li> <li>Select the <code>Create and use the default HTTP listener (port: 9090)</code> option from the <code>Listeners</code> dropdown.</li> <li>Select the <code>Design from Scratch</code> option as the <code>Service Contract</code> and use <code>/blogs</code> as the <code>Service base path</code>.</li> <li> <p>Click on the <code>Create</code> button to create the new service with the specified configurations.</p> <p></p> </li> <li> <p>The service will have a default resource named <code>greeting</code> with the <code>GET</code> method. Click on the three dots that appear in front of the <code>/blogs</code> service and select <code>Edit</code> from the menu.</p> </li> <li>Then click the <code>Edit</code> button in front of <code>/greeting</code> resource.</li> <li>Change the resource HTTP method to <code>POST</code>.</li> <li>Change the resource name to <code>review</code>.</li> <li>Click on <code>Add Payload</code> and specify <code>blog</code> as the name and <code>Blog</code> as the type.</li> <li>Change the 201 response return type to <code>Review</code>.</li> <li> <p>Click on the <code>Save</code> button to update the resource with the specified configurations.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/natural-functions/natural-functions/#step-5-implement-the-resource-logic","title":"Step 5: Implement the resource logic","text":"<ol> <li>Click on the <code>review</code> resource to navigate to the resource implementation designer view.</li> <li>Hover over the arrow after start and click the \u2795 button to add a new action to the resource.</li> <li>Select <code>Call Natural Function</code> from the node panel.</li> <li>Select the <code>reviewBlog</code> function from the suggestions.</li> <li> <p>For the <code>Blog</code> parameter, use <code>blog</code> as the argument and click on the <code>Save</code> button.</p> <p></p> </li> <li> <p>Add a new node after the <code>reviewBlog</code> function call and select <code>Return</code> from the node panel.</p> </li> <li> <p>Select the <code>review</code> variable from the dropdown and click <code>Save</code>.</p> <p></p> </li> <li> <p>The resource implementation is now complete. The function <code>reviewBlog</code> is called with the <code>blog</code> content as input, and the <code>review</code> is returned as the response.</p> </li> </ol>"},{"location":"integration-guides/ai/natural-functions/natural-functions/#step-6-configure-model-for-natural-function","title":"Step 6: Configure model for natural function","text":"<ol> <li> <p>Press <code>Ctrl + Shift + P</code> on Windows and Linux, or <code>Shift + \u2318 + P</code> on a Mac, and type <code>&gt;Ballerina: Configure default model for natural functions (Experimental)</code> to configure the default model for natural functions. </p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/natural-functions/natural-functions/#step-7-run-the-integration","title":"Step 7: Run the integration","text":"<p>Response May Vary</p> <p>Since this integration involves an LLM (Large Language Model) call, the response values may not always be identical across different executions.</p> <ol> <li>Click on the <code>Run</code> button in the top-right corner to run the integration.</li> <li>The integration will start and the service will be available at <code>http://localhost:9090/blogs</code>.</li> <li>Click on the <code>Try it</code> button to open the embedded HTTP client.</li> <li> <p>Enter the blog content in the request body and click on the \u25b6\ufe0f button to send the request.</p> <pre><code>{\n\"title\": \"The Healthy Maven\",\n\"content\": \"For those who want a 360-degree approach to self-care, with advice for betterment in the workplace, home, gym, and on the go, look no further. The Healthy Maven offers recipes for every type of meal under the sun (salads, sides, soups, and more), DIY tips (you\u2019ll learn how to make your own yoga mat spray), and quick workouts. If you like where all this is going, there\u2019s a supplementary podcast run by blogger Davida with guest wellness experts.\"\n}\n</code></pre> </li> <li> <p>The blog content is analyzed by the natural function to suggest a category and rate it based on predefined criteria.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/rag/build-a-rag-application/","title":"Build a RAG Application","text":"<p>This tutorial guides you through creating a Retrieval-Augmented Generation (RAG) system using WSO2 Integrator: BI. While there are several ways to structure a RAG workflow, we\u2019ll focus on a typical two-phase approach: ingestion and retrieval.</p>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#rag-ingestion","title":"RAG ingestion","text":"<p>This step is managed through Devant and it focuses on preparing documents for efficient retrieval in the RAG system.</p> <ul> <li>Chunk the information into smaller, meaningful sections</li> <li>Convert each chunk into embeddings using an embedding model</li> <li>Store embeddings in the vector database for efficient retrieval</li> </ul> <p>We assume that you've already used Devant to process and ingest the documents. Devant handles the entire ingestion process independently of the main application flow. The following steps of the tutorial focus solely on RAG retrieval.</p>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#rag-retrieval","title":"RAG retrieval","text":"<p>This tutorial focuses on implementing the retrieval component of a Retrieval-Augmented Generation (RAG) system using the WSO2 Integrator: BI.</p> <ul> <li>Convert the user's question into embeddings</li> <li>Perform a similarity search in the vector database</li> <li>Fetch the most relevant chunks</li> <li>Include only the relevant data in the prompt</li> <li>Generate a fact-grounded answer using the LLM</li> </ul> <p>By the end of this tutorial, you'll have a working RAG system that can retrieve relevant information and generate accurate, grounded responses using pre-ingested documents.</p>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to Pinecone vector database (requires API key and service URL)</li> <li>Access to Azure OpenAI (requires API key and endpoint URL)</li> <li>Access to Devant</li> </ul>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#step-1-create-an-http-service","title":"Step 1: Create an HTTP service","text":"<ol> <li>In the design view, click on the Add Artifact button.</li> <li>Select HTTP Service under the Integration as API category.</li> <li>Select the Create and use the default HTTP listener (port:9090) option from the Listeners dropdown.</li> <li>Select the Design from Scratch option as the Service Contract and use <code>/personalAssistant</code> as the Service base path.</li> <li> <p>Click on the Create button to create the new service with the specified configurations.</p> <p></p> </li> <li> <p>The service will have a default resource named <code>greeting</code> with the GET method.</p> </li> <li>Click the Edit FunctionModel button in front of <code>/greeting</code> resource.</li> <li>Change the resource HTTP method to POST.</li> <li>Change the resource name to <code>chat</code>.</li> <li>Click on Add Parameter under the Parameters and specify the parameters you need. Select the Param Type as QUERY and specify <code>request</code> as the name and <code>ChatRequestMessage</code> as the type.</li> <li>Change the 200 response return type to <code>string</code>.</li> <li> <p>Click on the Save button to update the resource with the specified configurations.</p> <p></p> </li> </ol> <p>Note</p> <p>Here we use a modular approach for the resource logic for the <code>/chat</code> resource. You may use your own logic calling directly in the <code>/chat</code> service without creating functions separately.</p> <p>This approach allows for flexibility in implementation - you can either:</p> <ul> <li>Follow the modular pattern shown in this tutorial for better organization and maintainability</li> <li>Implement your logic directly within the <code>/chat</code> resource function based on your specific requirements</li> </ul>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#step-2-implementation-of-rag","title":"Step 2: Implementation of RAG","text":""},{"location":"integration-guides/ai/rag/build-a-rag-application/#21-retrieve-embeddings-for-user-query","title":"2.1 Retrieve embeddings for user query","text":"<p>Follow these steps to create a function that retrieves embeddings using Azure OpenAI:</p>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#211-create-an-embeddings-function","title":"2.1.1 Create an embeddings function","text":"<ol> <li>Click the + button in the Integrator side panel under the Functions section.</li> <li> <p>Provide the required details to create the function. Use <code>getEmbeddings</code> as the function name and specify the parameters and return types.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#212-add-embeddings-connection","title":"2.1.2 Add embeddings connection","text":"<ol> <li>Click the + button and select the + Add Connection in the side panel.</li> <li>Select the connector Embeddings - ballerinax/azure.openai.embeddings.</li> </ol>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#213-configure-the-embeddings-connector","title":"2.1.3 Configure the embeddings connector","text":"<ol> <li>In the configuration of the connector, under the Config select the Add Expression to open the Expression Helper window.</li> <li>In the Expression Helper, navigate to Configurables, click the Create new configurable variable. Here we create <code>azure_api_key</code> and <code>azure_service_url</code>.</li> <li>Select the ConnectionConfig under the Construct Record in the Expression Helper window.</li> <li>Change the BearerTokenConfig to ApiKeysConfig in the auth.</li> <li>Select the Configurables and click the <code>azure_api_key</code>.</li> <li>Expand the Advanced Configurations section. Under the ServiceUrl select the Add Expression to open the Expression Helper window.</li> <li> <p>In the Expression Helper, navigate to Configurables, select on <code>azure_service_url</code> as the value for ServiceUrl and click Save button.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#214-implement-the-embeddings-function-logic","title":"2.1.4 Implement the embeddings function logic","text":"<ol> <li>Click the + button and select the Declare Variable under the Statement.</li> <li>Create variable name as <code>embeddingsBody</code> and specify its type and expression.</li> <li>Click the + button and select the <code>embeddingsClient</code>.</li> <li>Configure the client with the DeploymentId, payload and API version.</li> <li>Configure the function to convert the returned decimal embeddings to float values.</li> <li>Return the final float array.</li> </ol>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#22-retrieve-relevant-chunks-from-vector-database","title":"2.2 Retrieve relevant chunks from vector database","text":"<p>Follow these steps to create a function that retrieves similar vectors from Pinecone using vector embeddings:</p>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#221-add-pinecone-vector-connection","title":"2.2.1 Add Pinecone vector connection","text":"<ol> <li>Click the + button in the Integrator side panel under the Connections section.</li> <li>Select the connector Vector - ballerinax/pinecone.vector.</li> </ol>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#222-configure-the-connector","title":"2.2.2 Configure the connector","text":"<ol> <li>In the configuration of the connector, under the ApiKeyConfig select the Add Expression to open the Expression Helper window.</li> <li>Select the Configurables and click the Create new configurable variable. Here we create <code>pinecone_api_key</code> and <code>pinecone_url</code>.</li> <li>Select the ConnectionConfig under the Construct Record in the Expression Helper window.</li> <li>Click the ApiKeysConfig in the auth, select the Configurables and click the <code>pinecone_api_key</code>.</li> <li> <p>Enter the <code>pinecone_url</code> as ServiceUrl and save it.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#223-create-a-retriever-function","title":"2.2.3 Create a retriever function","text":"<ol> <li>Click the + button in the Integrator side panel under the Functions section.</li> <li> <p>Provide the required details to create the function. Use <code>retrieveData</code> as the function name and specify the parameters and return types.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#224-implement-the-retriever-function-logic","title":"2.2.4 Implement the retriever function logic","text":"<ol> <li>Click the + button and select the <code>vectorClient</code>.</li> <li>Select Query from the vectorClient dropdown.</li> <li>Configure the vector client and specify the payload. Here, we use <code>{ topK: 4}</code> for the record QueryRequest.</li> <li>Extract the matches array from the QueryResponse.</li> <li>Handle null response scenarios with appropriate error handling.</li> <li> <p>Return the relevant matching array from the client response.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#23-augment-queries-with-relevant-chunks","title":"2.3 Augment queries with relevant chunks","text":"<p>Follow these steps to create a function that augments queries with relevant text chunks from vector search results:</p>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#231-create-an-augment-function","title":"2.3.1 Create an augment function","text":"<ol> <li>Click the + button in the Integrator side panel under the Functions section.</li> <li>Create the function with <code>augment</code> as the function name and specify the parameter type and return type.</li> </ol>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#232-implement-the-augment-function-logic","title":"2.3.2 Implement the augment function logic","text":"<ol> <li>Create an empty string variable named <code>context</code>.</li> <li>Add a foreach loop to process each match in the input array.</li> <li>Extract metadata from each match and convert to the appropriate type.</li> <li>Concatenate the text from metadata to the context string.</li> <li> <p>Return the aggregated context string with all relevant text chunks.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#24-generate-response-using-the-context","title":"2.4 Generate response using the context","text":""},{"location":"integration-guides/ai/rag/build-a-rag-application/#241-add-chat-client-connection","title":"2.4.1 Add chat client connection","text":"<ol> <li>Click the + button in the Integrator side panel under the Connections section.</li> <li>Select the connector Chat - ballerinax/azure.openai.chat.</li> <li>In the configuration of the connector, under the Config select the ConnectionConfig under the Construct Record in the Expression Helper window.</li> <li>Change the BearerTokenConfig to ApiKeysConfig in the auth.</li> <li>Select the Configurables and click the <code>azure_api_key</code>.</li> <li> <p>Expand the Advanced Configurations and Enter the <code>azure_service_url</code> as ServiceUrl and save it.</p> <p></p> </li> </ol> Model Flexibility<p>While this tutorial demonstrates Azure OpenAI integration, the same principles apply to other AI providers. You can adapt this implementation to work with:</p> <ul> <li>OpenAI API </li> <li>Anthropic's Claude API</li> <li>Google's PaLM API</li> <li>Local models (via APIs like Ollama)</li> <li>Other cloud AI services</li> </ul> <p>Simply replace the connector and adjust the API configuration parameters according to your chosen provider's requirements.</p>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#242-create-a-generate-function","title":"2.4.2 Create a generate function","text":"<ol> <li>Click the + button in the Integrator side panel under the Functions section.</li> <li>Create the function with <code>generateText</code> as the function name and specify the parameters and return types.</li> </ol>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#243-implement-the-generate-function-logic","title":"2.4.3 Implement the generate function logic","text":"<ol> <li>Create variables such as <code>systemPrompt</code> and <code>chatRequest</code>.</li> <li>Click the + button and select the <code>chatClient</code>.</li> <li>Select Creates a completion for the chat message from the chatClient dropdown.</li> <li>Configure the client and specify the DeploymentId, API version, and payload.</li> <li>Return the chat response from the client.</li> </ol>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#step-3-create-the-combined-llm-function","title":"Step 3: Create the combined LLM function","text":""},{"location":"integration-guides/ai/rag/build-a-rag-application/#31-create-the-llm-function","title":"3.1 Create the LLM function","text":"<ol> <li>Click the + button in the Integrator side panel under the Functions section.</li> <li> <p>Create the function with <code>llmChat</code> as the function name and specify the parameters and return types.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#32-implement-the-function-logic","title":"3.2 Implement the function logic","text":"<p>This function orchestrates the entire RAG (Retrieval-Augmented Generation):</p> <ol> <li>Get Embeddings: Call the <code>getEmbeddings</code> function with the user query to convert it into vector embeddings.</li> <li>Retrieve Data: Use the embeddings to query the vector database through the <code>retrieveData</code> function to get relevant document chunks.</li> <li>Augment Context: Process the retrieved chunks using the <code>augment</code> function to create a consolidated context string.</li> <li>Generate Response: Call the <code>generateText</code> function with both the original query and the augmented context to generate the final response.</li> <li> <p>Return Result: Return the generated response string.</p> <p></p> </li> </ol> <p>This completes the end-to-end RAG where user queries are processed through embeddings, vector search, context augmentation, and LLM generation before returning intelligent responses through the HTTP API.</p>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#step-4-integrate-with-http-service","title":"Step 4: Integrate with HTTP service","text":""},{"location":"integration-guides/ai/rag/build-a-rag-application/#41-update-the-chat-resource","title":"4.1 Update the chat resource","text":"<p>Go back to the HTTP service created in Step 1. In the <code>/chat</code> resource implementation:</p> <ol> <li>Call the <code>llmChat</code> function with the user's query.</li> <li> <p>Return the chat response.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/rag/build-a-rag-application/#step-5-run-the-integration-and-query-the-rag","title":"Step 5: Run the integration and query the RAG","text":"<ol> <li>Click on the Run button in the top-right corner to run the integration.</li> <li>If you have added any variables to the project, you\u2019ll be prompted to update their values in the <code>Config.toml</code> file. Configure them to continue with the execution of the request.</li> <li> <p>Query the RAG by sending the curl request below.</p> <pre><code>curl --location 'http://localhost:9090/personalAssistant/chat' \\\n--header 'Content-Type: application/json' \\\n--data '{\"message\": \"What is the process for reporting safety concerns?\"}'\n</code></pre> <p></p> </li> </ol> <p>Response May Vary</p> <p>Since this integration involves an LLM (Large Language Model) call, the response values may not always be identical across different executions.</p> <p>Your RAG system is now ready to answer questions using retrieved context from your vector database!</p>"},{"location":"integration-guides/ai/rag/rag-ingestion/","title":"RAG Ingestion","text":"<p>In this tutorial, you'll build a Retrieval-Augmented Generation (RAG) ingestion pipeline using WSO2 Integrator: BI. The pipeline loads content from a file, chunks them into smaller sections, generates embedding and stores those embeddings in a vector knowledge base for efficient retrieval.</p> <p>By the end of this tutorial, you'll have created a complete ingestion flow that reads a markdown file, processes the content, and stores it in a vector store for use in RAG applications.</p> <p>Note</p> <p>This tutorial focuses solely on the ingestion aspect of RAG. Retrieval and querying will be covered in a separate guide. The ingestion pipeline is designed using WSO2 Integrator: BI's low-code interface, allowing you to visually orchestrate each step with ease.</p>"},{"location":"integration-guides/ai/rag/rag-ingestion/#prerequisites","title":"Prerequisites","text":"<p>To get started, you need a knowledge file (in Markdown format) that you want to ingest into the vector store.</p> <p>Note: This tutorial uses an in-memory vector store for simplicity, but you can also use external vector stores like Pinecone, Milvus, or Weaviate.</p> <p>What is an In-Memory Vector Store?</p> <p>An in-memory vector store holds your data (the chunked and embedded text) directly in your computer's active memory (RAM). This makes it very fast and easy to set up, as it requires no external databases or services. However, this data is temporary\u2014it will be completely erased when you stop the integration or close the project. </p>"},{"location":"integration-guides/ai/rag/rag-ingestion/#step-1-create-a-new-integration-project","title":"Step 1: Create a new integration project","text":"<ol> <li>Click on the BI icon in the sidebar.</li> <li>Click on the Create New Integration button.</li> <li>Enter the project name as <code>rag_ingestion</code>.</li> <li>Select a directory location by clicking on the Select Path button.</li> <li> <p>Click Create New Integration to generate the project.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/rag/rag-ingestion/#step-2-create-an-automation","title":"Step 2: Create an automation","text":"<p>In WSO2 Integrator: BI, an automation is a flow that runs automatically when the integration starts. We will use this to ensure our data is loaded and ingested into the knowledge base as soon as the application is running, making it ready for the query service.</p> <ol> <li>In the design screen, click on + Add Artifact.</li> <li>Select Automation under the Automation artifact category.</li> <li> <p>Click Create to open the flow editor.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/rag/rag-ingestion/#step-3-create-a-text-data-loader","title":"Step 3: Create a text data loader","text":"<ol> <li>Hover over the flow line and click the + icon to open the side panel.</li> <li>Click on Data Loader from the AI section.</li> <li>Click + Add Data Loader to create a new instance.</li> <li>Choose Text Data Loader.</li> <li>Under the paths field, click on + Add Another Value and add the path to your markdown file.</li> <li>Set Data Loader Name as <code>loader</code>.</li> <li> <p>Click Save to continue.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/rag/rag-ingestion/#step-4-load-data-using-the-data-loader","title":"Step 4: Load data using the data loader","text":"<ol> <li>In the Data Loaders section, click on <code>loader</code>.</li> <li>Click on load to open the configuration panel.</li> <li>Name the result as <code>doc</code>.</li> <li> <p>Click Save to complete the data loading step.</p> <p></p> </li> </ol> <p>This step wraps the file content into a <code>ai:Document</code> record, preparing it for chunking and embedding.</p> <p>Note</p> <p>In WSO2 Integrator: BI, an <code>ai:Document</code> is a generic container that wraps the content of any data source\u2014such as a file, webpage, or database entry. It not only holds the main content but can also include additional metadata, which becomes useful during retrieval operations in RAG workflows. In this tutorial, no metadata is used.</p>"},{"location":"integration-guides/ai/rag/rag-ingestion/#step-5-create-a-vector-knowledge-base","title":"Step 5: Create a vector knowledge base","text":"<p>A vector knowledge base in WSO2 Integrator: BI acts as an interface to a vector store and manages the ingestion and retrieval of documents.</p> <p>Note</p> <p>This tutorial uses an In-Memory Vector Store for simplicity and to get you started quickly. For production use cases or persistent storage, you can choose from other supported vector stores including Pinecone, Milvus, Weaviate, and more. Simply select your preferred option when creating the vector store in step 4 below.</p> <p>When using external vector stores, you may need to provide API keys and other configuration details. It's recommended to externalize sensitive values like API keys using configurables to avoid exposing them in your project files. See Configurations for more information.</p> <ol> <li>Hover over the flow line and click the + icon.</li> <li>Select Vector Knowledge Bases under the AI section.</li> <li>Click + Add Vector Knowledge Base to create a new instance.</li> <li>In the Vector Store section, click + Create New Vector Store and choose InMemory Vector Store, then click Save to create the vector store. This will return you to the vector knowledge base configuration.</li> <li>In the Embedding Model section, click + Create New Embedding Model, select Default Embedding Provider (WSO2), then click Save.</li> <li>For the Chunker setting, you can leave it at the default value of AUTO or create a new chunker if needed.</li> <li>Set the Vector Knowledge Base Name to <code>knowledgeBase</code>.</li> <li> <p>Click Save to complete the configuration.</p> <p></p> </li> </ol> <p>Embedding Dimensions</p> <p>The Default Embedding Provider (WSO2) generates dense vectors with 1536 dimensions. If you're using an external vector store (Pinecone, Milvus, Weaviate, etc.), ensure your vector store index is configured to support 1536-dimensional vectors.</p>"},{"location":"integration-guides/ai/rag/rag-ingestion/#step-6-ingest-data-into-the-knowledge-base","title":"Step 6: Ingest data into the knowledge base","text":"<ol> <li>In the Vector Knowledge Bases section, click on <code>knowledgeBase</code>.</li> <li>Click on ingest to open the configuration panel.</li> <li>Provide <code>doc</code> as the input for Documents.</li> <li> <p>Click Save to complete the ingestion step.</p> <p></p> </li> </ol> <p>This step chunks the document and sends them to the vector store, converting each chunk into an embedding and storing them for future retrieval.</p>"},{"location":"integration-guides/ai/rag/rag-ingestion/#step-7-add-a-confirmation-message","title":"Step 7: Add a confirmation message","text":"<ol> <li>Hover over the flow line and click the + icon.</li> <li>Select Log Info under the Logging section.</li> <li>Enter <code>\"Ingestion completed.\"</code> in the Msg field.</li> <li> <p>Click Save.</p> <p></p> </li> </ol> <p>This step will print a confirmation once the ingestion is complete.</p>"},{"location":"integration-guides/ai/rag/rag-ingestion/#step-8-configure-default-wso2-provider-and-run-the-integration","title":"Step 8: Configure default WSO2 provider and run the integration","text":"<ol> <li>As the workflow uses the <code>Default Embedding Provider (WSO2)</code>, you need to configure its settings:<ul> <li>Press <code>Ctrl/Cmd + Shift + P</code> to open the VS Code command palette.</li> <li>Run the command: <code>Ballerina: Configure default WSO2 model provider</code>.    This will automatically generate the required configuration entries.</li> </ul> </li> <li>Click the Run button in the top-right corner to execute the integration.</li> <li> <p>Once the integration runs successfully, you will see the message <code>\"Ingestion completed.\"</code> in the console.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/rag/rag-query/","title":"RAG Query","text":"<p>In this tutorial, you'll build a simple Retrieval-Augmented Generation (RAG) query flow using WSO2 Integrator: BI. You'll create an HTTP service that retrieves relevant information from a previously ingested vector knowledge base and uses a Large Language Model (LLM) to generate a context-aware response.</p> <p>By the end of this tutorial, you'll have a working integration that takes a user query, retrieves relevant chunks from the knowledge base, and returns a natural language answer using the configured LLM.</p>"},{"location":"integration-guides/ai/rag/rag-query/#prerequisites","title":"Prerequisites","text":"<p>To get started, make sure you have completed the following steps:</p> <ul> <li>Completed the RAG Ingestion Tutorial.</li> </ul> <p>Using the Same Project</p> <p>Since this tutorial uses an In-Memory Vector Store (as configured in the ingestion tutorial), the ingested data is only available within the same integration project and runtime session. You'll need to add the HTTP service to the same <code>rag_ingestion</code> project you created in the previous tutorial, rather than creating a new project.</p> <p>If you used an external vector store like Pinecone, Milvus, or Weaviate in the ingestion tutorial, you can create a separate project and configure the same external vector store connection.</p>"},{"location":"integration-guides/ai/rag/rag-query/#step-1-open-your-existing-integration-project","title":"Step 1: Open your existing integration project","text":"<ol> <li>Open the <code>rag_ingestion</code> project that you created in the RAG Ingestion Tutorial.</li> <li>When you run this integration, the ingestion automation will execute first, automatically loading your data into the in-memory vector store before the HTTP service becomes available.</li> </ol>"},{"location":"integration-guides/ai/rag/rag-query/#step-2-create-an-http-service","title":"Step 2: Create an HTTP service","text":"<ol> <li>In the design view, click on the Add Artifact button.</li> <li>Select HTTP Service under the Integration as API category.</li> <li>Choose Create and use the default HTTP listener from the Listener dropdown.</li> <li>Select Design from Scratch as the Service contract option.</li> <li>Specify the Service base path as <code>/</code>.</li> <li> <p>Click Create to create the service.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/rag/rag-query/#step-3-update-the-resource-method","title":"Step 3: Update the resource method","text":"<ol> <li>The service will have a default resource named <code>greeting</code> with the GET method. Click the edit button next to the <code>/greeting</code> resource.</li> <li>Change the HTTP method to POST.</li> <li>Rename the resource to <code>query</code>.</li> <li>Add a payload parameter named <code>userQuery</code> of type <code>string</code>.</li> <li>Keep others set to defaults.</li> <li> <p>Click Save to apply the changes.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/rag/rag-query/#step-4-retrieve-data-from-the-knowledge-base","title":"Step 4: Retrieve data from the knowledge base","text":"<p>Since you're working in the same project where you completed the ingestion tutorial, the vector knowledge base <code>knowledgeBase</code> that you created earlier is already available. You can use it to retrieve relevant chunks based on the user query.</p> <ol> <li>Click on the newly created <code>POST</code> resource to open it in the flow diagram view.</li> <li>Hover over the flow line and click the + icon.</li> <li>Select Vector Knowledge Bases under the AI section.</li> <li>In the Vector Knowledge Bases section, click on <code>knowledgeBase</code>.</li> <li>Click on retrieve to open the configuration panel.</li> <li>Set the Query input to the <code>userQuery</code> variable.</li> <li>Set the Result to <code>context</code> to store the matched chunks in a variable named <code>context</code>.</li> <li> <p>Click Save to complete the retrieval step.</p> <p></p> </li> </ol> <p>Using External Vector Stores</p> <p>If you have an external vector store (Pinecone, Milvus, Weaviate, etc.) with pre-ingested content, you can create a new vector knowledge base by clicking + Add Vector Knowledge Base and following the instructions in Step 5 of the RAG Ingestion Tutorial. Make sure to configure the same vector store and embedding provider settings that were used during ingestion.</p>"},{"location":"integration-guides/ai/rag/rag-query/#step-5-augment-the-user-query-with-retrieved-content","title":"Step 5: Augment the user query with retrieved content","text":"<p>WSO2 Integrator: BI includes a built-in function to augment the user query with retrieved context from the knowledge base. We'll use that in this step.</p> <ol> <li>Hover over the flow line and click the + icon.</li> <li>Select Augment Query under the AI section.</li> <li>Set Context to <code>context</code>.</li> <li>Set Query to <code>userQuery</code>.</li> <li>Set Result to <code>augmentedUserMsg</code>.</li> <li> <p>Click Save to complete the augmentation step.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/rag/rag-query/#step-6-connect-to-an-llm-provider","title":"Step 6: Connect to an LLM provider","text":"<p>After augmenting the query with retrieved context, we can now pass it to an LLM for a grounded response. WSO2 Integrator: BI provides an abstraction called <code>Model Provider</code> to connect with various LLM services.</p> <ol> <li>Hover over the flow line and click the + icon.</li> <li>Select Model Provider under the AI section.</li> <li>Click + Add Model Provider to create a new instance.</li> <li>Select <code>Default Model Provider (WSO2)</code> \u2014 a WSO2-hosted LLM \u2014 for this tutorial.</li> <li>Set the Model Provider Name to <code>defaultModel</code>.</li> <li> <p>Click Save to complete the configuration.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/rag/rag-query/#step-7-generate-the-response","title":"Step 7: Generate the response","text":"<p>Now send the augmented query to the LLM to generate the grounded response.</p> <ol> <li>Click on the <code>defaultModel</code> under the Model Providers section in the side panel.</li> <li>Select the <code>generate</code> action.</li> <li>Set the Prompt to the expression: <code>check augmentedUserMsg.content.ensureType()</code>.</li> <li>Set the Result variable to <code>response</code>.</li> <li>Set the Expected Type to <code>string</code>.</li> <li> <p>Click Save.</p> <p></p> </li> </ol> <p>Understanding the Expression</p> <p>The expression <code>check augmentedUserMsg.content.ensureType()</code> extracts the augmented query content and ensures it's in the correct string format that the LLM expects. The <code>check</code> keyword handles any potential type conversion errors.</p>"},{"location":"integration-guides/ai/rag/rag-query/#step-8-return-the-response-from-the-service-resource","title":"Step 8: Return the response from the service resource","text":"<ol> <li>Hover over the flow line and click the + icon.</li> <li>Under the Control section, click on Return.</li> <li> <p>Set Expression to <code>response</code>.</p> <p></p> </li> </ol>"},{"location":"integration-guides/ai/rag/rag-query/#step-9-configure-default-wso2-providers-and-run-the-integration","title":"Step 9: Configure default WSO2 providers and run the integration","text":"<ol> <li>As the workflow uses the <code>Default Model Provider (WSO2)</code> and <code>Default Embedding Provider (WSO2)</code>, you need to configure its settings:<ul> <li>Press <code>Ctrl/Cmd + Shift + P</code> to open the VS Code command palette.</li> <li>Run the command: <code>Ballerina: Configure default WSO2 model provider</code>.    This will automatically generate the required configuration entries.</li> </ul> </li> <li>Click the Run button in the top right corner to start the integration.</li> <li>The integration will compile and launch in the embedded Ballerina runtime. The ingestion automation will run first, followed by the HTTP service.</li> <li>You can also test the service using tools like Postman or curl:    <pre><code>curl -X POST http://localhost:9090/query -H \"Content-Type: application/json\" -d '\"Who should I contact for refund approval?\"'\n</code></pre></li> <li> <p>To stop the integration, click the \u23f9\ufe0f button or press <code>Shift + F5</code>.</p> <p></p> </li> </ol>"},{"location":"integration-guides/file-integration/file-integration-with-directory-service/","title":"File Integration With Directory Service","text":"<p>In this section, we will learn how to create a file integration using the WSO2 Integrator: BI.  The integration will listen to events in a directory and will be triggered for file-related events.</p>"},{"location":"integration-guides/file-integration/file-integration-with-directory-service/#step-1-create-a-new-integration-project","title":"Step 1: Create a new integration project","text":"<ol> <li>Click on the BI icon on the sidebar.</li> <li>Click on the Create New Integration button.</li> <li>Enter the project name as <code>FileIntegration</code>.</li> <li>Select Project Directory and click on the Select Location button.</li> <li>Click on the Create New Integration button to create the integration project.</li> </ol>"},{"location":"integration-guides/file-integration/file-integration-with-directory-service/#step-2-create-an-directory-service","title":"Step 2: Create an Directory service","text":"<ol> <li>In the design view, click on the Add Artifact button.</li> <li>Select Directory Service under the File Integration category.</li> <li>Enter the path to the directory you want to monitor. For example, <code>/user/home/Downloads</code>.</li> <li> <p>Click on the Save button to create the directory service.</p> <p> </p> </li> </ol>"},{"location":"integration-guides/file-integration/file-integration-with-directory-service/#step-3-configure-file-event-resources","title":"Step 3: Configure file event resources","text":"<ol> <li> <p>Click Add Handler button and select onCreate handler.</p> <p> </p> </li> <li> <p>Click on the onCreate function to navigate to the function implementation designer view.</p> </li> <li>Click on + and select Log Info from the node panel under Logging category.</li> <li>Add the log message as <code>\"File created \"+ event.name</code> in the Msg field.</li> <li> <p>Click on the Save button to add the log action to the function.</p> <p> </p> </li> <li> <p>Repeat the above steps to add the onDelete and onModify functions to the service.</p> </li> <li>Add the log message as <code>\"File deleted \"+ event.name</code> in the Msg field for the onDelete function.</li> <li>Add the log message as <code>\"File modified \"+ event.name</code> in the Msg field for the onModify function.</li> <li> <p>The final service will look like this:      </p> <p> </p> </li> </ol>"},{"location":"integration-guides/file-integration/file-integration-with-directory-service/#step-4-run-the-integration","title":"Step 4: Run the integration","text":"<ol> <li>Click on the Run button in the top-right corner to run the integration.</li> <li>The integration will start listening to the events in the directory specified in step 2. </li> <li>Create a new file in the directory to trigger the onCreate event.</li> <li>Modify the file to trigger the onModify event.</li> <li>Delete the file to trigger the onDelete event.</li> <li> <p>The log messages will be displayed in the console.   </p> <p> </p> </li> </ol>"},{"location":"integration-guides/integration-as-api/message-routing/","title":"Content-Based Message Routing","text":""},{"location":"integration-guides/integration-as-api/message-routing/#overview","title":"Overview","text":"<p>In this tutorial, you'll create a service that allows users to reserve appointments at various hospitals.  Requests will be directed to the appropriate hospital based on the request payload's content. To accomplish this, you\u2019ll build a REST service with a single resource in WSO2 Integrator: BI extension.  The resource will handle user requests, identify the hospital endpoint based on the hospital ID,  forward the request to the specified hospital service to make the reservation, and return the reservation details.</p> <p>Here\u2019s an overview of the process flow.</p> <p> </p> <ol> <li> <p>Receive a request with a JSON payload similar to the following.</p> <p>ReservationRequest.json<pre><code>{\n\"patient\": {\n\"name\": \"John Doe\",\n\"dob\": \"1940-03-19\",\n\"ssn\": \"234-23-525\",\n\"address\": \"California\",\n\"phone\": \"8770586755\",\n\"email\": \"johndoe@gmail.com\"\n},\n\"doctor\": \"thomas collins\",\n\"hospital\": \"grand oak community hospital\",\n\"hospital_id\": \"grandoak\",\n\"appointment_date\": \"2023-10-02\"\n}\n</code></pre> 2. Extract the <code>hospital_id</code> field and select the corresponding hospital service endpoint.</p> <ul> <li>grandoak -&gt; <code>http://localhost:9090/grandoak/categories</code></li> <li>clemency -&gt; <code>http://localhost:9090/clemency/categories</code></li> <li>pinevalley -&gt; <code>http://localhost:9090/pinevalley/categories</code> </li> </ul> </li> <li> <p>Forward the request to the selected hospital service and retrieve the response which will be similar to the following.</p> ReservationResponse.json<pre><code>{\n\"appointmentNumber\": 8,\n\"doctor\": {\n\"name\": \"thomas collins\",\n\"hospital\": \"grand oak community hospital\",\n\"category\": \"surgery\",\n\"availability\": \"9.00 a.m - 11.00 a.m\",\n\"fee\": 7000.0\n},\n\"patientName\": \"John Doe\",\n\"hospital\": \"grand oak community hospital\",\n\"confirmed\": false,\n\"appointmentDate\": \"2023-10-02\"\n}\n</code></pre> </li> </ol>"},{"location":"integration-guides/integration-as-api/message-routing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed on your machine.</li> </ul>"},{"location":"integration-guides/integration-as-api/message-routing/#step-1-create-a-new-integration-project","title":"Step 1: Create a new integration project","text":"<ol> <li>Click on the BI icon on the sidebar.</li> <li>Click on the Create New Integration button.</li> <li>Enter the project name as <code>MessageRouting</code>.</li> <li>Select Project Directory and click on the Select Location button.</li> <li>Click on the Create New Integration button to create the integration project.</li> </ol>"},{"location":"integration-guides/integration-as-api/message-routing/#step-2-create-an-http-service","title":"Step 2: Create an HTTP service","text":"<ol> <li>In the design view, click on the Add Artifact button.</li> <li>Select HTTP Service under the Integration as API category.</li> <li>Select the + Listeners option from the Listeners dropdown to add a new listener.</li> <li>Add the service base path as <code>/healthcare</code> and select the Design from Scratch option as the The contract of the service.</li> <li>Enter the listener name as <code>healthListener</code>, <code>8290</code> as the port in Advanced Configurations.</li> <li> <p>Click on the Save button to create the new service with the specified configurations.</p> <p> </p> </li> </ol>"},{"location":"integration-guides/integration-as-api/message-routing/#step-3-define-types","title":"Step 3: Define types","text":"<ol> <li>Click on the Add Artifacts button and select Type in the Other Artifacts section.</li> <li>Click on + Add Type to add a new type</li> <li>Add the Name as <code>ReservationRequest</code> and paste the following JSON payload. Click on the Import button.    <pre><code> {\n\"patient\": {\n\"name\": \"John Doe\",\n\"dob\": \"1940-03-19\",\n\"ssn\": \"234-23-525\",\n\"address\": \"California\",\n\"phone\": \"8770586755\",\n\"email\": \"johndoe@gmail.com\"\n},\n\"doctor\": \"thomas collins\",\n\"hospital\": \"grand oak community hospital\",\n\"hospital_id\": \"grandoak\",\n\"appointment_date\": \"2023-10-02\"\n}\n</code></pre></li> <li>Repeat the above steps to add a new type named <code>ReservationResponse</code> with the following JSON payload.     <pre><code>{\n\"appointmentNumber\": 8,\n\"doctor\": {\n\"name\": \"thomas collins\",\n\"hospital\": \"grand oak community hospital\",\n\"category\": \"surgery\",\n\"availability\": \"9.00 a.m - 11.00 a.m\",\n\"fee\": 7000.0\n},\n\"patientName\": \"John Doe\",\n\"hospital\": \"grand oak community hospital\",\n\"confirmed\": false,\n\"appointmentDate\": \"2023-10-02\"\n}\n</code></pre></li> <li> <p>The final Type diagram will look like below.     </p> <p> </p> </li> </ol>"},{"location":"integration-guides/integration-as-api/message-routing/#step-4-add-connectors","title":"Step 4: Add connectors","text":"<ol> <li>Navigate to design view and click on the Add Artifacts button and select Connection in the Other Artifacts section.</li> <li>Search and select the HTTP Client connector.</li> <li>Enter the connector name as <code>grandOakEp</code>, URL as <code>\"http://localhost:9090/grandoak/categories\"</code>.</li> <li> <p>Click on the Save button to create the new connector with the specified configurations.</p> <p>  5. Repeat the above steps to add connectors for the <code>clemency</code> and <code>pinevalley</code> hospitals with the following configurations.</p> Connector Name URL clemencyEp <code>\"http://localhost:9090/clemency/categories\"</code> pineValleyEp <code>\"http://localhost:9090/pinevalley/categories\"</code> </li> <li> <p>The final connectors will look like below.     </p> <p> </p> </li> </ol> HTTP Connector<p>To learn more about HTTP client, see Ballerina HTTP Client. See supported advanced client configurations in the HTTP Client Configurations.</p>"},{"location":"integration-guides/integration-as-api/message-routing/#step-5-add-a-resource-method","title":"Step 5: Add a resource method","text":"<ol> <li>Click Add Resource and select POST method.</li> <li>Set the resource path as <code>categories/[string category]/reserve</code>.</li> <li>Define the payload type as <code>ReservationRequest</code>.</li> <li>Change the 201 response return type to <code>ReservationStatus</code>.</li> <li>Add a new response of type HttpNotFound under the responses.   </li> <li> <p>Click on the Save button to save the resource.   </p> <p> </p> </li> </ol>"},{"location":"integration-guides/integration-as-api/message-routing/#step-6-add-the-routing-logic","title":"Step 6: Add the routing logic","text":"<ol> <li>Click on the <code>categories/[string category]/reserve</code> resource to navigate to the resource implementation designer view.</li> <li>Delete the default <code>Return</code> action from the resource.</li> <li>Hover to the arrow after start and click the \u2795 button to add a new action to the resource.</li> <li>Select Declare Variable from the node panel on the left. This variable will be used to store the request payload for the hospital service.</li> <li> <p>Change the variable name to <code>hospitalRequset</code>, type as <code>json</code> and expression as below and click Save.     <pre><code>{\n    patient: payload.patient.toJson(),\n    doctor: payload.doctor,\n    hospital: payload.hospital,\n    appointment_date: payload.appointment_date\n}\n</code></pre></p> <p> </p> </li> <li> <p>Add If from the node panel after <code>hospitalRequest</code> variable. Enter the conditions as If Else If blocks as below for each hospital.</p> <ul> <li>grandOak -&gt; <code>payload.hospital_id == \"grandoak\"</code></li> <li>clemency -&gt; <code>payload.hospital_id == \"clemency\"</code></li> <li>pineValley -&gt; <code>payload.hospital_id == \"pinevalley\"</code> </li> </ul> <p> </p> </li> <li> <p>Select the <code>grandOakEP</code> condition true path \u2795 sign and select grandOakEP connector from the node panel and select post from the dropdown.  Then, fill in the required fields with the values given below.</p> Field Value Variable Name <code>oakEPResponse</code> Variable Type <code>ReservationResponse</code> Path <code>string `/${category}/reserve`</code> Message <code>hospitalRequset</code> </li> <li> <p>Click Save.</p> <p> </p> </li> <li> <p>Click on the \u2795 sign again and select Return from the node panel. Select the <code>oakEPResponse</code> variable from the dropdown and click Save.</p> <p> </p> </li> <li> <p>The steps above will add the routing logic for the <code>grandoak</code> hospital. A variable named <code>oakEPResponse</code> will store the response from the <code>grandoak</code> hospital service. The response will be returned to the client.</p> </li> <li> <p>Repeat the 7,8,9 steps for the <code>clemency</code> and <code>pinevalley</code> hospitals with the following configurations.</p> <p>clemency:</p> Field Value Variable Name <code>clemencyEPResponse</code> Variable Type <code>ReserveResponse</code> Path <code>string `/${category}/reserve`</code> Message <code>hospitalRequset</code> <p>pinevalley:</p> Field Value Variable Name <code>pineValleyEPResponse</code> Variable Type <code>ReserveResponse</code> Path <code>string `/${category}/reserve`</code> Message <code>hospitalRequset</code> </li> <li> <p>For the else condition, click on the <code>If</code> condition <code>Else</code> path \u2795 sign and add a Return from the node panel. Enter <code>http:NOT_FOUND</code> as the value and click Save.             </p> </li> <li> <p>The final design will look like below.             </p> <p> </p> </li> </ol>"},{"location":"integration-guides/integration-as-api/message-routing/#step-7-run-the-service","title":"Step 7: Run the service","text":"<ol> <li>Start the backend service by executing the following command in a terminal.     <pre><code>docker run --name hospital-backend -p 9090:9090 -d anuruddhal/kola-hospital-backend\n</code></pre></li> <li>Click on the Run on the run button in the top right corner to run the service.</li> <li>The service will start and the service will be available at <code>http://localhost:8290/healthcare/categories/[category]/reserve</code>.</li> <li>Click on the Try it button to open the embedded HTTP client.</li> <li> <p>Replace the {category} with <code>surgery</code> in the resource path and enter the following JSON payload in the request body and click on the \u25b6\ufe0f button to send the request.     <pre><code>{\n\"patient\":{\n\"name\": \"John Doe\",\n\"dob\": \"1940-03-19\",\n\"ssn\": \"234-23-525\",\n\"address\": \"California\",\n\"phone\": \"8770586755\",\n\"email\": \"johndoe@gmail.com\"\n},\n\"doctor\": \"thomas collins\",\n\"hospital_id\": \"grandoak\",\n\"hospital\": \"grand oak community hospital\",\n\"appointment_date\": \"2023-10-02\"\n}\n</code></pre> </p> </li> <li> <p>The response will be similar to the following.    <pre><code>{\n\"appointmentNumber\": 1,\n\"doctor\": {\n\"name\": \"thomas collins\",\n\"hospital\": \"grand oak community hospital\",\n\"category\": \"surgery\",\n\"availability\": \"9.00 a.m - 11.00 a.m\",\n\"fee\": 7000.0\n},\n\"patient\": {\n\"name\": \"John Doe\",\n\"dob\": \"1940-03-19\",\n\"ssn\": \"234-23-525\",\n\"address\": \"California\",\n\"phone\": \"8770586755\",\n\"email\": \"johndoe@gmail.com\"\n},\n\"hospital\": \"grand oak community hospital\",\n\"confirmed\": false,\n\"appointmentDate\": \"2023-10-02\"\n}\n</code></pre></p> </li> <li>Optionally, you can test the service using curl command as below.    <pre><code> curl -X POST \"http://localhost:8290/healthcare/categories/surgery/reserve\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n \"patient\": {\n \"name\": \"John Doe\",\n \"dob\": \"1940-03-19\",\n \"ssn\": \"234-23-525\",\n \"address\": \"California\",\n \"phone\": \"8770586755\",\n \"email\": \"johndoe@gmail.com\"\n },\n \"doctor\": \"thomas collins\",\n \"hospital_id\": \"grandoak\",\n \"hospital\": \"grand oak community hospital\",\n \"appointment_date\": \"2023-10-02\"\n }'\n</code></pre></li> </ol>"},{"location":"integration-guides/integration-as-api/message-routing/#step-8-stop-the-integration","title":"Step 8: Stop the integration","text":"<ol> <li>Click on the Stop button to stop the integration.</li> <li>Stop the hospital backend server by running the following command:    <pre><code>docker stop hospital-backend\n</code></pre></li> </ol>"},{"location":"integration-guides/integration-as-api/message-transformation/","title":"Message Transformation","text":""},{"location":"integration-guides/integration-as-api/message-transformation/#overview","title":"Overview","text":"<p>This guide explains how to create a simple integration to convert a JSON payload to an XML payload using WSO2 Integrator: BI.  An HTTP service with a single resource (<code>toXml</code>) will be created to accept a JSON payload and return the XML representation of the payload.</p> <p> </p>"},{"location":"integration-guides/integration-as-api/message-transformation/#step-1-create-a-new-integration-project","title":"Step 1: Create a new integration project","text":"<ol> <li>Click on the BI icon on the sidebar.</li> <li>Click on the Create New Integration button.</li> <li>Enter the integration name as <code>JsonToXml</code>.</li> <li>Select project directory location by clicking on the Select Location button.</li> <li> <p>Click on the Create Integration button to create the integration project.</p> <p> </p> </li> </ol>"},{"location":"integration-guides/integration-as-api/message-transformation/#step-2-create-an-http-service","title":"Step 2: Create an HTTP service","text":"<ol> <li>In the design view, click on the Add Artifact button.</li> <li>Select HTTP Service under the Integration as API category.</li> <li>Select the Create and use the default HTTP listener option from the Listener dropdown.</li> <li>Select Design from Scratch option as the Service Contract.</li> <li>Specify the Service base path as <code>/convert</code>.</li> <li> <p>Click on the Save button to create the new service with the specified configurations.</p> <p> </p> </li> </ol>"},{"location":"integration-guides/integration-as-api/message-transformation/#step-3-add-the-resource-method","title":"Step 3: Add the resource method","text":"<ol> <li>Click the Add Resource button.</li> <li>Select the POST method from the right side panel. </li> <li>Enter the resource path as <code>toXml</code>.</li> <li>Click on the Define Payload action and select Continue with JSON Type.</li> <li>Change the 201 response return type to <code>xml</code> and Save response.</li> <li> <p>Click on the Save button with the specified configurations.</p> <p> </p> </li> </ol> <p>Resource Method</p> <p>To learn more about resources, see Ballerina Resources.</p>"},{"location":"integration-guides/integration-as-api/message-transformation/#step-4-add-the-transformation-logic","title":"Step 4: Add the transformation logic","text":"<ol> <li>Click on the <code>toXml</code> resource to navigate to the resource implementation designer view.</li> <li>Hover over the arrow after the start and click the \u2795 button to add a new action to the resource.</li> <li>Select Function Call from the node panel.</li> <li>Search for <code>json to xml</code> and select the fromJson function from the suggestions.</li> <li>Set the JsonValue to <code>payload</code> from inputs.</li> <li> <p>Click on the Save button to add the function call to the resource.</p> <p> </p> </li> <li> <p>Add a new node after the <code>fromJson</code> function call and select Return from the node panel.</p> </li> <li> <p>Select the <code>xmlResult</code> variable from the dropdown and click Save.</p> <p> </p> </li> </ol> <p>JSON to XML Conversion</p> <p>To learn more about json to xml conversion, see Ballerina JSON to XML conversion.</p>"},{"location":"integration-guides/integration-as-api/message-transformation/#step-5-run-the-integration","title":"Step 5: Run the integration","text":"<ol> <li>Click on the Run button in the top-right corner to run the integration.</li> <li>The integration will start and the service will be available at <code>http://localhost:9090/convert</code>.</li> <li>Click on the Try it button to open the embedded HTTP client.</li> <li>Enter the JSON payload in the request body and click on the \u25b6\ufe0f button to send the request.     <pre><code>{\n\"name\": \"John\",\n\"age\": 30,\n\"car\": \"Honda\"\n}\n</code></pre></li> <li> <p>The response will be an XML representation of the JSON payload. <code>&lt;root&gt;         &lt;name&gt;John&lt;/name&gt;         &lt;age&gt;30&lt;/age&gt;         &lt;car&gt;Honda&lt;/car&gt;     &lt;/root&gt;</code> </p> </li> <li> <p>Additionally, the service can be tested using tools like Postman or curl by sending a POST request with a JSON payload to the service endpoint.    <pre><code>curl -X POST \"http://localhost:9090/convert/toXml\" -H \"Content-Type: application/json\" -d '{\"name\":\"John\", \"age\":30, \"car\":\"Honda\"}'\n</code></pre></p> </li> </ol>"},{"location":"integration-guides/integration-as-api/service-orchestration/","title":"Service Orchestration","text":""},{"location":"integration-guides/integration-as-api/service-orchestration/#overview","title":"Overview","text":"<p>In this tutorial, you\u2019ll create a service to process appointment requests for hospitals.  The service will call multiple backend services sequentially, using data from each call to inform the next.  This approach integrates several services into one, known as service orchestration. To implement this, you\u2019ll build a REST service with a single resource in WSO2 Integrator: BI extension and then run the service.  The resource will receive user requests, make the necessary backend calls, and respond with the appointment details.</p> <p>The flow is as follows.</p> <ol> <li>The user sends an appointment request to the service.     <pre><code>  {\n\"patient\": {\n\"name\": \"John Doe\",\n\"dob\": \"1940-03-19\",\n\"ssn\": \"234-23-525\",\n\"address\": \"California\",\n\"phone\": \"8770586755\",\n\"email\": \"johndoe@gmail.com\",\n\"cardNo\": \"7844481124110331\"\n},\n\"doctor\": \"thomas collins\",\n\"hospital_id\": \"grandoaks\",\n\"hospital\": \"grand oak community hospital\",\n\"appointment_date\": \"2024-11-06\"\n}\n</code></pre></li> <li>Extract the necessary details from the request (e.g., hospital, patient, doctor, etc.) and make a call to the hospital backend service to request an appointment. A response similar to the following will be returned from the hospital backend service on success.      <pre><code>  {\n\"appointmentNumber\": 1,\n\"doctor\": {\n\"name\": \"thomas collins\",\n\"hospital\": \"grand oak community hospital\",\n\"category\": \"surgery\",\n\"availability\": \"9.00 a.m - 11.00 a.m\",\n\"fee\": 7000\n},\n\"patient\": {\n\"name\": \"John Doe\",\n\"dob\": \"1940-03-19\",\n\"ssn\": \"234-23-525\",\n\"address\": \"California\",\n\"phone\": \"8770586755\",\n\"email\": \"johndoe@gmail.com\"\n},\n\"hospital\": \"grand oak community hospital\",\n\"confirmed\": false,\n\"appointmentDate\": \"2023-10-02\"\n}\n</code></pre></li> <li>Use the hospital ID and the appointment number and call the hospital backend service to retrieve the fee for the appointment. The response will be similar to the following.     <pre><code>  {\n\"patientName\": \"John Doe\",\n\"doctorName\": \"thomas collins\",\n\"actualFee\": \"7000\"\n}\n</code></pre></li> <li>Finally, call the payment backend service to make the payment and retrieve the reservation status.    <pre><code>  {\n\"appointmentNo\": 2,\n\"doctorName\": \"thomas collins\",\n\"patient\": \"John Doe\",\n\"actualFee\": 7000,\n\"discount\": 20,\n\"discounted\": 5600.0,\n\"paymentID\": \"f130e2ed-a34e-4434-9b40-6a0a8054ee6b\",\n\"status\": \"settled\"\n}\n</code></pre></li> </ol>"},{"location":"integration-guides/integration-as-api/service-orchestration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed on the machine.</li> </ul>"},{"location":"integration-guides/integration-as-api/service-orchestration/#step-1-create-a-new-integration-project","title":"Step 1: Create a new integration project","text":"<ol> <li>Click on the BI icon on the sidebar.</li> <li>Click on the Create New Integration button.</li> <li>Enter the project name as <code>ServiceOrchestration</code>.</li> <li>Select project directory location by clicking on the Select Location button.</li> <li>Click on the Create New Integration button to create the integration project.</li> </ol>"},{"location":"integration-guides/integration-as-api/service-orchestration/#step-2-create-an-http-service","title":"Step 2: Create an HTTP service","text":"<ol> <li>In the design view, click on the Add Artifact button.</li> <li>Select HTTP Service under the Integration as API category.</li> <li>Select the + Listeners option from the Listeners dropdown to add a new listener.</li> <li>Add the service base path as <code>/healthcare</code> and select the Design from Scratch option as the The contract of the service.</li> <li>Enter the listener name as <code>healthListener</code>, <code>8290</code> as the port in Advanced Configurations.</li> <li> <p>Click on the Save button to create the new service with the specified configurations.</p> <p> </p> </li> </ol>"},{"location":"integration-guides/integration-as-api/service-orchestration/#step-3-define-types","title":"Step 3: Define types","text":"<ol> <li> <p>Click on the Add Artifacts button and select Type in the Other Artifacts section.</p> <p> </p> </li> <li> <p>Click on + Add Type to add a new type. Use expected JSON samples as follows to create types for the hospital appointment scenario, and make sure to select the JSON option from the format dropdown when creating them. The values are given below.</p> Name Sample JSON value ReservationRequest <code>{\"patient\":{\"name\":\"John Doe\",\"dob\":\"1940-03-19\",\"ssn\":\"234-23-525\",\"address\":\"California\",\"phone\":\"8770586755\",\"email\":\"johndoe@gmail.com\",\"cardNo\":\"7844481124110331\"},\"doctor\":\"thomas collins\",\"hospital_id\":\"grandoaks\",\"hospital\":\"grand oak community hospital\",\"appointment_date\":\"2024-11-06\"}</code> ReservationStatus <code>{\"appointmentNo\":1, \"doctorName\":\"thomas collins\", \"patient\":\"John Doe\", \"actualFee\":7000.0, \"discount\":20, \"discounted\":5600.0, \"paymentID\":\"e560ea82-1c42-4972-a471-af5c1ad4995f\", \"status\":\"settled\"}</code> Appointment <code>{\"appointmentNumber\":12345,\"doctor\":{\"name\":\"Dr. Alice Carter\",\"hospital\":\"Green Valley Hospital\",\"category\":\"Cardiology\",\"availability\":\"Mon-Fri, 9 AM - 5 PM\",\"fee\":200},\"patientName\":\"Emma Johnson\",\"hospital\":\"Green Valley Hospital\",\"confirmed\":true,\"appointmentDate\":\"2024-11-20T10:00:00\"}</code> Fee <code>{\"patientName\":\"Emma Johnson\",\"doctorName\":\"Dr. Alice Carter\",\"actualFee\":\"150.00\"}</code> </li> <li> <p>The final types will look like the following.   </p> <p></p> </li> </ol>"},{"location":"integration-guides/integration-as-api/service-orchestration/#step-4-add-connections","title":"Step 4: Add connections","text":"<ol> <li>Navigate to design view and click on the Add Artifacts button and select Connection in the Other Artifacts section.</li> <li>Search and select the HTTP Client connector.</li> <li> <p>Enter the Url as <code>http://localhost:9090</code>, Connection Name as <code>hospitalEp</code> and click on the Save button.</p> <p> </p> </li> <li> <p>Add another connector for the payment backend service with the URL <code>http://localhost:9090/healthcare/payments</code> and the name <code>paymentEp</code>.    </p> <p> </p> </li> </ol> HTTP Connector<p>To learn more about HTTP client, see Ballerina HTTP Client. See supported client configurations in the HTTP Client Configurations.</p>"},{"location":"integration-guides/integration-as-api/service-orchestration/#step-5-design-the-resource","title":"Step 5: Design the resource","text":"<ol> <li>Click Add Resource and select POST method.</li> <li>Set the resource path as <code>categories/[string category]/reserve</code>.</li> <li>Define the payload type as <code>ReservationRequest</code>.</li> <li>Change the 201 response return type to <code>ReservationStatus</code>.</li> <li>Add a new response of type HttpNotFound under the responses.   </li> <li> <p>Click on the Save button to save the resource.   </p> <p> </p> </li> </ol>"},{"location":"integration-guides/integration-as-api/service-orchestration/#step-6-make-the-appointment-request","title":"Step 6: Make the appointment request","text":"<ol> <li>Click on the <code>categories/[string category]/reserve</code> resource to navigate to the resource implementation designer view.</li> <li>Delete the default <code>Return</code> action from the resource.</li> <li>Hover to the arrow after start and click the \u2795 button to add a new action to the resource.</li> <li>Select Declare Variable from the node panel on the left. This variable will be used to store the request payload for the hospital service.</li> <li>Change the variable name to <code>hospitalRequest</code>, type as <code>json</code> and expression as below.     <pre><code>     {\n     patient:{\n         name: reservation.patient.name,\n         dob: reservation.patient.dob,\n         ssn: reservation.patient.ssn,\n         address: reservation.patient.address,\n         phone: reservation.patient.phone,\n         email: reservation.patient.email\n      },\n     doctor: reservation.doctor,\n     hospital: reservation.hospital,\n     appointment_date: reservation.appointment_date\n    }\n</code></pre></li> <li> <p>Click on the Save button to add the variable.   </p> <p> </p> </li> <li> <p>Click \u2795 sign and select hospitalEp connector from the node panel and select post from the dropdown. Then, fill in the required fields with the values given below and click Save.</p> Field Value Variable Name <code>appointment</code> Variable Type <code>Appointment</code> Resource Path <code>string `/${payload.hospital_id}/categories/${category}/reserve`</code> message <code>hospitalRequest</code> </li> <li> <p>The connector action will look like the following.   </p> <p> </p> </li> </ol>"},{"location":"integration-guides/integration-as-api/service-orchestration/#step-7-get-the-fee","title":"Step 7: Get the fee","text":"<ol> <li> <p>Declare an int variable named <code>appointmentNumber</code> with expression <code>appointment.appointmentNumber</code> after the hospital service request.  </p> <p> </p> </li> <li> <p>Let's add another connector invocation to get the fee for the appointment. Click on the \u2795 sign and select hospitalServicesEp connector from the node panel.  </p> </li> <li> <p>Select get from the dropdown. Then, fill in the required fields with the values given below and click Save.</p> Field Value Variable Name <code>fee</code> Variable Type <code>Fee</code> Resource Path <code>string `/${payload.hospital_id}/categories/appointments/${appointmentNumber}/fee`</code> <p> </p> </li> </ol>"},{"location":"integration-guides/integration-as-api/service-orchestration/#step-8-make-the-payment","title":"Step 8: Make the payment","text":"<ol> <li>Declare a decimal type variable named <code>actualFee</code> with expression <code>check decimal:fromString(fee.actualFee)</code> after the fee request. </li> <li> <p>Create another new to prepare the payment request. Click on the \u2795 sign and select Declare Variable from the node panel. Add a variable named <code>paymentRequest</code> with the type json and expression as below.    <pre><code>{\n  appointmentNumber: appointmentNumber,\n  doctor: appointment.doctor.toJson(),\n  patient: check hospitalRequest.patient,\n  fee: actualFee,\n  confirmed: false,\n  card_number: reservation.patient.cardNo\n }\n</code></pre> </p> </li> <li> <p>Let's add another connector action to make the payment. Click on the \u2795 sign and select paymentEP connector from the node panel. Select post from the dropdown.   </p> <p> </p> </li> <li> <p>Then, fill in the required fields with the values given below and click Save.</p> Field Value Variable Name <code>status</code> Variable Type <code>ReservationStatus</code> Resource Path <code>\"/\"</code> message <code>paymentRequest</code> </li> <li> <p>Click on the \u2795 sign and select Return from the node panel. Add the <code>status</code> variable to the return node.</p> </li> <li> <p>The final integration will look like the following.   </p> <p> </p> </li> </ol>"},{"location":"integration-guides/integration-as-api/service-orchestration/#step-9-run-the-service","title":"Step 9: Run the service","text":"<ol> <li>Click on the Run button to start the service.</li> <li>Start the backend service by executing the following command in a terminal.     <pre><code>docker run --name hospital-backend -p 9090:9090 -d anuruddhal/kola-hospital-backend\n</code></pre></li> <li>Click on the Run on the run button (\u25b6\ufe0f) in the top right corner to run the service.</li> <li>The service will start and the service will be available at <code>http://localhost:8290/healthcare/categories/[category]/reserve</code>.</li> <li>Click on the Try it button to open the embedded HTTP client.</li> <li>Replace the {category} with <code>surgery</code> in the resource path and enter the following JSON payload in the request body and click on the \u25b6\ufe0f button to send the request.     <pre><code>    {\n\"patient\": {\n\"name\": \"John Doe\",\n\"dob\": \"1940-03-19\",\n\"ssn\": \"234-23-525\",\n\"address\": \"California\",\n\"phone\": \"8770586755\",\n\"email\": \"johndoe@gmail.com\",\n\"cardNo\": \"7844481124110331\"\n},\n\"doctor\": \"thomas collins\",\n\"hospital_id\": \"grandoak\",\n\"hospital\": \"grand oak community hospital\",\n\"appointment_date\": \"2023-10-02\"\n}\n</code></pre></li> <li> <p>The response will be similar to the following.    <pre><code> {\n\"appointmentNo\": 1,\n\"doctorName\": \"thomas collins\",\n\"patient\": \"John Doe\",\n\"actualFee\": 7000,\n\"discount\": 20,\n\"discounted\": 5600,\n\"paymentID\": \"b219c4ad-5365-4a22-ae35-048bb8e570e7\",\n\"status\": \"settled\"\n}\n</code></pre></p> <p> </p> </li> <li> <p>You can also test the service using the curl command.    <pre><code> curl -X POST \"http://localhost:8290/healthcare/categories/surgery/reserve\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n   \"patient\": {\n     \"name\": \"John Doe\",\n     \"dob\": \"1940-03-19\",\n     \"ssn\": \"234-23-525\",\n     \"address\": \"California\",\n     \"phone\": \"8770586755\",\n     \"email\": \"johndoe@gmail.com\",\n     \"cardNo\": \"7844481124110331\"\n   },\n   \"doctor\": \"thomas collins\",\n   \"hospital_id\": \"grandoak\",\n   \"hospital\": \"grand oak community hospital\",\n   \"appointment_date\": \"2023-10-02\"\n }'\n</code></pre></p> </li> </ol>"},{"location":"integration-guides/integration-as-api/service-orchestration/#step-10-stop-the-integration","title":"Step 10: Stop the integration","text":"<ol> <li>Click on the Stop button to stop the integration or press <code>Shift</code> + <code>F5</code>.</li> <li>Stop the hospital backend server by running the following command:    <pre><code>docker stop hospital-backend\n</code></pre></li> </ol>"},{"location":"integration-guides/usecases/datamapper/overview/","title":"Data Mapper Use Cases","text":"<p>The Data Mapper in WSO2 Integrator: BI enables you to visually transform data between formats, such as JSON, XML, and CSV, using an intuitive, low-code interface. This section provides a collection of end-to-end transformation examples that demonstrate practical, real-world integrations across various formats.</p>"},{"location":"integration-guides/usecases/datamapper/overview/#what-youll-learn","title":"What you\u2019ll learn","text":"<p>Each use case in this section helps you understand how to:</p> <ul> <li>Design input and output types in the Type Editor.</li> <li>Use inline or reusable Data Mappers.</li> <li>Apply mappings, aggregations, and expressions to manipulate data visually.</li> <li>Integrate transformations into automation or other integration flows.</li> </ul>"},{"location":"integration-guides/usecases/datamapper/overview/#available-use-cases","title":"Available use cases","text":"Use Case Description Read CSV File and Transform to XML File Demonstrates how to read a CSV file, map it to a structured <code>Orders</code> record, and convert it into XML output using an inline Data Mapper. Coming soon: JSON to XML Shows how to convert a nested JSON structure into XML with namespaces and attributes using the Data Mapper. Coming soon: XML to JSON Illustrates how to map XML elements into a simplified JSON representation. Coming soon: CSV to JSON Walks through mapping tabular data from a CSV file into structured JSON objects."},{"location":"integration-guides/usecases/datamapper/overview/#when-to-use-data-mapper","title":"When to use data mapper","text":"<p>Use the Data Mapper when you need to:</p> <ul> <li>Transform data between different payload formats (CSV, JSON, XML).</li> <li>Build file processing or ETL-style integrations.</li> <li>Create API payload transformations between client-facing and backend schemas.</li> <li>Minimize manual code and maintain readable, visual mappings.</li> </ul>"},{"location":"integration-guides/usecases/datamapper/overview/#related-topics","title":"Related topics","text":"<ul> <li>Data Mapping Quick Start</li> <li>Design the Integrations</li> <li>Create a Project</li> </ul>"},{"location":"integration-guides/usecases/datamapper/read-csv-file-and-transform-to-xml-file/","title":"Read a CSV File and Transform It to XML File","text":""},{"location":"integration-guides/usecases/datamapper/read-csv-file-and-transform-to-xml-file/#overview","title":"Overview","text":"<p>This is a low-code walkthrough that uses the Ballerina Integrator Data Mapper and file APIs to build an end-to-end pipeline \u2014 without writing code by hand. You will:</p> <ol> <li>Pick up files (CSV) from a folder (e.g., <code>input/</code>).</li> <li>Map each CSV row to a <code>&lt;Row&gt;</code> element inside <code>&lt;Orders&gt;</code> using the Data Mapper visual UI.</li> <li>Add a row number as a child <code>&lt;index&gt;</code> element. Use the mapper\u2019s row position function and set it to 0-based.</li> <li>Write the result to a new XML file in an <code>output/</code> folder.</li> </ol>"},{"location":"integration-guides/usecases/datamapper/read-csv-file-and-transform-to-xml-file/#why-this-use-case","title":"Why this use case","text":"<ul> <li>Converts flat CSV order data into XML that downstream, XML-centric systems can validate and consume.</li> <li>Demonstrates record indexing as an element (<code>&lt;index&gt;</code>) for traceability back to the original row.</li> <li>Scales easily from a single file to batch folders or listener-based near-real-time ingestion.</li> </ul>"},{"location":"integration-guides/usecases/datamapper/read-csv-file-and-transform-to-xml-file/#input-csv-example","title":"Input CSV Example","text":"<pre><code>order_id,sku,qty,price\nS001,P-1001,2,149.99\nS002,P-3001,3,39.99\nS003,P-2003,1,89.50\n</code></pre>"},{"location":"integration-guides/usecases/datamapper/read-csv-file-and-transform-to-xml-file/#expected-output-xml","title":"Expected Output XML","text":"<pre><code>&lt;Orders&gt;\n&lt;Row&gt;\n&lt;index&gt;0&lt;/index&gt;\n&lt;order_id&gt;S001&lt;/order_id&gt;\n&lt;sku&gt;P-1001&lt;/sku&gt;\n&lt;qty&gt;2&lt;/qty&gt;\n&lt;price&gt;149.99&lt;/price&gt;\n&lt;/Row&gt;\n&lt;Row&gt;\n&lt;index&gt;1&lt;/index&gt;\n&lt;order_id&gt;S002&lt;/order_id&gt;\n&lt;sku&gt;P-3001&lt;/sku&gt;\n&lt;qty&gt;3&lt;/qty&gt;\n&lt;price&gt;39.99&lt;/price&gt;\n&lt;/Row&gt;\n&lt;Row&gt;\n&lt;index&gt;2&lt;/index&gt;\n&lt;order_id&gt;S003&lt;/order_id&gt;\n&lt;sku&gt;P-2003&lt;/sku&gt;\n&lt;qty&gt;1&lt;/qty&gt;\n&lt;price&gt;89.50&lt;/price&gt;\n&lt;/Row&gt;\n&lt;/Orders&gt;\n</code></pre>"},{"location":"integration-guides/usecases/datamapper/read-csv-file-and-transform-to-xml-file/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have the following:</p> <ul> <li>Visual Studio Code: Install Visual Studio Code if you don't have it already.</li> <li>WSO2 Integrator: BI Extension: Install the WSO2 Integrator: BI extension. Refer to Install WSO2 Integrator: BI for detailed instructions.</li> </ul>"},{"location":"integration-guides/usecases/datamapper/read-csv-file-and-transform-to-xml-file/#step-1-creating-a-new-integration-project","title":"Step 1: Creating a new integration project","text":"<p>WSO2 Integrator: BI extension provides a low-code graphical environment to visually design, map, and deploy integrations using Ballerina.</p> <ol> <li> <p>Launch VS Code and click the WSO2 Integrator: BI icon on the left sidebar.     You\u2019ll be taken to the welcome page that introduces the integration workspace.</p> </li> <li> <p>Under the Get Started Quickly section, you\u2019ll see three main options:</p> <ul> <li>Create New Integration \u2013 start a new integration project from scratch using the graphical designer.</li> <li>Import External Integration \u2013 bring in existing integrations from other platforms (e.g., MuleSoft, TIBCO).</li> <li>Explore Pre-Built Samples \u2013 open existing templates or tutorials.</li> </ul> </li> <li> <p>Click Create New Integration.    This opens the integration creation wizard, where you can:</p> <ul> <li>Define the integration name and location.  </li> <li>Choose to start with a blank project or from a sample template.  </li> <li>Initialize the workspace structure with folders for input/output mappings, resources, and configuration.</li> </ul> </li> <li> <p>Once the project is created, you\u2019ll enter the graphical integration designer view.     From here, you can start adding connectors, data mappings, and logic components to build your flow visually.</p> </li> </ol> <p>This is the entry point for all low-code projects in Ballerina Integrator.</p> <p></p>"},{"location":"integration-guides/usecases/datamapper/read-csv-file-and-transform-to-xml-file/#step-2-add-configurable-variables","title":"Step 2: Add configurable variables","text":"<p>In this step, you will define the input and output file paths as configurable variables. These parameters make your integration portable and environment-agnostic \u2014 you can change file paths without editing the logic.</p> <ol> <li>In the Project Explorer, select Configurations.</li> <li>Click + Add Configurable to open the Configurable Variables panel.</li> <li> <p>Create the following two variables:</p> <ul> <li> <p>Name: <code>inputCSV</code> Type: <code>string</code> Default Value: <code>./input/customer_order_details.csv</code> Description: Path to the source CSV file that contains the customer order details.</p> </li> <li> <p>Name: <code>outputXML</code> Type: <code>string</code> Default Value: <code>./output/customer_order_details.xml</code> Description: Output path where the generated XML file will be saved.</p> </li> </ul> </li> <li> <p>Click Save once both configurables are defined.</p> <p>Your Configurable Variables view should now display:</p> Name Type Default Value inputCSV string <code>./input/customer_order_details.csv</code> outputXML string <code>./output/customer_order_details.xml</code> </li> </ol> <p>These configurables can now be used by the file connectors and the data mapper in later steps \u2014 allowing your integration to dynamically read and write files based on runtime settings.</p> <p></p>"},{"location":"integration-guides/usecases/datamapper/read-csv-file-and-transform-to-xml-file/#step-3-create-a-structure-to-represent-each-csv-row","title":"Step 3: Create a structure to represent each csv row","text":"<p>In this step, you\u2019ll define a structure (called a Type in Ballerina) that describes what one row in your CSV file looks like. Think of it as creating a template so the Data Mapper can recognize each column by name and map them correctly to the XML later.</p> <ol> <li>In the Project Explorer, click Types and then + Add Type.</li> <li>Enter a name for the new structure: <code>CSV</code>.</li> <li> <p>Add the following fields \u2014 these represent the columns in your input CSV file:</p> Field Name Data Type Description <code>order_id</code> Text (string) The unique ID for each order. <code>sku</code> Text (string) The product code or SKU for the item. <code>qty</code> Text (string) The quantity ordered. <code>price</code> Text (string) The item price for that row. </li> <li> <p>Click Save once all fields are added. </p> </li> </ol> <p>Your CSV structure now acts as the blueprint for reading each record from the file.</p> <p>When the system reads the CSV file, it will treat every line (after the header) as one <code>CSV</code> record containing these four fields.</p> <p>This step helps the Data Mapper understand the shape of your data \u2014 instead of dealing with raw text lines, it can now work with meaningful fields like <code>order_id</code>, <code>sku</code>, <code>qty</code>, and <code>price</code>.</p> <p></p>"},{"location":"integration-guides/usecases/datamapper/read-csv-file-and-transform-to-xml-file/#step-4-generate-xml-types-from-a-sample-payload","title":"Step 4: Generate xml types from a sample payload","text":"<p>In this step, you\u2019ll create the XML output structure automatically by pasting a sample XML. The Integrator\u2019s Type Creator reads this example and builds the corresponding type definitions for you.</p> <ol> <li>In the Project Explorer, click Types, then click + Add Type.  </li> <li>Go to the Import tab.  </li> <li> <p>Paste your sample XML payload in the dialog box. For this example, use:</p> <pre><code>&lt;Orders&gt;\n&lt;Row&gt;\n&lt;index&gt;1&lt;/index&gt;\n&lt;order_id&gt;S001&lt;/order_id&gt;\n&lt;sku&gt;P-1001&lt;/sku&gt;\n&lt;qty&gt;2&lt;/qty&gt;\n&lt;price&gt;149.99&lt;/price&gt;\n&lt;/Row&gt;\n&lt;Row&gt;\n&lt;index&gt;2&lt;/index&gt;\n&lt;order_id&gt;S002&lt;/order_id&gt;\n&lt;sku&gt;P-3001&lt;/sku&gt;\n&lt;qty&gt;3&lt;/qty&gt;\n&lt;price&gt;39.99&lt;/price&gt;\n&lt;/Row&gt;\n&lt;Row&gt;\n&lt;index&gt;3&lt;/index&gt;\n&lt;order_id&gt;S003&lt;/order_id&gt;\n&lt;sku&gt;P-2003&lt;/sku&gt;\n&lt;qty&gt;1&lt;/qty&gt;\n&lt;price&gt;89.50&lt;/price&gt;\n&lt;/Row&gt;\n&lt;/Orders&gt;\n</code></pre> </li> <li> <p>Click Import to load the sample XML into the type creator. </p> </li> </ol> <p>The Integrator will automatically analyze the payload and infer the XML structure. </p> <p>The generated structure will appear as:</p> <pre><code>Orders\n \u2514\u2500 Row[]\n     \u251c\u2500 index: int\n     \u251c\u2500 order_id: string\n     \u251c\u2500 sku: string\n     \u251c\u2500 qty: int\n     \u2514\u2500 price: decimal\n</code></pre> <p>You now have an record type that defines the exact structure of your output xml file. This type will act as the target structure in the Data Mapper, allowing each <code>CSV</code> record to be mapped directly into a <code>&lt;Row&gt;</code> element under <code>&lt;Orders&gt;</code>.</p> <p></p>"},{"location":"integration-guides/usecases/datamapper/read-csv-file-and-transform-to-xml-file/#step-5-create-an-automation-and-set-up-csv-reading","title":"Step 5: Create an automation and set up csv reading","text":"<p>In this step, you\u2019ll create an Automation entry point in WSO2 Integrator BI and configure it to read a CSV file from the path defined in your configurable variable (<code>inputCSV</code>). This automation serves as the starting point of the data-transformation pipeline that converts CSV orders into XML output.</p> <ol> <li> <p>Open the \u201cEntry Points\u201d Panel</p> <p>From the Project Explorer on the left, expand the <code>customer_details_csv_to_xml</code> integration.   Click Entry Points (+) to add a new integration artifact.</p> </li> <li> <p>Select \u201cAutomation\u201d as the Entry Point</p> <p>The Artifacts palette opens.   Under the Automation section (top of the list), click Automation.   Automation integrations can be triggered manually or periodically, and are ideal for scheduled file transformations.</p> </li> <li> <p>Create the Automation</p> <p>A page titled Automation \u2013 An automation that can be invoked periodically or manually appears.   Click the blue Create button to add a new Automation flow.   You may optionally expand Optional Configurations to adjust scheduling or trigger settings, but for this tutorial we keep defaults.</p> <p>The tool shows a brief \u201cCreating\u2026\u201d indicator, and then a new canvas opens.</p> </li> <li> <p>Observe the Automation Canvas</p> <p>Once created, the visual editor displays a blank automation diagram with a starting node.   This represents the root of your pipeline where you will define the steps to read, map, and write data.</p> </li> </ol>"},{"location":"integration-guides/usecases/datamapper/read-csv-file-and-transform-to-xml-file/#step-6-add-a-file-read-function-to-the-automation-flow","title":"Step 6: Add a file read function to the automation flow","text":"<p>In this step, you\u2019ll configure the automation\u2019s first functional node \u2014 a Call Function block \u2014 to read CSV data from the configured input path (<code>inputCSV</code>) into an array of CSV records (<code>CSV[]</code>). This forms the initial data acquisition step of the workflow.</p> <ol> <li> <p>Open the Automation Flow</p> <p>With the automation already created, you\u2019ll now see a Start node connected to an Error Handler node.   This is the default template for new automations.</p> </li> <li> <p>Add the First Node</p> <ol> <li>Hover your mouse below the Start node until a small \u201c+\u201d button appears.  </li> <li>Click + to add a new operation.     The system briefly displays <code>Generating next suggestion...</code> \u2014 this means it\u2019s fetching available node types.</li> </ol> </li> <li> <p>Choose the Node Type</p> <p>Once suggestions appear, look to the right-hand Node Panel under the Statement category.   You\u2019ll see options such as:</p> <ul> <li>Declare Variable  </li> <li>Assign  </li> <li>Call Function </li> <li>Map Data  </li> </ul> </li> </ol> <p>Click Call Function to insert this node into the automation flow.</p>"},{"location":"integration-guides/usecases/datamapper/read-csv-file-and-transform-to-xml-file/#step-7-use-filereadcsv-to-load-the-csv-file-into-memory","title":"Step 7: Use <code>fileReadCsv</code> to load the csv file into memory","text":"<p>Now that your Call Function flow is open, the next step is to configure a CSV File Read Function using the built-in <code>io:fileReadCsv()</code> operation. This function reads CSV data from the path defined in your project\u2019s configurables and loads it as a structured array of <code>CSV</code> records that can be mapped later.</p> <ol> <li> <p>Open the Function Search Panel</p> <p>In the right-side Functions panel, scroll or search for a CSV-related function:</p> <ol> <li>Click inside the Search library functions field.  </li> <li>Type csv to filter the standard library.  </li> <li>You\u2019ll see a list under data.csv and io modules containing functions like:<ul> <li><code>parseStream</code></li> <li><code>parseString</code></li> <li><code>fileReadCsv</code></li> <li><code>fileWriteCsv</code></li> </ul> </li> </ol> </li> <li> <p>Select <code>io:fileReadCsv</code></p> <p>Click fileReadCsv from the results list. A configuration panel opens on the right describing the function:</p> <p>\u201cRead file content as a CSV. When the expected data type is record[], the first entry of the CSV file should contain matching headers.\u201d</p> </li> <li> <p>Configure Parameters</p> <p>You can see the fields as follows:</p> Field Description Example Path Path to your CSV file <code>${inputCSV}</code> (the variable created in Configurables) Result Name of the result variable that stores the read content <code>csvRecords</code> Return Type The data type of the returned value <code>CSV[]</code> <p>If the return type list doesn\u2019t appear automatically, click the blue record selector icon, expand Types \u2192 CSV, and select CSV[] from your defined records.</p> </li> <li> <p>Bind the CSV Reader function to a Configurable Path</p> <p>After selecting the <code>io:fileReadCsv</code> function, the next task is to configure how the file path is supplied.   Rather than hardcoding a literal path, this step dynamically binds the function to a configurable variable (<code>inputCSV</code>) defined in your project\u2019s configurations.</p> <ol> <li> <p>Switch the Path Input Mode</p> <p>In the <code>fileReadCsv</code> configuration pane:</p> <ol> <li>Locate the Path* field at the top.</li> <li>By default, it shows a Text input mode.</li> <li>Click Expression next to it.     This changes the mode to accept dynamic values such as variables or configurables instead of a static string.</li> </ol> </li> <li> <p>Open the Value Picker</p> <p>Once in Expression mode:</p> <ol> <li>A blue \u0192x button appears to the left of the input field.  </li> <li>Click that icon to open the Value Picker dropdown.  </li> <li> <p>The picker shows several categories:</p> <ul> <li><code>Create Value</code></li> <li><code>Inputs</code></li> <li><code>Variables</code></li> <li>Configurables </li> <li><code>Functions</code></li> </ul> </li> </ol> </li> <li> <p>From the Configurables section, select <code>inputCSV</code> (this is the configurable variable that stores the file path for the input CSV).</p> </li> </ol> </li> <li> <p>Name the Result Variable</p> <p>In the Result* field, enter a meaningful variable name to store the data returned by the reader: csvRecords This variable will now hold the array of CSV records returned from <code>fileReadCsv</code>.</p> </li> <li> <p>Define the Return Type</p> <p>In Return Type*, click the type selector box and pick from the list of available record types under your Types section. This ensures the system interprets each row of the CSV as a structured <code>CSV</code> record instead of a generic string map.</p> </li> <li> <p>Save the Configuration</p> <p>Click Save to insert the configured node into the automation flow.</p> <p>You now have a fully functional CSV reader node that:</p> <ul> <li>Reads the file from <code>${inputCSV}</code>,</li> <li>Parses its contents into structured <code>CSV[]</code> data,</li> <li>Stores it in the variable <code>csvRecords</code>.</li> </ul> </li> </ol> <p>The fileReadCsv node is added to your Automation diagram, connected directly under Start.</p> <p>This means the Automation will:</p> <ol> <li>Start execution.  </li> <li>Use <code>fileReadCsv</code> to read the input file located at the path stored in <code>${inputCSV}</code>.  </li> <li>Store the resulting CSV records in the variable <code>csvData</code> for later mapping.</li> </ol> <p>Tip: Always ensure the first row of your CSV file includes headers that match the <code>CSV</code> record field names. Otherwise, <code>fileReadCsv</code> will not correctly map the columns to your CSV record.</p> <p></p>"},{"location":"integration-guides/usecases/datamapper/read-csv-file-and-transform-to-xml-file/#step-8-create-the-xml-output-variable-and-open-data-mapper","title":"Step 8: Create the xml output variable and open data mapper","text":"<p>Now that the CSV data has been successfully read into the variable <code>csvRecords</code>, the next step is to prepare the XML output structure that the CSV will be transformed into. For this, you will declare a new variable of type <code>Orders</code> and open it in the Data Mapper.</p> <ol> <li> <p>Add a \u201cDeclare Variable\u201d Node</p> <ol> <li>Click the + icon below the <code>io:fileReadCsv</code> node in the automation flow.</li> <li>In the right-hand panel under Statement, select Declare Variable.</li> <li>A new node placeholder titled \u201cSelect node from node panel\u201d appears in the flow.</li> </ol> <p>This node will define a new variable to hold the transformed XML data.</p> </li> <li> <p>Configure the Variable name</p> <p>In the properties pane on the right:</p> <ul> <li>Name: xmlRecord</li> </ul> <p>This variable name represents the XML output structure that will hold the mapped data.</p> </li> <li> <p>Assign the Variable Type</p> <ol> <li>Click the Type field (marked with the blue <code>T</code> icon).</li> <li>Begin typing <code>Order</code>.</li> <li>From the dropdown suggestions, select Orders.</li> </ol> <p>This binds the variable to your predefined <code>Orders</code> record type, which defines the XML schema structure for the output.</p> </li> <li> <p>Open the Data Mapper</p> <p>Once the variable name and type are defined, click Open in Data Mapper at the bottom right of the properties pane.</p> <p>This action opens the Data Mapper where you can map CSV fields to XML elements visually.</p> </li> <li> <p>Explore the Data Mapper Interface</p> <p>Inside the Data Mapper view, you\u2019ll see two main panels:</p> <p>Left Panel \u2014 Input Data</p> <ul> <li><code>csvRecords</code> \u2192 Type: <code>CSV[]</code></li> <li><code>inputCSV</code> \u2192 Type: <code>string</code></li> <li><code>outputXML</code> \u2192 Type: <code>string</code></li> </ul> <p>Right Panel \u2014 Output Structure</p> <ul> <li><code>xmlRecord</code> \u2192 Type: <code>Orders</code></li> <li><code>Row[]</code><ul> <li><code>index</code> (int)</li> <li><code>order_id</code> (string)</li> <li><code>sku</code> (string)</li> <li><code>qty</code> (int)</li> <li><code>price</code> (decimal)</li> </ul> </li> </ul> <p>This view provides a visual workspace to connect input CSV fields with XML output fields directly.</p> </li> </ol> <p>At this point, you have:</p> <ol> <li>Declared a new variable named xmlRecord of type <code>Orders</code>.</li> <li>Opened it inside the Data Mapper.</li> <li>Verified that both <code>csvRecords</code> (input) and <code>xmlRecord</code> (output) are available for mapping.</li> </ol> <p>You\u2019re now ready to perform the visual mapping between CSV and XML data structures.</p> <p></p>"},{"location":"integration-guides/usecases/datamapper/read-csv-file-and-transform-to-xml-file/#step-9-map-csv-fields-to-xml-rows-in-the-data-mapper","title":"Step 9: Map csv fields to xml rows in the data mapper","text":"<p>Now that you\u2019ve opened the Data Mapper view, it\u2019s time to visually connect the fields between your CSV input and XML output. This step transforms the tabular data structure into a hierarchical XML format.</p> <ol> <li> <p>Understand the Mapping Context</p> <p>On the left, you have:</p> <ul> <li><code>csvRecords</code> \u2192 Type: <code>CSV[]</code><ul> <li>Each item contains: <code>order_id</code>, <code>sku</code>, <code>qty</code>, and <code>price</code> (all as <code>string</code>).</li> </ul> </li> </ul> <p>On the right, you have:</p> <ul> <li><code>xmlRecord</code> \u2192 Type: <code>Orders</code><ul> <li>Contains: <code>Row[]</code><ul> <li>Each <code>Row</code> has: <code>index (int)</code>, <code>order_id (string)</code>, <code>sku (string)</code>, <code>qty (int)</code>, and <code>price (decimal)</code>.</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Start the Mapping by Linking the Record Arrays</p> <ol> <li>Drag from <code>csvRecords</code> (left) to <code>Row</code> (right).</li> <li>This creates a parent-level mapping between the two collections.</li> </ol> <p>If you see an error such as:</p> <p><code>incompatible types: expected 'Row[]', found 'CSV[]'</code></p> <p>That\u2019s expected \u2014 the data mapper is warning that nested mapping is required to handle field-level conversions.</p> </li> <li> <p>Map Individual Fields</p> <ol> <li>Expand both <code>csvRecordsItem</code> and <code>RowItem</code>.</li> <li>Start connecting matching fields:<ul> <li><code>order_id</code> \u2192 <code>order_id</code></li> <li><code>sku</code> \u2192 <code>sku</code></li> <li><code>qty</code> \u2192 <code>qty</code></li> <li><code>price</code> \u2192 <code>price</code></li> </ul> </li> </ol> <p>You\u2019ll notice that type mismatches occur for numeric fields (<code>qty</code>, <code>price</code>) since CSV data is read as strings.</p> </li> <li> <p>Resolve Type Mismatches for Numeric Fields</p> <p>When connecting <code>qty</code> and <code>price</code>, errors like the following appear:</p> <p><code>incompatible types: expected 'int', found 'string'</code> <code>incompatible types: expected 'decimal', found 'string'</code></p> <p>To fix this, use type conversion expressions directly in the Data Mapper:</p> <p>For <code>qty</code>:</p> <ol> <li>Click on the red error line between <code>qty</code> fields.</li> <li> <p>In the expression editor at the top, replace it with:</p> <pre><code>check int:fromString(csvRecordsItem.qty)\n</code></pre> </li> </ol> <p>For <code>price</code></p> <ol> <li>Click on the <code>price</code> connection in the mapping canvas.</li> <li> <p>In the expression editor at the top, replace the existing mapping with the following conversion function:</p> <pre><code>check decimal:fromString(csvRecordsItem.price)\n</code></pre> </li> </ol> <p>This ensures the string values from the CSV are safely parsed into numeric types before mapping.</p> <p></p> </li> <li> <p>Add the Missing <code>index</code> Mapping</p> <p>The <code>index</code> field in the Row record is required and cannot be left unmapped. If you skip it, the compiler will show this error:</p> <p><code>missing non-defaultable required record field 'index'</code></p> <p>To fix this, we\u2019ll dynamically calculate the index value for each CSV record by following the below steps.</p> <ol> <li>In the Data Mapper, click on the <code>index</code> field under the <code>Row</code> structure (right side).</li> <li> <p>In the expression editor at the top, type the following expression:           <pre><code>csvRecords.indexOf(csvRecordsItem)\n</code></pre></p> <p>This retrieves the current record\u2019s position in the csvRecords array (starting from 0).</p> </li> <li> <p>If you see an error like:</p> <p><code>incompatible types: expected 'int', found 'int?'</code></p> <p>that means the result is an optional integer. To handle it safely, wrap it with <code>&lt;int&gt;</code>:       <pre><code>&lt;int&gt;csvRecords.indexOf(csvRecordsItem)\n</code></pre></p> </li> </ol> <p>Once applied, the red error indicator on the index mapping disappears.</p> <p>All lines in the mapper should now appear blue, indicating valid and complete mappings.</p> </li> </ol> <p>Your data mapper now correctly converts CSV input into a fully-typed XML structure, with each record assigned an auto-generated index.</p> <p></p>"},{"location":"integration-guides/usecases/datamapper/read-csv-file-and-transform-to-xml-file/#step-10-convert-the-orders-record-to-xml-and-write-it-to-the-output-file","title":"Step 10: Convert the <code>Orders</code> record to xml and write it to the output file","text":"<p>You already have <code>xmlRecord : Orders</code> populated by the Data Mapper. Now we\u2019ll turn that record into XML and save it.</p> <ol> <li> <p>Add <code>xmldata:toXml</code> to convert the record to XML</p> <ol> <li>Open the right-side Functions panel.</li> <li>Search for <code>toXml</code> (from Standard Library \u2192 data.xmldata).</li> <li> <p>Click toXml to insert the node below the Declare Variable node.</p> <p>Configure the toXml node:</p> <ul> <li>Map Value: <code>xmlRecord</code></li> <li>Result: <code>xmlResultAsString</code></li> <li>Result Type: <code>xml</code></li> </ul> </li> </ol> <p>After saving, you should see <code>xmldata:toXml</code> in the flow with the result variable name xmlResultAsString.</p> </li> <li> <p>Add <code>io:fileWriteXml</code> to write the XML to disk</p> <ol> <li>With the Functions panel open, search for <code>fileWriteXml</code> (under Imported Functions \u2192 io).</li> <li> <p>Click fileWriteXml to insert it after <code>xmldata:toXml</code>.</p> <p>Configure the fileWriteXml node:</p> <ul> <li>Path: <code>outputXML</code> (this is your output file path variable/string.)<ol> <li>Toggle from Text to Expression by clicking on the Expression icon at the right top corner of the Path Textbox</li> <li>Choose the configurables and select outputXML</li> </ol> </li> <li>Content: <code>xmlResultAsString</code></li> </ul> </li> </ol> <p>Click Save.</p> </li> <li> <p>Verify the flow and run</p> <p>Your flow should now show (top \u2192 bottom):</p> <ul> <li>io:fileReadCsv \u2192 Declare Variable (xmlRecord) \u2192 xmldata:toXml (xmlResultAsString) \u2192 io:fileWriteXml (outputXML, xmlResultAsString) \u2192 Error Handler</li> </ul> </li> </ol> <p>Make sure the input CSV file is in the correct location identified by the <code>inputCSV</code> configurable.</p> <p>Run the integration. On success, the XML produced from <code>xmlRecord</code> will be written to the file path in <code>outputXML</code>.</p> <p></p>"},{"location":"observability-and-monitoring/monitoring-with-wso2-integrator-icp/","title":"Monitoring with WSO2 Integrator: ICP","text":"<p>The WSO2 Integrator: ICP monitors the runtime artifacts in a deployment. It provides a graphical view of the integration artifacts that are deployed. In this guide, you will learn how to enable ICP for an integration developed using WSO2 Integrator: BI.</p>"},{"location":"observability-and-monitoring/monitoring-with-wso2-integrator-icp/#prerequisites","title":"Prerequisites","text":"<ol> <li>Java 11 or later versions should be installed on your machine.</li> <li>You must set your <code>JAVA_HOME</code> environment variable to point to the directory where the Java Development Kit (JDK) is installed on the computer.</li> </ol>"},{"location":"observability-and-monitoring/monitoring-with-wso2-integrator-icp/#step-1-download-and-start-icp-server","title":"Step 1: Download and start ICP server","text":"<ol> <li>Go to the WSO2 Integrator: ICP web page. </li> <li>Click Download. </li> <li>Provide the necessary details. </li> <li>Click Zip Archive to download the ICP as a ZIP file. </li> <li>Extract the archive file to a dedicated directory for the ICP, which will hereafter be referred to as <code>&lt;ICP_HOME&gt;</code>.</li> <li>Open a terminal and navigate to the <code>&lt;ICP_HOME&gt;/bin</code> folder.</li> <li>Execute one of the commands given below.</li> </ol> On MacOS/LinuxOn Windows <pre><code>./dashboard.sh\n</code></pre> <pre><code>dashboard.bat\n</code></pre>"},{"location":"observability-and-monitoring/monitoring-with-wso2-integrator-icp/#step-2-access-the-icp-dashboard","title":"Step 2: Access the ICP dashboard","text":"<ol> <li>Open a web browser and navigate to https://localhost:9743/dashboard.</li> <li> <p>Log in using the default credentials: </p> <ul> <li>Username: <code>admin</code></li> <li>Password: <code>admin</code></li> </ul> <p></p> </li> </ol>"},{"location":"observability-and-monitoring/monitoring-with-wso2-integrator-icp/#step-3-deploy-the-integration","title":"Step 3: Deploy the integration","text":"<ol> <li>Navigate to the Visualizer view by clicking on the BI icon on the sidebar.</li> <li> <p>Check Enable ICP under the Integration Control Plane section in the right panel.</p> <p></p> </li> <li> <p>Click on the Run button to start the integration. </p> </li> <li> <p>Click on the Create Config.toml on the prompt to create the <code>Config.toml</code> file.</p> <p></p> </li> <li> <p>Click on the Edit in Config.toml button in the top right corner and, update the <code>Config.toml</code> file with the following entry.</p> <p></p> <pre><code>  [ballerinax.wso2.controlplane.dashboard]\nurl = \"https://localhost:9743/dashboard/api\"\nheartbeatInterval = 10\ngroupId = \"cluster1\"\nmgtApiUrl =\"https://localhost:9264/management/\"\n</code></pre> </li> <li> <p>Click on the Run button to start the integration.  </p> <p></p> </li> <li> <p>A log message will be displayed in the console indicating that the integration is connected to the ICP dashboard.  </p> <pre><code>time=2025-03-17T15:14:59.970+05:30 level=INFO module=ballerinax/wso2.controlplane message=\"Connected to dashboard server https://localhost:9743/dashboard/api\"\n</code></pre> </li> </ol>"},{"location":"observability-and-monitoring/monitoring-with-wso2-integrator-icp/#step-4-view-the-integration-in-the-icp-dashboard","title":"Step 4: View the integration in the ICP dashboard","text":"<ol> <li>Go to the ICP dashboard and log in https://localhost:9743/dashboard.</li> <li>In the dashboard, you will see the integration details.</li> <li> <p>Click on the node to view the node details.   </p> <p></p> </li> <li> <p>Click on the Services to view the listener and resources of the service. </p> <p></p> </li> <li> <p>Click on the Listeners to view details of the listener.</p> <p></p> </li> </ol>"},{"location":"observability-and-monitoring/observability-with-devant/","title":"Observability with Devant","text":"<p>Integrations developed with WSO2 Integrator: BI can be deployed to Devant, where built-in observability tools provide deep insights into service behavior and performance. Refer to the Deploy to Devant guide for instructions on deploying to Devant. The Devant observability dashboard provides a comprehensive interface to visualize and monitor the performance of services deployed on Devant.</p> <p> </p> <p>The Observability dashboard allows you to:</p> <ul> <li>View runtime logs generated over a specific timeframe.</li> <li>Observe the throughput and latencies of requests served over a given period.</li> <li>Observe the CPU and memory usage over a given period.</li> <li>Compare metrics side-by-side to facilitate efficient diagnosis.</li> </ul>"},{"location":"observability-and-monitoring/observability-with-devant/#logs","title":"Logs","text":"<p>The Logs pane serves as a centralized view to observe logs of the integrations you deploy on Devant. This facilitates rigorous troubleshooting and analysis.</p> <p> </p>"},{"location":"observability-and-monitoring/observability-with-devant/#metrics","title":"Metrics","text":"<p>The Metrics pane provides a graphical representation of the following metrics:</p> <ul> <li>Request per minute</li> <li>Latency</li> <li>Memory usage</li> <li>CPU usage</li> <li>Data transfer</li> <li>Disk usage</li> </ul> <p> </p> <p>By default, Devant renders this graph for the data generated within the past 24 hours. You can change the default time window by selecting the time range and zone from the options bar. To expand the graph, click and drag the cursor over the period you want to drill down.</p> <p>You can view the Devant service logs in the Runtime Logs pane below the metrics. Clicking on a graph updates the Runtime Logs view to contain the corresponding log entries generated at that time. You can use these logs to identify the reasons for any anomalies you detect using the graph.</p>"},{"location":"observability-and-monitoring/overview/","title":"Overview of Observability and Monitoring","text":"<p>Observability is a measure of how well the internal states of a system can be understood from its external outputs.</p> <p>In BI, observability is a core feature that helps monitor, debug, and optimize integration services. It focuses on the following three key pillars:</p> <p>Metrics \u2013 Numeric data collected and aggregated over time to monitor system performance.</p> <p>Tracing \u2013 Tracking the flow of requests or messages through various services and components, from entry to exit.</p> <p>Logging \u2013 Text-based records of application behavior, annotated with timestamps and contextual information.</p> <p>Observability platforms allow developers and operators to gain insight into system behavior, troubleshoot issues, and ensure reliability in production deployments.</p>"},{"location":"observability-and-monitoring/overview/#observability-in-bi","title":"Observability in BI","text":"<p>BI provides built-in support for observability across its runtime. Integration services, APIs, and connectors emit rich telemetry data that can be exported to standard monitoring tools.</p>"},{"location":"observability-and-monitoring/overview/#available-observability-options","title":"Available Observability Options","text":"<p>BI supports multiple options for observing and monitoring deployed integrations. Depending on the deployment environment and the level of visibility required, you can choose from the following observability solutions.</p>"},{"location":"observability-and-monitoring/overview/#1-wso2-integrator-icp-integration-control-plane","title":"1. WSO2 Integrator: ICP (Integration Control Plane)","text":"<p>The WSO2 Integrator: ICP provides centralized monitoring and management of runtime artifacts across a deployment. It offers a graphical interface to view deployed integration artifacts and their relationships, enabling teams to,</p> <ul> <li>Track the status and availability of deployed services.</li> <li>View runtime metadata and node-level details.</li> <li>Manage and visualize the health of integration components across environments.</li> </ul> <p>This is ideal for teams that manage integrations across hybrid or distributed environments and require control-plane-level visibility. Refer to Monitoring with WSO2:Integrator ICP for further details.</p>"},{"location":"observability-and-monitoring/overview/#2-devant-by-wso2","title":"2. Devant by WSO2","text":"<p>Devant is WSO2\u2019s AI-powered Integration Platform as a Service (iPaaS). When BI integrations are deployed to Devant, the platform provides built-in observability capabilities, including the following.</p> <ul> <li>A unified dashboard for visualizing service performance and metrics.</li> <li>Insight into request flow, throughput, latency, and error rates.</li> <li>AI-powered analytics to detect anomalies and optimize system performance.</li> </ul> <p>Devant observability is suitable for cloud-native teams looking for a fully managed, intelligent monitoring experience. Refer to Observability with Devant for further details.</p>"},{"location":"observability-and-monitoring/overview/#3-integration-with-third-party-observability-tools","title":"3. Integration with Third-Party Observability Tools","text":"<p>BI is designed to work seamlessly with standard observability stacks. You can integrate your deployments with tools like,</p> <ul> <li>Prometheus and Grafana for metrics collection and visualization.</li> <li>Jaeger for distributed tracing.</li> <li>ELK for centralized logging.</li> </ul> <p>These integrations provide flexibility for teams already invested in their own observability ecosystems and allow for consistent monitoring practices across different services and platforms. Refer to Supported Observability Tools and Platforms for further details.</p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/datadog/","title":"Observe metrics and tracing using Datadog","text":"<p>The sample shop service will be used in this guide. Follow the steps given below to observe BI tracing and metrics in Datadog.</p> <p>Create a new account in Datadog. Select a billing plan according to your needs (A free plan is also included).</p> <p>Then follow the steps below to set up your Datadog account to view metrics and tracing provided by Ballerina.</p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/datadog/#step-1-create-a-datadog-account-and-an-api-key","title":"Step 1 - Create a Datadog account and  an API key","text":"<ol> <li> <p>Add Prometheus to the Integrations for your account</p> <p>You need to add Prometheus in the Integrations. Please go to the Integrations tab and search for Prometheus.</p> <p></p> </li> <li> <p>Create an API key</p> <p>You need to create an API key for the Datadog agent. To create an API key, <code>Click Profile \u2192 Organization Settings \u2192 API keys</code></p> <p></p> </li> </ol>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/datadog/#step-2-set-up-the-datadog-agent","title":"Step 2 - Set up the Datadog agent","text":"<p>After setting up your Datadog account, you need to set up a Datadog Agent in your instance.</p> <p>You can follow this documentation to get started with the Datadog agent on your local machine.</p> <p>You need to include the API key you generated in your Datadog account to <code>datadog.yaml</code> in the <code>datadog-agent/etc</code> folder.</p> <p>Then follow the steps below to configure metrics and tracing data publishing to Datadog.</p> <ol> <li> <p>Add configuration for metrics</p> <p>Once you add Prometheus by following <code>step 1</code>, you will get a guide to configure a Datadog agent in your instance.</p> <p></p> <p>You can follow the instructions given in the above configuration to set up a Datadog agent.</p> <p>A sample of the <code>conf.yaml</code> file which you should include in the prometheus.d folder can be found here.</p> <pre><code>```yaml\ninit_config:\n\ninstances:\n- prometheus_url: http://localhost:9797/metrics\n    namespace: ballerina\n    metrics:\n    - response_time_seconds_value\n    - response_time_seconds_max\n    - response_time_seconds_min \n    - response_time_seconds_mean  \n    - response_time_seconds_stdDev\n    - response_time_seconds\n    - response_time_nanoseconds_total_value\n    - requests_total_value\n    - response_errors_total_value \n    - inprogress_requests_value\n    - kafka_publishers_value\n    - kafka_consumers_value\n    - kafka_errors_value  \n    headers:\n    Accept: \"text/plain; version=0.0.4\"\n```</code></pre> </li> <li> <p>Add configuration for tracing</p> <p>You need to use the following configurations in the <code>datadog.yaml</code>.</p> <p>To view traces in Datadog, we need to enable the APM (Application Performance Monitoring) in your Datadog agent.</p> <pre><code>apm_config:\nenabled: true\n</code></pre> <p>BI uses OpenTelemetry to provide traces. Therefore, we need to set up OpenTelemetry configurations as follows.</p> <pre><code>otlp_config:\nreceiver:\nprotocols:\ngrpc:\nendpoint: 0.0.0.0:4317\n</code></pre> </li> </ol>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/datadog/#step-3-import-prometheus-and-jaeger-extensions-for-bi","title":"Step 3 - Import Prometheus and Jaeger extensions for BI","text":"<p>Create the sample shop service. To include the Prometheus and Jaeger extensions into the executable, the <code>ballerinax/prometheus</code> and <code>ballerinax/jaeger</code> modules need to be imported into your BI project. Navigate to file explorer and add the following to the <code>main.bal</code> file.</p> <pre><code>import ballerinax/prometheus as _;\nimport ballerinax/jaeger as _;\n</code></pre> <p>To support Prometheus as the metrics reporter, an HTTP endpoint starts with the context of <code>/metrics</code> in default port 9797 when starting the Ballerina service.</p> <p>Jaeger extension has an <code>Opentelemetry GRPC Span Exporter</code> which will push tracing data as batches to the endpoint (default - http://localhost:4317) in opentelemetry format.</p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/datadog/#step-4-configure-bi-runtime-configurations","title":"Step 4 - Configure BI runtime configurations","text":"<p>Tracing and metrics can be enabled in your BI project using configurations similar to the following.  Navigate to file explorer and add the following to the <code>Config.toml</code> file.</p> <pre><code>[ballerina.observe]\ntracingEnabled=true\ntracingProvider=\"jaeger\"\nmetricsEnabled=true\nmetricsReporter=\"prometheus\"\n\n[ballerinax.prometheus]\nport=9797\nhost=\"0.0.0.0\"\n\n[ballerinax.jaeger]\nagentHostname=\"localhost\"\nagentPort=4317\nsamplerType=\"const\"\nsamplerParam=1.0\nreporterFlushInterval=2000\nreporterBufferSize=1000\n</code></pre> <p>The table below provides the descriptions of each configuration option and possible values that can be assigned.</p> Configuration key Description Default value Possible values ballerinax.prometheus. port The value of the port to which the '/metrics' service will bind. This service will be used by Prometheus to scrape the information of the Ballerina service. <code>9797</code> Any suitable value for port 0 - 0 - 65535. However, within that range, ports 0 - 1023 are generally reserved for specific purposes, therefore it is advisable to select a port without that range. ballerinax.prometheus. host The name of the host to which the '/metrics' service will bind. This service will be used by Prometheus to scrape the information of the Ballerina service. <code>0.0.0.0</code> IP or Hostname or 0.0.0.0 of the node in which the Ballerina service is running. ballerinax.jaeger. agentHostname Hostname of the Jaeger agent localhost IP or hostname of the Jaeger agent. If it is running on the same node as Ballerina, it can be localhost. ballerinax.jaeger. agentPort Port of the Jaeger agent 4317 The port on which the Jaeger agent is listening. ballerinax.jaeger. samplerType Type of the sampling methods used in the Jaeger tracer. const <code>const</code>, <code>probabilistic</code>, or <code>ratelimiting</code>. ballerinax.jaeger. samplerParam It is a floating value. Based on the sampler type, the effect of the sampler param varies 1.0 For <code>const</code> <code>0</code> (no sampling) or <code>1</code> (sample all spans), for <code>probabilistic</code> <code>0.0</code> to <code>1.0</code>, for <code>ratelimiting</code> any positive integer (rate per second). ballerinax.jaeger. reporterFlushInterval The Jaeger client will be sending the spans to the agent at this interval. 2000 Any positive integer value. ballerinax.jaeger. reporterBufferSize Queue size of the Jaeger client. 1000 Any positive integer value."},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/datadog/#step-5-run-the-service","title":"Step 5 - Run the service","text":"<p>When observability is enabled, the BI runtime collects tracing and metrics data and will be published to Datadog.</p> <p>Start the BI service using the <code>Run</code> option in the top right corner. You will see the following logs.</p> <pre><code>Compiling source\n\nRunning executable\n\nballerina: started Prometheus HTTP listener 0.0.0.0:9797\nballerina: started publishing traces to Jaeger on localhost:4317\n</code></pre>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/datadog/#step-6-send-requests","title":"Step 6 - Send requests","text":"<p>Send requests to http://localhost:8090/shop/products.</p> <p>Example cURL commands:</p> <p><pre><code>$ curl -X GET http://localhost:8090/shop/products\n</code></pre> <pre><code>$ curl -X POST http://localhost:8090/shop/product \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"id\": 4, \n    \"name\": \"Laptop Charger\", \n    \"price\": 50.00\n}'\n</code></pre> <pre><code>$ curl -X POST http://localhost:8090/shop/order \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"productId\": 1, \n    \"quantity\": 1\n}'\n</code></pre> <pre><code>$ curl -X GET http://localhost:8090/shop/order/0\n</code></pre></p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/datadog/#step-7-view-metrics-on-datadog","title":"Step 7 - View metrics on Datadog","text":"<p>You can observe the metrics in the Datadog platform under the <code>Metrics</code> tab in the left navigation.</p> <p></p> <p>You can add filters and use functions in the Datadog to visualize what you want with the metrics provided by BI.</p> <p>Ballerina provides a dashboard in the Datadog to observe metrics in Ballerina applications.</p> <p>You can add a new dashboard in the Datadog under the Dashboards tab in the left navigation. After creating the new dashboard, go to the Configure tab in the dashboard. Import the <code>dashboard.json</code> file provided above.</p> <p></p> <p>The Ballerina Dashboard in the Datadog will be displayed as below.</p> <p></p> <p></p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/datadog/#step-8-view-tracing-on-datadog","title":"Step 8 - View tracing on Datadog","text":"<p>To view traces of the BI application, go to APM \u2192 Traces in the Datadog.</p> <p></p> <p>You can filter the traces with the service name, resource, operation name, span kind, etc.</p> <p></p> <p>Once you select a trace, you can get more information with the tags attached to the span.</p> <p></p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/elastic-stack/","title":"Observe logs using Elastic Stack","text":"<p>In BI, distributed logging and analysis are supported by the Elastic Stack. BI has a log module for logging into the console. To monitor the logs, the BI standard output needs to be redirected to a file.</p> <p>The Elastic Stack comprises the following components.</p> <ol> <li>Beats - Multiple agents that ship data to Logstash or Elasticsearch. In our context, Filebeat will ship the BI logs to Logstash. Filebeat should be a container running on the same host as the BI service. This is so that the log file (ballerina.log) can be mounted to the Filebeat container.</li> <li>Logstash - Used to process and structure the log files received from Filebeat and send them to Elasticsearch.</li> <li>Elasticsearch - Storage and indexing of the logs sent by Logstash.</li> <li>Kibana - Visualizes the data stored in Elasticsearch.</li> </ol> <p>The sample shop service will be used in this guide. Follow the steps given below to observe BI logs in Elastic Stack.</p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/elastic-stack/#step-1-set-up-elastic-stack","title":"Step 1 - Set up Elastic Stack","text":"<p>Elasticsearch and Kibana are provided as Cloud services. Alternatively, Docker containers can be used to set up Elasticsearch and Kibana as well.</p> <ol> <li> <p>Download the Docker images using the following commands.</p> <pre><code># Elasticsearch Image\n$ docker pull docker.elastic.co/elasticsearch/elasticsearch:8.15.2\n# Kibana Image\n$ docker pull docker.elastic.co/kibana/kibana:8.15.2\n# Filebeat Image\n$ docker pull docker.elastic.co/beats/filebeat:8.15.2\n# Logstash Image\n$ docker pull docker.elastic.co/logstash/logstash:8.15.2\n</code></pre> </li> <li> <p>Start Elasticsearch and Kibana containers by executing the following commands.</p> <pre><code>$ docker run -p 9200:9200 -p 9300:9300 -it -h elasticsearch --name elasticsearch docker.elastic.co/elasticsearch/elasticsearch:8.15.2\n$ docker run -p 5601:5601 -h kibana --name kibana --link elasticsearch:elasticsearch docker.elastic.co/kibana/kibana:8.15.2\n</code></pre> <p>If you are using Linux, you may have to increase the <code>vm.max_map_count</code> for the Elasticsearch container to start.  Execute the following command to do that.</p> <pre><code>$ sudo sysctl -w vm.max_map_count=262144\n</code></pre> </li> <li> <p>Create a <code>logstash.conf</code> file in the <code>/tmp/pipeline/</code> directory and include the following content in the file.</p> <pre><code>input {\n  beats {\n    port =&gt; 5044\n    }\n}\nfilter {\n  grok  {\n    match =&gt; { \"message\" =&gt; \"%{TIMESTAMP_ISO8601:date}%{SPACE}%{WORD:logLevel}%{SPACE}\\[%{GREEDYDATA:module}\\]%{SPACE}\\-%{SPACE}%{GREEDYDATA:logMessage}\"}\n  }\n}\noutput {\n    elasticsearch {\n        hosts =&gt; \"elasticsearch:9200\"\n        index =&gt; \"ballerina\"\n      document_type =&gt; \"ballerina_logs\"\n    }\n}\n</code></pre> <p>Here, the 3 stages are specified in the pipeline. Input is specified as beats and listens to port 5044.  A Grok filter is used to structure the Ballerina logs and the output is specified to push to Elasticsearch on <code>elasticsearch:9200</code>.</p> </li> <li> <p>Start the Logstash container by executing the following command.</p> <pre><code>$ docker run -h logstash --name logstash --link elasticsearch:elasticsearch -it --rm -v /tmp/pipeline:/usr/share/logstash/pipeline/ -p 5044:5044 docker.elastic.co/logstash/logstash:8.15.2\n</code></pre> </li> <li> <p>Configure Filebeat to ship the Ballerina logs. Create a <code>filebeat.yml</code> file in the <code>/tmp/</code> directory and include the following content in the file.</p> <pre><code>filebeat.prospectors:\n- type: log\n  paths:\n    - /usr/share/filebeat/ballerina.log\noutput.logstash:\n  hosts: [\"logstash:5044\"]\n</code></pre> </li> <li> <p>Start the Filebeat container with the following command.</p> <pre><code>$ docker run -v /tmp/filebeat.yml:/usr/share/filebeat/filebeat.yml -v /&lt;path-to-ballerina.log&gt;/ballerina.log:/usr/share/filebeat/ballerina.log --link logstash:logstash docker.elastic.co/beats/filebeat:8.15.2\n</code></pre> <p>The <code>-v</code> flag is used for bind mounting, where the container will read the file from the host machine. Provide the path to the <code>ballerina.log</code> file to be bind-mounted to the filebeat container.</p> </li> </ol>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/elastic-stack/#step-2-run-ballerina-service","title":"Step 2 - Run Ballerina Service","text":"<p>Create the sample shop service and run it using the <code>Run</code> option in BI.</p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/elastic-stack/#step-3-send-requests","title":"Step 3 - Send requests","text":"<p>Send requests to <code>http://localhost:8090/shop/products</code>.</p> <p>Example cURL commands:</p> <p><pre><code>$ curl -X GET http://localhost:8090/shop/products\n</code></pre> <pre><code>$ curl -X POST http://localhost:8090/shop/product \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"id\": 4, \n    \"name\": \"Laptop Charger\", \n    \"price\": 50.00\n}'\n</code></pre> <pre><code>$ curl -X POST http://localhost:8090/shop/order \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"productId\": 1, \n    \"quantity\": 1\n}'\n</code></pre> <pre><code>$ curl -X GET http://localhost:8090/shop/order/0\n</code></pre></p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/elastic-stack/#step-4-view-logs-on-kibana","title":"Step 4 - View logs on Kibana","text":"<p>Access Kibana to visualize the logs at <code>http://localhost:5601</code>. Add an index named <code>ballerina</code> and click on <code>Discover</code> to visualize the logs.</p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/jaeger/","title":"Observe tracing using Jaeger","text":"<p>The sample shop service will be used in this guide. Follow the steps given below to observe tracing for BI application in Jaeger.</p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/jaeger/#step-1-set-up-jaeger","title":"Step 1 - Set up Jaeger","text":"<p>You can configure BI project to support distributed tracing with Jaeger. This section focuses on configuring Jaeger with Docker as a quick installation.</p> Tip<p>There are many possible ways to deploy Jaeger. For more information, see Jaeger Deployment. The easiest option is to use executable binaries listed in Downloads.</p> <p>Install Jaeger via Docker and start the Docker container by executing the command below.</p> <pre><code>$ docker run -d -p 13133:13133 -p 16686:16686 -p 4317:4317 jaegertracing/opentelemetry-all-in-one\n</code></pre>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/jaeger/#step-2-import-ballerina-jaeger-extension","title":"Step 2 - Import Ballerina Jaeger extension","text":"<p>Create the sample shop service. To include the Jaeger extension into the executable, the <code>ballerinax/jaeger</code> module needs to be imported into your Ballerina project <code>main.bal</code> file.</p> <pre><code>import ballerinax/jaeger as _;\n</code></pre> <p>Jaeger extension has an <code>Opentelemetry GRPC Span Exporter</code> which will push tracing data as batches to the Jaeger server endpoint (default - http://localhost:4317) in opentelemetry format.</p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/jaeger/#step-3-configure-ballerina-runtime-configurations","title":"Step 3 - Configure Ballerina runtime configurations","text":"<p>Tracing can be enabled in your Ballerina project using configurations similar to the following in your <code>Config.toml</code> file.</p> <pre><code>[ballerina.observe]\ntracingEnabled=true\ntracingProvider=\"jaeger\"\n\n[ballerinax.jaeger]\nagentHostname=\"localhost\"\nagentPort=4317\nsamplerType=\"const\"\nsamplerParam=1.0\nreporterFlushInterval=2000\nreporterBufferSize=1000\n</code></pre> <p>The table below provides the descriptions of each configuration option and possible values that can be assigned.</p> Configuration key Description Default value Possible values ballerinax.jaeger. agentHostname Hostname of the Jaeger agent localhost IP or hostname of the Jaeger agent. If it is running on the same node as BI, it can be localhost. ballerinax.jaeger. agentPort Port of the Jaeger agent 4317 The port on which the Jaeger agent is listening. ballerinax.jaeger. samplerType Type of the sampling methods used in the Jaeger tracer. const <code>const</code>, <code>probabilistic</code>, or <code>ratelimiting</code>. ballerinax.jaeger. samplerParam It is a floating value. Based on the sampler type, the effect of the sampler param varies 1.0 For <code>const</code> <code>0</code> (no sampling) or <code>1</code> (sample all spans), for <code>probabilistic</code> <code>0.0</code> to <code>1.0</code>, for <code>ratelimiting</code> any positive integer (rate per second). ballerinax.jaeger. reporterFlushInterval The Jaeger client will be sending the spans to the agent at this interval. 2000 Any positive integer value. ballerinax.jaeger. reporterBufferSize Queue size of the Jaeger client. 1000 Any positive integer value."},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/jaeger/#step-4-run-the-bi-service","title":"Step 4 - Run the BI service","text":"<p>When observability is enabled, the BI runtime collects tracing data and traces will be published to Jaeger.</p> <p>Run the BI service and you will see an output as follows.</p> <pre><code>Compiling source\n\nRunning executable\n\nballerina: started publishing traces to Jaeger on localhost:4317\n</code></pre>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/jaeger/#step-5-send-requests","title":"Step 5 - Send requests","text":"<p>Send requests to http://localhost:8090/shop/products.</p> <p>Example cURL commands:</p> <p><pre><code>$ curl -X GET http://localhost:8090/shop/products\n</code></pre> <pre><code>$ curl -X POST http://localhost:8090/shop/product \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"id\": 4, \n    \"name\": \"Laptop Charger\", \n    \"price\": 50.00\n}'\n</code></pre> <pre><code>$ curl -X POST http://localhost:8090/shop/order \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"productId\": 1, \n    \"quantity\": 1\n}'\n</code></pre> <pre><code>$ curl -X GET http://localhost:8090/shop/order/0\n</code></pre></p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/jaeger/#step-6-view-distributed-tracing-on-the-jaeger-server","title":"Step 6 - View distributed tracing on the Jaeger server","text":"<p>Go to http://localhost:16686 and load the web UI of Jaeger to make sure it is functioning properly. You can select the service for which you need tracing information find traces.</p> <p>The image below is the sample tracing information you can see in Jaeger.</p> <p></p> <p></p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/moesif/","title":"Observe metrics, traces and logs using Moesif","text":"<p>The sample shop service will be used in this guide.</p> <p>Follow the steps given below to view BI metrics, traces and logs in Moesif.</p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/moesif/#step-1-create-a-moesif-account-and-get-an-application-id","title":"Step 1 - Create a Moesif account and get an application ID","text":"<p>After you log into Moesif Portal, get your <code>Moesif Application ID</code> during the onboarding steps. <code>Application ID</code> can be accessed by following the below steps from Moesif Portal after logging in.</p> <p>Go to Account -&gt; Settings -&gt; API keys -&gt; Collector Application ID.</p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/moesif/#step-2-set-up-bi-application-for-publishing-traces-and-metrics-to-moesif","title":"Step 2 - Set up BI application for publishing traces and metrics to Moesif","text":"<ul> <li>Create the sample shop service.</li> <li>Navigate to file explorer view and add the following to the <code>main.bal</code> file.</li> </ul> <pre><code>import ballerinax/moesif as _;\n</code></pre> <ul> <li>Navigate to file explorer view and create the <code>Config.toml</code> file in the package directory to set the runtime configurations as follows.</li> </ul> <pre><code>[ballerina.observe]\ntracingEnabled=true\ntracingProvider=\"moesif\"\n\n[ballerinax.moesif]\napplicationId = \"&lt;MOESIF_APPLICATION_ID&gt;\"    # Mandatory Configuration.\nreporterBaseUrl = \"https://api.moesif.net\"   # Optional Configuration. Default value is 'https://api.moesif.net'\ntracingReporterFlushInterval = 1000          # Optional Configuration. Default value is 1000\ntracingReporterBufferSize = 10000            # Optional Configuration. Default value is 10000\nisTraceLoggingEnabled = false                # Optional Configuration. Default value is false\nisPayloadLoggingEnabled = false              # Optional Configuration. Default value is false\n</code></pre> <ul> <li>To enable metrics publishing, add the following to the <code>Config.toml</code>.</li> </ul> <p><pre><code>[ballerina.observe]\nmetricsEnabled=true\nmetricsReporter=\"moesif\"\n\n[ballerinax.moesif]\napplicationId = \"&lt;MOESIF_APPLICATION_ID&gt;\"     # Mandatory Configuration.\nreporterBaseUrl = \"https://api.moesif.net\"   # Optional Configuration. Default value is 'https://api.moesif.net'\nmetricsReporterFlushInterval = 15000         # Optional Configuration. Default value is 15000\nmetricsReporterClientTimeout = 10000         # Optional Configuration. Default value is 10000\nisTraceLoggingEnabled = false                # Optional Configuration. Default value is false\nisPayloadLoggingEnabled = false              # Optional Configuration. Default value is false\n\n# Additional attributes for metrics\n[ballerinax.moesif.additionalAttributes]\nkey1 = \"value1\"\nkey2 = \"value2\"\n</code></pre> * Replace <code>&lt;MOESIF_APPLICATION_ID&gt;</code> with the application ID obtained in Step 1.</p> <p>The table below provides the descriptions of each configuration option and possible values that can be assigned.</p> Configuration key Description Default value Possible values ballerina.observe.tracingEnabled Enables or disables the collection of trace data. <code>false</code> <code>true</code> or <code>false</code> ballerina.observe.tracingProvider Specifies Moesif as the tracing provider. <code>none</code> <code>\"moesif\"</code> ballerina.observe.metricsEnabled Enables or disables the collection of metrics data. <code>false</code> <code>true</code> or <code>false</code> ballerina.observe.metricsReporter Specifies Moesif as the metrics reporter. <code>none</code> <code>\"moesif\"</code> ballerinax.moesif.applicationId Moesif application ID used for authentication. Mandatory configuration. <code>none</code> A valid Moesif application ID string ballerinax.moesif.reporterBaseUrl The base URL of the Moesif API. <code>https://api.moesif.net</code> Any valid Moesif API endpoint URL ballerinax.moesif.tracingReporterFlushInterval Interval (in milliseconds) for flushing trace data to Moesif. <code>1000</code> Any positive integer value ballerinax.moesif.tracingReporterBufferSize Maximum buffer size for trace data before sending to Moesif. <code>10000</code> Any positive integer value ballerinax.moesif.metricsReporterFlushInterval Interval (in milliseconds) for flushing metrics data to Moesif. <code>15000</code> Any positive integer value ballerinax.moesif.metricsReporterClientTimeout Timeout (in milliseconds) for the metrics reporter client requests. <code>10000</code> Any positive integer value ballerinax.moesif.isTraceLoggingEnabled Enables or disables trace logging for debugging purposes. <code>false</code> <code>true</code> or <code>false</code> ballerinax.moesif.isPayloadLoggingEnabled Enables or disables payload logging for debugging purposes. <code>false</code> <code>true</code> or <code>false</code> ballerinax.moesif.additionalAttributes Additional key-value attributes to include with metrics reporting. <code>none</code> Any valid set of key-value pairs.e.g., <code>key1=\"value1\", key2=\"value2\"</code> <p>These configurations enable traces and metrics publishing for the BI application and configure the Moesif exporter.</p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/moesif/#step-3-publish-bi-application-logs-to-moesif","title":"Step 3 - Publish BI application logs to Moesif","text":"<p>This setup leverages <code>Fluent Bit</code> to forward logs to an <code>OTEL Collector</code>, which then sends the logs to Moesif\u2019s log endpoint.</p> <p>BI \u2192 Fluent Bit \u2192 OTEL Collector \u2192 Moesif</p> <ul> <li>Copy the following configs into a local directory to set up containerized log publishing.</li> </ul> <pre><code>    .\n    |____ docker-compose.yaml\n    |\n    |____ fluent-bit.conf\n    |\n    |____ otelcol.yaml\n</code></pre> <p>docker-compose.yaml \u2013 Container setup for Fluent Bit and OTEL Collector.   fluent-bit.conf \u2013 Reads BI logs and forwards them.   otelcol.yaml \u2013 Processes logs and sends to Moesif.</p> <p>docker-compose.yaml</p> <p>Update the <code>&lt;ballerina-log-path&gt;</code> with the log storage location, and <code>&lt;MOESIF_APPLICATION_ID&gt;</code> with the    application ID obtained in Step 1.</p> <pre><code>services:\notelcol:\nimage: otel/opentelemetry-collector-contrib:0.132.0\ncontainer_name: moesif-otel-collector\ncommand: [\"--config\", \"/etc/otelcol.yaml\"]\nenvironment:\nMOESIF_APP_ID: \"&lt;MOESIF_APPLICATION_ID&gt;\"\nports:\n- \"4317:4317\"\n- \"4318:4318\"\nvolumes:\n- ./otelcol.yaml:/etc/otelcol.yaml:ro\nnetworks:\n- otelnet\n\nfluent-bit:\nimage: fluent/fluent-bit:3.0\ncontainer_name: fluent-bit\ndepends_on:\n- otelcol\nports:\n- \"2020:2020\"\nvolumes:\n- ./fluent-bit.conf:/fluent-bit/etc/fluent-bit.conf:ro\n# Mount the local log directory into the container\n- &lt;ballerina-log-path&gt;:/app/logs:ro\nnetworks:\n- otelnet\n\nnetworks:\notelnet:\ndriver: bridge\n</code></pre> <p>fluent-bit.conf</p> <pre><code>[SERVICE]\n    Flush         1\n    Log_Level     debug\n    Daemon        off\n    HTTP_Server   On\n    HTTP_Listen   0.0.0.0\n    HTTP_Port     2020\n\n# Read logs from local BI app\n[INPUT]\n    Name              tail\n    Path              /app/logs/app.log\n    Tag               bi.*\n    Read_from_Head    true\n    Skip_Long_Lines   On\n    Skip_Empty_Lines  On\n    Refresh_Interval  1\n\n# Add metadata\n[FILTER]\n    Name         modify\n    Match        bi.*\n    Add          service.name ballerina-service\n    Add          deployment.environment prod\n\n# Convert to OTEL format and send to collector\n[OUTPUT]\n    Name          opentelemetry\n    Match         bi.*\n    Host          otelcol\n    Port          4318\n    Logs_uri      /v1/logs\n    Log_response_payload True\n    Tls           Off\n    Tls.verify    Off\n\n# Debug output to see what's being processed\n[OUTPUT]\n    Name          stdout\n    Match         bi.*\n    Format        json_lines\n</code></pre> <p>otelcol.yaml</p> <p>Update the <code>&lt;MOESIF_APPLICATION_ID&gt;</code> with the application ID obtained in Step 1.</p> <pre><code>receivers:\notlp:\nprotocols:\ngrpc:\nendpoint: \"0.0.0.0:4317\"\nhttp:\nendpoint: \"0.0.0.0:4318\"\n\nprocessors:\nresource:\nattributes:\n- key: service.name\nvalue: ballerina-service\naction: upsert\n- key: deployment.environment\nvalue: prod\naction: upsert\n\ntransform/severity_from_message:\nlog_statements:\n- context: log\nstatements:\n# Set default severity to INFO for all logs first\n- set(severity_number, 9) where body != nil\n- set(severity_text, \"INFO\") where body != nil\n\n# Try to parse JSON body, but handle parsing errors gracefully\n- set(cache[\"is_json\"], false)\n- set(cache[\"is_json\"], true) where body != nil and IsMatch(body, \"^\\\\s*\\\\{\")\n\n# For JSON logs, parse and extract level\n- set(cache[\"parsed_body\"], ParseJSON(body)) where cache[\"is_json\"] == true\n\n# Override with specific levels based on JSON level field\n- set(severity_number, 1) where cache[\"is_json\"] == true and cache[\"parsed_body\"][\"level\"] == \"TRACE\"\n- set(severity_text, \"TRACE\") where cache[\"is_json\"] == true and cache[\"parsed_body\"][\"level\"] == \"TRACE\"\n- set(severity_number, 5) where cache[\"is_json\"] == true and cache[\"parsed_body\"][\"level\"] == \"DEBUG\"\n- set(severity_text, \"DEBUG\") where cache[\"is_json\"] == true and cache[\"parsed_body\"][\"level\"] == \"DEBUG\"\n- set(severity_number, 9) where cache[\"is_json\"] == true and cache[\"parsed_body\"][\"level\"] == \"INFO\"\n- set(severity_text, \"INFO\") where cache[\"is_json\"] == true and cache[\"parsed_body\"][\"level\"] == \"INFO\"\n- set(severity_number, 13) where cache[\"is_json\"] == true and cache[\"parsed_body\"][\"level\"] == \"WARN\"\n- set(severity_text, \"WARN\") where cache[\"is_json\"] == true and cache[\"parsed_body\"][\"level\"] == \"WARN\"\n- set(severity_number, 17) where cache[\"is_json\"] == true and cache[\"parsed_body\"][\"level\"] == \"ERROR\"\n- set(severity_text, \"ERROR\") where cache[\"is_json\"] == true and cache[\"parsed_body\"][\"level\"] == \"ERROR\"\n- set(severity_number, 21) where cache[\"is_json\"] == true and cache[\"parsed_body\"][\"level\"] == \"FATAL\"\n- set(severity_text, \"FATAL\") where cache[\"is_json\"] == true and cache[\"parsed_body\"][\"level\"] == \"FATAL\"\n\nbatch: {}\n\nexporters:\n# OTLP over HTTP to Moesif\notlphttp:\nendpoint: \"https://api.moesif.net\"\nlogs_endpoint: \"https://api.moesif.net/v1/logs\"\nheaders:\nX-Moesif-Application-Id: \"&lt;MOESIF_APPLICATION_ID&gt;\"\ncompression: none\ntimeout: 10s\nsending_queue:\nenabled: true\nnum_consumers: 2\nqueue_size: 512\nretry_on_failure:\nenabled: true\ninitial_interval: 1s\nmax_interval: 10s\nmax_elapsed_time: 0s\n\nservice:\ntelemetry:\nlogs:\nlevel: debug\npipelines:\nlogs:\nreceivers:  [otlp]\nprocessors: [resource, transform/severity_from_message, batch]\nexporters:  [otlphttp]\n</code></pre> <ul> <li> <p>Run the above components stack using the following command.    <code>docker compose up</code></p> </li> <li> <p>Navigate to file explorer view and Create the <code>Config.toml</code> file in the package directory with the following content to log to a file in <code>json</code> format.</p> </li> </ul> <pre><code>[ballerina.log]\nformat = \"json\"\n\n[[ballerina.log.destinations]]\n# Replace /path/to/your/bi/logs with the absolute path to the BI application's log directory\npath = \"/path/to/your/bi/logs/app.log\"\n</code></pre>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/moesif/#step-4-run-the-bi-project","title":"Step 4 - Run the BI project","text":"<p>When observability is enabled, the BI runtime collects metrics, logs and traces.</p> <p>Start the service in BI.</p> <pre><code>$ bal run\n\nCompiling source\n\nRunning executable\n\nballerina: started publishing traces to Moesif HTTP endpoint at https://api.moesif.net/v1/traces\nballerina: started publishing metrics to Moesif endpoint: https://api.moesif.net/v1/actions/batch with 2 additional attributes\n</code></pre>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/moesif/#step-5-send-requests","title":"Step 5 - Send requests","text":"<p>Send requests to http://localhost:8090/shop.</p> <p>Example cURL commands:</p> <p><pre><code>$ curl -X GET http://localhost:8090/shop/products\n</code></pre> <pre><code>$ curl -X POST http://localhost:8090/shop/product \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"id\": 4, \n    \"name\": \"Laptop Charger\", \n    \"price\": 50.00\n}'\n</code></pre> <pre><code>$ curl -X POST http://localhost:8090/shop/order \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"productId\": 1, \n    \"quantity\": 1\n}'\n</code></pre> <pre><code>$ curl -X GET http://localhost:8090/shop/order/0\n</code></pre></p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/moesif/#step-6-visualize-the-observability-data-in-moesif-dashboards","title":"Step 6 - Visualize the observability data in Moesif dashboards","text":"<p>Traces, metrics, and logs are published to Moesif as events and can be explored in the Live Event Log for real-time monitoring. Moesif provides a set of pre-built dashboards that help visualize and analyze this data effectively. In addition, custom dashboards can be created to gain deeper, domain-specific insights.</p> <p>The following sample dashboards illustrate how different types of observability data can be monitored and analyzed.</p> <ul> <li>Traces Dashboard</li> </ul> <p>Used to filter and view incoming requests. You can drill down into each request to track its related traces in detail.</p> <p></p> <ul> <li>Metrics Dashboard</li> </ul> <p>Provides visibility into key performance indicators such as latency, throughput, and error rates.</p> <p></p> <ul> <li>Logs Dashboard</li> </ul> <p>Displays log events captured from the application.</p> <p></p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/new-relic/","title":"Observe metrics and tracing using New Relic","text":"<p>The sample shop service will be used in this guide. Follow the steps given below to observe BI tracing and metrics in New Relic.</p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/new-relic/#step-1-create-a-new-relic-account-and-an-api-key","title":"Step 1 - Create a New Relic account and  an API key","text":"<p>Sign up and Generate an API Key in New Relic.</p> <p>To configure the API key in Newrelic:</p> <p>Go to Profile -&gt; API keys -&gt; Insights Insert key -&gt; Insert keys to create an account in New Relic.</p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/new-relic/#step-2-import-the-new-relic-extension","title":"Step 2 - Import the New Relic extension","text":"<p>Create the sample shop service. To include the New Relic extension into the executable, the <code>ballerinax/newrelic</code> module needs to be imported into your BI project. Navigate to file explorer and add the following to <code>main.bal</code> file.</p> <pre><code>import ballerinax/newrelic as _;\n</code></pre> <p>New Relic extension has an <code>Opentelemetry GRPC Span Exporter</code> which will push tracing data as batches to the New Relic server endpoint (https://otlp.nr-data.net:4317) in opentelemetry format.</p> <p>New Relic extension pushes metrics in New Relic metric format to the New Relic server endpoint (https://metric-api.newrelic.com/metric/v1).</p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/new-relic/#step-3-configure-runtime-configurations-for-observability","title":"Step 3 - Configure runtime configurations for Observability","text":"<p>Tracing and metrics can be enabled in your BI project using configurations similar to the following. Navigate to file explorer and add the following to <code>Config.toml</code> file.</p> <pre><code>[ballerina.observe]\ntracingEnabled=true\ntracingProvider=\"newrelic\"\nmetricsEnabled=true\nmetricsReporter=\"newrelic\"\n\n[ballerinax.newrelic]\napiKey=\"&lt;NEW_RELIC_LICENSE_KEY&gt;\"    tracingSamplerType=\"const\"          tracingSamplerParam=1               tracingReporterFlushInterval=15000  tracingReporterBufferSize=10000     metricReporterFlushInterval=15000   metricReporterClientTimeout=10000\n</code></pre> <p>The table below provides the descriptions of each configuration option and possible values that can be assigned.</p> Configuration key Description Default value Possible values ballerinax.newrelic. apiKey API key generated by the user in the New Relic platform. This configuration is mandatory. <code>None</code> ballerinax.newrelic. tracingSamplerType Type of the sampling methods used in the New Relic tracer. const <code>const</code>, <code>probabilistic</code>, or <code>ratelimiting</code>. ballerinax.newrelic. tracingSamplerParam It is a floating value. Based on the sampler type, the effect of the sampler param varies 1.0 For <code>const</code> <code>0</code> (no sampling) or <code>1</code> (sample all spans), for <code>probabilistic</code> <code>0.0</code> to <code>1.0</code>, for <code>ratelimiting</code> any positive integer (rate per second). ballerinax.newrelic. tracingReporterFlushInterval The New Relic tracing client will be sending the spans to the agent at this interval. 15000 Any positive integer value. ballerinax.newrelic. tracingReporterBufferSize Queue size of the New Relic tracing client. 10000 Any positive integer value. ballerinax.newrelic. metricReporterFlushInterval The New Relic client will be sending the metrics to the agent at this interval. 15000 Any positive integer value. ballerinax.newrelic. metricReporterClientTimeout Queue size of the New Relic metric client. 10000 Any positive integer value."},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/new-relic/#step-4-run-the-bi-service","title":"Step 4 - Run the BI service","text":"<p>When observability is enabled, the BI runtime collects tracing and metrics data and both metrics and traces will be published to New Relic.</p> <p>Start the service.</p> <pre><code>Compiling source\n\nRunning executable\n\nballerina: started publishing traces to New Relic on https://otlp.nr-data.net:4317\nballerina: started publishing metrics to New Relic on https://metric-api.newrelic.com/metric/v1\n</code></pre>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/new-relic/#step-5-send-requests","title":"Step 5 - Send requests","text":"<p>Send requests to http://localhost:8090/shop/products.</p> <p>Example cURL commands:</p> <p><pre><code>$ curl -X GET http://localhost:8090/shop/products\n</code></pre> <pre><code>$ curl -X POST http://localhost:8090/shop/product \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"id\": 4, \n    \"name\": \"Laptop Charger\", \n    \"price\": 50.00\n}'\n</code></pre> <pre><code>$ curl -X POST http://localhost:8090/shop/order \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"productId\": 1, \n    \"quantity\": 1\n}'\n</code></pre> <pre><code>$ curl -X GET http://localhost:8090/shop/order/0\n</code></pre></p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/new-relic/#step-6-view-metrics-on-the-new-relic-platform","title":"Step 6 - View metrics on the New Relic platform","text":"<p>You can view the metrics that were published to the New Relic platform in the New Relic query builder. You can view the metrics query data in graphical format, as shown below.</p> <p></p> <p>You can create a dashboard from the metrics provided by BI in the New Relic platform.</p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/new-relic/#step-7-view-tracing-on-the-new-relic-platform","title":"Step 7 - View tracing on the New Relic platform","text":"<p>You can view the traces that were published to the New Relic platform in New Relic traces. </p> <p></p> <p></p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/opensearch/","title":"Observe metrics, traces, and logs using OpenSearch","text":"<p>The sample shop service will be used in this guide. Follow the steps given below to observe BI tracing, metrics, and logs in OpenSearch.</p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/opensearch/#step-1-set-up-opensearch-deployment","title":"Step 1 - Set up OpenSearch Deployment","text":"<ul> <li> <p>Main components:</p> <ul> <li>Sample Service: service application that is being observed.</li> <li>OpenSearch: stores and indexes logs/traces; exposes API and metrics ports.</li> <li>OpenSearch Dashboards: visualizes data from OpenSearch via web UI.</li> <li>Data Prepper: receives and processes OpenTelemetry data, sends it to OpenSearch.</li> <li>Fluent Bit: collects app logs and forwards them for indexing.</li> <li>Setup Container: automates initial setup \u2014 creates index templates and imports dashboards.</li> </ul> </li> <li> <p>Download and unzip the opensearch-observability-dashboard.zip in your local machine.</p> </li> <li> <p>The structure of the <code>opensearch-observability-dashboard</code> directory is as follows.</p> </li> </ul> <pre><code>    .\n    |____config\n    |       |____dashboards\n    |       |       |____opensearch_dashboards.yml\n    |       |____data-prepper\n    |       |     |____pipelines.yaml\n    |       |____fluent-bit\n    |       |      |____fluent-bit.conf\n    |       |      |____parser.conf\n    |       |      |____scripts\n    |       |            |____scripts.lua\n    |       |____.env\n    |\n    |____logs\n    |         |____ballerina\n    |\n    |____setup\n    |       |____opensearch-dashboards-template.ndjson\n    |       |____index-template-request.json\n    |\n    |____docker-compose.yml\n</code></pre> <ul> <li>Update OPENSEARCH_INITIAL_ADMIN_PASSWORD in the <code>path/to/opensearch-observability-dashboard/config/.env</code> file.</li> </ul> <pre><code>OPENSEARCH_INITIAL_ADMIN_PASSWORD=&lt;PASSWORD&gt; # Password for the OpenSearch admin user\n</code></pre> <p>This password will be used to access the OpenSearch server.</p> <ul> <li>Navigate to the <code>path/to/opensearch-observability-dashboard</code> directory and run the following <code>docker compose</code> command in the terminal to start the OpenSearch deployment along with the BI application.</li> </ul> <pre><code>docker compose -f docker-compose.yml up -d\n</code></pre>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/opensearch/#step-2-set-up-bi-application-for-observability","title":"Step 2 - Set up BI application for observability","text":"<ul> <li> <p>Create the sample shop service.</p> </li> <li> <p>Navigate to file explorer view and add the following to <code>main.bal</code>.</p> <pre><code>import ballerinax/metrics.logs as _;\nimport ballerinax/jaeger as _;\n</code></pre> </li> <li> <p>Navigate to file explorer view and create the <code>Config.toml</code> file in the package directory to set the runtime configurations as follows.</p> <pre><code>[ballerina.observe]\nmetricsLogsEnabled = true\ntracingEnabled = true\ntracingProvider = \"jaeger\"\n\n[ballerinax.jaeger]\nagentHostname = \"localhost\"\nagentPort = 4317\nsamplerType = \"const\"\nsamplerParam = 1.0\nreporterFlushInterval = 2000\nreporterBufferSize = 1000\n\n[ballerinax.metrics.logs]\nlogFilePath = \"&lt;PATH&gt;/&lt;TO&gt;/opensearch-observability-dashboard/logs/ballerina/&lt;SERVICE_NAME&gt;/app.log\"\n</code></pre> </li> </ul> <p>These configurations enable metrics logs and traces in the BI application and configures the Jaeger exporter.</p> <p>The table below provides the descriptions of each configuration option and possible values that can be assigned.</p> Configuration key Description Default value Possible values ballerinax.jaeger. agentHostname Hostname of the Jaeger agent localhost IP or hostname of the Jaeger agent. Can be localhost if running on same node as Ballerina. ballerinax.jaeger. agentPort Port of the Jaeger agent 4317 The port on which the Jaeger agent is listening. ballerinax.jaeger. samplerType Type of sampling methods used in Jaeger tracer const <code>const</code>, <code>probabilistic</code>, or <code>ratelimiting</code> ballerinax.jaeger. samplerParam Floating value parameter for sampler 1.0 const: <code>0</code> (no sampling) or <code>1</code> (sample all)probabilistic: <code>0.0</code> to <code>1.0</code>ratelimiting: positive integer (rate/sec) ballerinax.jaeger. reporterFlushInterval Interval for sending spans to agent 2000 Any positive integer value ballerinax.jaeger. reporterBufferSize Queue size of Jaeger client 1000 Any positive integer value ballerinax.metrics.logs. logFilePath Path to application log file <code>none</code> <code>PATH/TO/opensearch-observability-dashboard/logs/ballerina/&lt;SERVICE_NAME&gt;/app.log</code>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/opensearch/#step-3-run-the-bi-project","title":"Step 3 - Run the BI project","text":"<p>When observability is enabled, the BI runtime collects metrics logs and traces.</p> <p>Start the service in BI.</p> <pre><code>$ bal run\n\nCompiling source\n\nRunning executable\n\nballerina: started publishing traces to Jaeger on localhost:4317\n</code></pre>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/opensearch/#step-4-send-requests","title":"Step 4 - Send requests","text":"<p>Send requests to http://localhost:8090/shop/products.</p> <p>Example cURL commands:</p> <p><pre><code>$ curl -X GET http://localhost:8090/shop/products\n</code></pre> <pre><code>$ curl -X POST http://localhost:8090/shop/product \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"id\": 4, \n    \"name\": \"Laptop Charger\", \n    \"price\": 50.00\n}'\n</code></pre> <pre><code>$ curl -X POST http://localhost:8090/shop/order \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"productId\": 1, \n    \"quantity\": 1\n}'\n</code></pre> <pre><code>$ curl -X GET http://localhost:8090/shop/order/0\n</code></pre></p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/opensearch/#step-5-view-distributed-tracing-on-the-opensearch-dashboard","title":"Step 5 - View distributed tracing on the OpenSearch Dashboard","text":"<p>Open the OpenSearch Dashboard in your browser at http://localhost:5601 and navigate to the <code>Traces</code> tab within the <code>Observability</code> section.</p> <ul> <li> <p>The following image shows a sample of the tracing information available in OpenSearch.</p> <p></p> </li> <li> <p>The following image shows the span details in OpenSearch.</p> <p></p> </li> <li> <p>The service map shows the relationship between different services in the system.</p> <p></p> </li> <li> <p>The following image shows the service details.</p> <p></p> </li> </ul>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/opensearch/#step-6-view-metrics-on-opensearch-dashboard","title":"Step 6 - View metrics on OpenSearch Dashboard","text":"<p>Open the OpenSearch Dashboard in your browser at <code>http://localhost:5601</code> and navigate to the Dashboards tab under OpenSearch Dashboards section.</p> <p>Then click on the Integration metrics dashboard to view the metrics.</p> <p></p> <p></p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/opensearch/#step-7-view-logs-on-opensearch-dashboard","title":"Step 7 - View logs on OpenSearch Dashboard","text":"<p>Open the OpenSearch Dashboard in your browser at http://localhost:5601 and navigate to the Dashboards tab under OpenSearch Dashboards section.</p> <p>Then click on the Integration logs dashboard to view the integration logs.</p> <p></p> <p></p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/overview/","title":"Supported Observability Tools and Platforms","text":"<p>Observability is a measure of how well the internal states of a system can be understood from its external outputs.</p> <p>In BI, observability is a core feature that helps monitor, debug, and optimize integration services. It focuses on the following three key pillars:</p> <p>Metrics \u2013 Numeric data collected and aggregated over time to monitor system performance.</p> <p>Tracing \u2013 Tracking the flow of requests or messages through various services and components, from entry to exit.</p> <p>Logging \u2013 Text-based records of application behavior, annotated with timestamps and contextual information.</p> <p>Observability platforms allow developers and operators to gain insight into system behavior, troubleshoot issues, and ensure reliability in production deployments.</p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/overview/#observability-in-bi","title":"Observability in BI","text":"<p>BI provides built-in support for observability across its runtime. Integration services, APIs, and connectors emit rich telemetry data that can be exported to standard monitoring tools.</p> <p>This guide explains how to enable and configure observability in BI using a simplified integration example. This integration sample is used to demonstrate the supported observability and monitoring tools in the next sections.</p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/overview/#example-observing-a-sample-integration-service","title":"Example: Observing a Sample Integration Service","text":""},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/overview/#create-a-sample-service","title":"Create a sample service","text":"<p>Let\u2019s consider an example where an integration service handles product management and ordering. The goal is to observe how it behaves under real-world usage.</p> <ol> <li>Create a new integration on BI</li> <li> <p>Define types to hold the <code>Product</code>, <code>Order</code> and <code>OrderRequest</code></p> <p>You can do this by navigating to the <code>types.bal</code> from the file explorer view and copying the following content.</p> <pre><code>type Product record {|\n    int id;\n    string name;\n    float price;\n|};\n\ntype OrderRequest record {|\n    int productId;\n    int quantity;\n|};\n\ntype Order record {|\n    int orderId;\n    int productId;\n    int quantity;\n    float totalPrice;\n|};\n</code></pre> </li> <li> <p>Create an HTTP service with base path <code>/shop</code> that has the following resources.</p> <ul> <li>List available products <code>get products()</code></li> <li>Add a new product <code>post product(Product product)</code></li> <li>Place a new order <code>'order(OrderRequest orderRequest)</code></li> <li>Get order details by ID <code>'order/[int orderId]()</code></li> </ul> <p>You can add the service related logic by navigating to the <code>main.bal</code> from the file explorer view and copying the following content. </p> <pre><code>import ballerina/http;\nimport ballerina/log;\n\n// Sample data\nmap&lt;Product&gt; products = {\n    \"1\": {id: 1, name: \"Laptop\", price: 1200.00},\n    \"2\": {id: 2, name: \"Smartphone\", price: 800.00},\n    \"3\": {id: 3, name: \"Headphones\", price: 150.00}\n};\n\nmap&lt;Order&gt; orders = {};\nint orderCount = 0;\n\n@display {\n    label: \"Shopping Service\"\n}\nservice /shop on new http:Listener(8090) {\n\n    // List available products.\n    resource function get products() returns Product[] {\n        log:printInfo(\"Fetching product list\");\n        return products.toArray();\n    }\n\n    // Add a new product.\n    resource function post product(Product product) returns http:Created|http:Conflict|error? {\n        log:printInfo(\"Adding a new product\");\n\n        if products.hasKey(product.id.toString()) {\n            log:printError(\"Product already exists with product ID\", id =product.id);\n            http:Conflict errorResponse = {\n                body:  string `Product already exists with product ID: ${product.id}`\n            };\n            return errorResponse;\n        }\n\n        products[product.id.toString()] = product;\n        log:printInfo(\"Product added successfully.\", product = product);\n        http:Created response = {\n            body: string `Product added successfully with product ID: ${product.id}`\n        };\n        return response;   \n    }\n\n    // Place a new order.\n    resource function post 'order(OrderRequest orderRequest) returns http:Accepted|http:NotFound|error? {\n        log:printInfo(\"Received order request\");\n\n        if !products.hasKey(orderRequest.productId.toString()) {\n            log:printError(\"Product not found with product ID\", id = orderRequest.productId);\n            http:NotFound errorResponse = {\n                body:  string `Product not found with product ID: ${orderRequest.productId.toString()}`\n            };\n            return errorResponse;\n        }\n        Product product = products.get(orderRequest.productId.toString());\n        Order newOrder = {orderId: orderCount, productId: orderRequest.productId, quantity: orderRequest.quantity, totalPrice: product.price * orderRequest.quantity};\n        orders[orderCount.toString()] = newOrder;\n        orderCount += 1;\n\n        log:printInfo(\"Order placed successfully.\", 'order = newOrder);\n        http:Accepted response = {\n            body:  newOrder\n        };\n        return response;\n    }\n\n    // Get order details by ID.\n    resource function get 'order/[int orderId]() returns http:Ok|http:NotFound|error? {\n        log:printInfo(\"Fetching order details\");\n\n        if !orders.hasKey(orderId.toString()) {\n            log:printError(\"Order not found with order ID\", id = orderId);\n            http:NotFound errorResponse = {\n                body: string `Order not found with order ID: ${orderId}`\n            };\n            return errorResponse;\n        }\n\n        Order 'order =  orders.get(orderId.toString());\n        log:printInfo(\"Order details fetched successfully\", 'order = 'order);\n        http:Ok response = {\n            body:  'order\n        };\n        return response;\n    }\n}\n</code></pre> </li> </ol> <p></p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/overview/#enable-observability-for-the-project","title":"Enable Observability for the project","text":"<p>Observability can be enabled in a BI project by adding the following section to the <code>Ballerina.toml</code> file by navigating to the file explorer view.</p> <pre><code>[build-options]\nobservabilityIncluded=true\n</code></pre>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/overview/#setting-up-runtime-configurations-for-observability","title":"Setting up runtime configurations for Observability","text":"<p>To enable observability (both metrics and tracing) in the BI runtime, use the following configurations in the <code>Ballerina.toml</code> file.</p> <pre><code>[ballerina.observe]\nenabled = true\nprovider = &lt;PROVIDER&gt;\n</code></pre> <p>Metrics and tracing can be enabled separately as well by using the following configurations. Add additional configurations specific to the tool or platform you are using.</p> <pre><code>[ballerina.observe]\nmetricsEnabled=true\nmetricsReporter=&lt;METRICS_REPORTER&gt;\ntracingEnabled=true\ntracingProvider=&lt;TRACING_PROVIDER&gt;\n</code></pre> Configuration key Description Default value Possible values <code>ballerina.observe.metricsEnabled</code> Whether metrics monitoring is enabled (true) or disabled (false) false <code>true</code> or <code>false</code> <code>ballerina.observe.metricsReporter</code> Reporter name that reports the collected Metrics to the remote metrics server. This is only required to be modified if a custom reporter is implemented and needs to be used. <code>None</code> <code>prometheus</code>, <code>newrelic</code>, or if any custom implementation, the name of the reporter. <code>ballerina.observe.tracingEnabled</code> Whether tracing is enabled (true) or disabled (false) false <code>true</code> or <code>false</code> <code>ballerina.observe.tracingProvider</code> The tracer name, which implements the tracer interface. <code>None</code> <code>jaeger</code>, <code>zipkin</code>, <code>newrelic</code> or the name of the tracer of any custom implementation."},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/overview/#observability-tools-and-platforms-supported-by-bi","title":"Observability tools and platforms supported by BI","text":"<p>This outlines how to enable and configure observability in BI for various tools and platforms. It provides a step-by-step guide for setting up monitoring, tracing, and logging using widely used observability solutions.</p> <p>Observability tools and platforms help monitor and analyze application performance, identify issues, and ensure reliability. The following are the main observability tools and platforms supported by BI:</p> <ul> <li> <p>Prometheus: A monitoring system and time-series database for metrics collection and alerting.</p> </li> <li> <p>Jaeger: A distributed tracing platform for monitoring and debugging microservices.</p> </li> <li> <p>Zipkin: A distributed tracing system to collect and look up trace data.</p> </li> <li> <p>New Relic: A full-stack observability platform for application performance monitoring (APM) and telemetry.</p> </li> <li> <p>Datadog: A cloud-based observability service offering monitoring, metrics, traces, and logging.</p> </li> <li> <p>Elastic Stack: A collection of tools (Elasticsearch, Logstash, Kibana) for centralized logging and analytics.</p> </li> </ul> <p>The following sections contain guides to set up and observe BI programs in each of the observability tools or platforms mentioned above.</p> <ul> <li>Observe metrics using Prometheus</li> <li>Observe tracing using Jaeger</li> <li>Observe tracing using Zipkin</li> <li>Observe metrics and tracing using New Relic</li> <li>Observe metrics and tracing using Datadog</li> <li>Observe logs using Elastic Stack</li> <li>Observe metrics, traces and logs using OpenSearch</li> <li>Observe metrics, traces and logs using Moesif</li> </ul>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/prometheus/","title":"Observe metrics using Prometheus","text":"<p>The sample shop service will be used in this guide. Follow the steps given below to observe BI metrics in Prometheus.</p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/prometheus/#step-1-set-up-prometheus","title":"Step 1 - Set up Prometheus","text":"<p>Prometheus is used as the monitoring system, which pulls out the metrics collected from the <code>/metrics</code> service exposed by BI runtime. This section focuses on the quick installation of Prometheus with Docker and the configuration required to collect metrics from the metrics service with the default configurations. Follow the steps below to configure Prometheus. </p> Tip<p>There are many other ways to install Prometheus and you can find possible options from the installation guide. The easiest option is to use precompiled binaries listed in Downloads.</p> <ol> <li> <p>Create a <code>prometheus.yml</code> file in a directory.</p> </li> <li> <p>Add the following content to the <code>prometheus.yml</code> file.</p> <pre><code>global:\nscrape_interval:     15s\nevaluation_interval: 15s\n\nscrape_configs:\n- job_name: 'prometheus'\nstatic_configs:\n- targets: ['a.b.c.d:9797']\n</code></pre> <p>Here, the <code>'a.b.c.d:9797'</code> targets should contain the host and port of the <code>/metrics</code> service that is exposed from  BI runtime for metrics collection. Add the IP of the host in which the BI service is running as <code>a.b.c.d</code> and its port (default <code>9797</code>). If you need more information, go to the Prometheus documentation. If your BI metrics service is running on localhost and Prometheus in a Docker container, add the target as <code>host.docker.internal:9797</code> to access the localhost from Docker.</p> </li> <li> <p>Start the Prometheus server in a Docker container with the command below.</p> <pre><code>$ docker run -p 9090:9090 -v &lt;path_to_prometheus.yml&gt;:/etc/prometheus/ prom/prometheus\n</code></pre> </li> </ol>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/prometheus/#step-2-import-prometheus-extension-for-bi","title":"Step 2 - Import Prometheus extension for BI","text":"<p>Create the sample shop service. To include the Prometheus extension into the executable, the <code>ballerinax/prometheus</code> module needs to be imported into your BI project. Navigate to file explorer and add the following to the <code>main.bal</code> file.</p> <pre><code>import ballerinax/prometheus as _;\n</code></pre> <p>To support Prometheus as the metrics reporter, an HTTP endpoint starts with the context of <code>/metrics</code> in the default port <code>9797</code> when starting the service in BI.</p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/prometheus/#step-3-configure-runtime-configurations-for-observability","title":"Step 3 - Configure runtime configurations for observability","text":"<p>You can set up Prometheus for your BI project using configurations similar to the following in the <code>Config.toml</code> file. Navigate to file explorer and add the following to the <code>Config.toml</code> file.</p> <pre><code>[ballerina.observe]\nmetricsEnabled=true\nmetricsReporter=\"prometheus\"\n\n[ballerinax.prometheus]\nport=9797\nhost=\"0.0.0.0\"\n</code></pre> Configuration key Description Default value Possible values <code>ballerinax.prometheus.port</code> The value of the port to which the '/metrics' service will bind. This service will be used by Prometheus to scrape the information of the BI service. <code>9797</code> Any suitable value for port 0 - 0 - 65535. However, within that range, ports <code>0</code> - <code>1023</code> are generally reserved for specific purposes. Therefore, it is advisable to select a port outside that range. <code>ballerinax.prometheus.host</code> The name of the host to which the '/metrics' service will bind. This service will be used by Prometheus to scrape the information of the BI service. <code>0.0.0.0</code> IP or Hostname or <code>0.0.0.0</code> of the node in which the BI service is running."},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/prometheus/#step-4-run-the-bi-service","title":"Step 4 - Run the BI service","text":"<p>When observability is enabled, the BI runtime exposes internal metrics via an HTTP endpoint (<code>/metrics</code>) for metrics monitoring, and the metrics will be published to Prometheus. Prometheus should be configured to scrape metrics from the metrics HTTP endpoint in BI.</p> <p>Start the BI service and you'll notice an output similar to the following.</p> <pre><code>Compiling source\n\nRunning executable\n\nballerina: started Prometheus HTTP listener 0.0.0.0:9797\n</code></pre>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/prometheus/#step-5-send-requests","title":"Step 5 - Send requests","text":"<p>Send requests to <code>http://localhost:8090/shop/products</code>.</p> <p>Example cURL commands:</p> <p><pre><code>$ curl -X GET http://localhost:8090/shop/products\n</code></pre> <pre><code>$ curl -X POST http://localhost:8090/shop/product \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"id\": 4, \n    \"name\": \"Laptop Charger\", \n    \"price\": 50.00\n}'\n</code></pre> <pre><code>$ curl -X POST http://localhost:8090/shop/order \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"productId\": 1, \n    \"quantity\": 1\n}'\n</code></pre> <pre><code>$ curl -X GET http://localhost:8090/shop/order/0\n</code></pre></p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/prometheus/#step-6-view-metrics-on-the-prometheus-server","title":"Step 6 - View metrics on the Prometheus server","text":"<p>Go to http://localhost:19090/ and check whether you can see the Prometheus graph. BI metrics should appear in the Prometheus graph's metrics list when the BI service is started.</p> <p></p> <p></p> <p>You can also use the following command to get the metrics.</p> <pre><code>$ curl http://localhost:9797/metrics\n</code></pre>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/prometheus/#set-up-grafana","title":"Set up Grafana","text":"<p>Grafana can be used to visualize BI metrics provided for Prometheus. First, users need to set up the BI project to observe metrics in Prometheus and follow the steps mentioned above.</p> <p>Let\u2019s use Grafana to visualize metrics in a dashboard. For this, we need to install Grafana and configure Prometheus as a data source. Follow the steps below to configure Grafana.</p> <ol> <li> <p>Start Grafana as a Docker container with the command below.</p> <p><pre><code>$ docker run -d --name=grafana -p 3000:3000 grafana/grafana\n</code></pre> For more information, go to Grafana in Docker Hub.</p> </li> <li> <p>Go to http://localhost:3000/ to access the Grafana dashboard running on Docker.</p> </li> <li> <p>Login to the dashboard with the default user, username: <code>admin</code> and password: <code>admin</code></p> </li> <li> <p>Add Prometheus as a data source with the <code>Browser</code> access configuration as provided below.</p> </li> </ol> <p></p> <ol> <li>Import the Grafana dashboard designed to visualize BI metrics from https://grafana.com/dashboards/5841 as shown below.</li> </ol> <p></p> <p>This dashboard consists of service and client invocation level metrics in near real-time view. </p> <p>The BI HTTP service metrics dashboard panel will be as shown below.</p> <p></p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/zipkin/","title":"Observe tracing using Zipkin","text":"<p>The sample shop service will be used in this guide. Follow the steps given below to observe BI tracing in Zipkin.</p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/zipkin/#step-1-set-up-zipkin","title":"Step 1 - Set up Zipkin","text":"<p>You can configure BI to support distributed tracing with Zipkin. This section focuses on configuring Zipkin with Docker as a quick installation.</p> Tip<p>There are many possible ways to deploy Zipkin. For more information, see Zipkin Quickstart.</p> <p>Install Zipkin via Docker and start the Docker container by executing the command below.</p> <pre><code>$ docker run -d -p 9411:9411 openzipkin/zipkin\n</code></pre>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/zipkin/#step-2-import-ballerina-zipkin-extension","title":"Step 2 - Import Ballerina Zipkin extension","text":"<p>Create the sample shop service. To include the Zipkin extension into the executable, the <code>ballerinax/zipkin</code> module needs to be imported into your BI project by navigating to file explorer and adding the following to <code>main.bal</code> file.</p> <pre><code>import ballerinax/zipkin as _;\n</code></pre> <p>Zipkin extension has a <code>Zipkin Span Exporter</code> which will push tracing data as batches to the Zipkin server endpoint (default - http://localhost:9411) in Zipkin format.</p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/zipkin/#step-3-configure-runtime-configurations-for-observability","title":"Step 3 - Configure runtime configurations for observability","text":"<p>Tracing can be enabled in your BI project using configurations similar to the following in your <code>Config.toml</code> file.</p> <pre><code>[ballerina.observe]\ntracingEnabled=true\ntracingProvider=\"zipkin\"\n\n[ballerinax.zipkin]\nagentHostname=\"localhost\"\nagentPort=9411\nsamplerType=\"const\"\nsamplerParam=1.0\nreporterFlushInterval=1000\nreporterBufferSize=10000\n</code></pre> <p>The table below provides the descriptions of each configuration option and possible values that can be assigned.</p> Configuration key Description Default value Possible values ballerinax.zipkin. agentHostname Hostname of the Zipkin agent localhost IP or hostname of the Zipkin agent. If it is running on the same node as Ballerina, it can be localhost. ballerinax.zipkin. agentPort Port of the Zipkin agent 4317 The port on which the Zipkin agent is listening. ballerinax.zipkin. samplerType Type of the sampling methods used in the Zipkin tracer. const <code>const</code>, <code>probabilistic</code>, or <code>ratelimiting</code>. ballerinax.zipkin. samplerParam It is a floating value. Based on the sampler type, the effect of the sampler param varies 1.0 For <code>const</code> <code>0</code> (no sampling) or <code>1</code> (sample all spans), for <code>probabilistic</code> <code>0.0</code> to <code>1.0</code>, for <code>ratelimiting</code> any positive integer (rate per second). ballerinax.zipkin. reporterFlushInterval The Zipkin client will be sending the spans to the agent at this interval. 2000 Any positive integer value. ballerinax.zipkin. reporterBufferSize Queue size of the Zipkin client. 2000 Any positive integer value."},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/zipkin/#step-4-run-the-bi-service","title":"Step 4 - Run the BI service","text":"<p>When BI observability is enabled, the BI runtime collects tracing data and traces will be published to Zipkin.</p> <p>Run the the BI service. </p> <pre><code>Compiling source\n\nRunning executable\n\nballerina: started publishing traces to Zipkin on http://localhost:9411\n</code></pre>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/zipkin/#step-5-send-requests","title":"Step 5 - Send requests","text":"<p>Send requests to http://localhost:8090/shop/products.</p> <p>Example cURL commands:</p> <p><pre><code>$ curl -X GET http://localhost:8090/shop/products\n</code></pre> <pre><code>$ curl -X POST http://localhost:8090/shop/product \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"id\": 4, \n    \"name\": \"Laptop Charger\", \n    \"price\": 50.00\n}'\n</code></pre> <pre><code>$ curl -X POST http://localhost:8090/shop/order \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"productId\": 1, \n    \"quantity\": 1\n}'\n</code></pre> <pre><code>$ curl -X GET http://localhost:8090/shop/order/0\n</code></pre></p>"},{"location":"observability-and-monitoring/supported-observability-tools-and-platforms/zipkin/#step-6-view-distributed-tracing-on-the-zipkin-server","title":"Step 6 - View distributed tracing on the Zipkin server","text":"<p>Go to http://localhost:9411 and load the web UI of Zipkin to make sure it is functioning properly. You can select the service for which you need tracing information find traces.</p> <p>The image below is the sample tracing information you can see in Zipkin.</p> <p></p>"},{"location":"references/ai-usage-and-data-handling-guidelines/","title":"AI Usage and Data Handling Guidelines","text":"<p>WSO2 Integrator: BI provides an AI-powered Copilot to enhance developer productivity. This page explains how the Copilot works, how user data is handled, and what best practices organizations should follow when using AI features.  </p> <p>These guidelines are designed to ensure transparency, security, and compliance when using AI-powered assistance in enterprise environments.</p>"},{"location":"references/ai-usage-and-data-handling-guidelines/#macro-architecture","title":"Macro architecture","text":"<p>The AI Copilot is integrated into the WSO2 Integrator: BI developer experience. It works as follows:</p> <p></p> <ul> <li>AI Copilot Code: Delivered as a Visual Studio Code (VS Code) extension, providing in-editor assistance such as code completion, explanations, and suggestions.  </li> <li>Language Server: Powers intelligent features inside the IDE, including syntax awareness and integration with Copilot services.  </li> <li>BI Intelligence Endpoint: A lightweight intermediary service that connects the extension to Anthropic or Bedrock models. This service does not retain data.  </li> <li>Anthropic or Bedrock Integration: The endpoint forwards user prompts and context to the selected Large Language Model (LLM) provider for processing.</li> </ul>"},{"location":"references/ai-usage-and-data-handling-guidelines/#authentication","title":"Authentication","text":"<p>To maintain security, all AI Copilot features require authentication:</p> <ul> <li>Users must log in to enable Copilot functionality.  </li> <li>Social login options are supported for ease of use.  </li> <li>Authentication and session management are handled by Asgardeo, WSO2\u2019s identity provider.  </li> </ul> <p>This ensures that only authorized users in your organization can access Copilot features.</p>"},{"location":"references/ai-usage-and-data-handling-guidelines/#data-flow","title":"Data flow","text":"<p>The movement of data through the Copilot is designed for zero-retention at the intermediary layer:</p> <p></p> <ul> <li>Direct Forwarding: BI Intelligence forwards user data directly to Anthropic for processing</li> <li>No Local Storage: BI Intelligence does not store any user data locally</li> <li>Real-time Processing: All data handling occurs in real-time without persistent storage at the BI Intelligence layer</li> </ul>"},{"location":"references/ai-usage-and-data-handling-guidelines/#bring-your-own-key-byok","title":"Bring your own key (BYOK)","text":"<p>Organizations can configure the Copilot to run using their own model provider accounts. This ensures enterprise-level control over data governance and billing.</p>"},{"location":"references/ai-usage-and-data-handling-guidelines/#anthropic-deployment","title":"Anthropic deployment","text":"<ul> <li>Copilot can connect directly to Anthropic\u2019s public deployments.  </li> <li>Requires an Anthropic API key that you provide.  </li> <li>This setup ensures that data flows directly between your environment and Anthropic without WSO2 retaining it.  </li> </ul>"},{"location":"references/ai-usage-and-data-handling-guidelines/#amazon-bedrock","title":"Amazon Bedrock","text":"<ul> <li>Copilot can also run using Claude models deployed on Amazon Bedrock.  </li> <li>Requires an active Claude deployment in your Amazon Bedrock environment.  </li> <li>Users must provide their own access keys for connectivity.</li> </ul>"},{"location":"references/ai-usage-and-data-handling-guidelines/#ballerina-copilot-code","title":"Ballerina copilot code","text":"<p>The Copilot is open source, enabling transparency and community contribution:</p> <ul> <li>The full source code is available for inspection, download, and modification.  </li> <li>This allows organizations to validate the behavior of the Copilot.  </li> <li>Enterprises can also extend the code to adapt to custom compliance needs.  </li> </ul> <p>This openness ensures that security-conscious users can audit how prompts and data are handled.</p>"},{"location":"references/ai-usage-and-data-handling-guidelines/#feedback-data","title":"Feedback data","text":"<p>To improve the Copilot experience, user feedback may be collected. </p> <p>Retention period</p> <ul> <li>Feedback data (such as thumbs up/down ratings) is retained for 1 week only.  </li> <li>After 1 week, feedback records are permanently deleted.  </li> </ul> <p>Collection scope</p> <ul> <li>Feedback is collected only when a user explicitly provides it.  </li> <li>No hidden or passive data collection is performed.  </li> </ul> <p>Transparency</p> <ul> <li>The feedback interface clearly explains what is being collected and why.  </li> <li>Users always have control over whether to provide feedback.</li> </ul>"},{"location":"references/ai-usage-and-data-handling-guidelines/#guidelines","title":"Guidelines","text":"<p>When using AI features, organizations must apply standard security and compliance practices.</p>"},{"location":"references/ai-usage-and-data-handling-guidelines/#data-usage-policies","title":"Data usage policies","text":"<ul> <li>All operations are subject to the Anthropic Data Usage Policy or the chosen model provider\u2019s terms.  </li> <li>WSO2 ensures that the Copilot does not bypass these policies.  </li> </ul>"},{"location":"references/ai-usage-and-data-handling-guidelines/#organizational-data-storage","title":"Organizational data storage","text":"<p>How long do we store your organization's data?</p> <p>We follow a zero-retention policy at the BI Intelligence level - your organizational data is not stored by our intermediate services.</p>"},{"location":"references/ai-usage-and-data-handling-guidelines/#best-practices","title":"Best practices","text":"<p>To ensure maximum security and privacy, we recommend avoiding sending organizational-specific details such as:</p> <ul> <li>Customer personal information</li> <li>Passwords or authentication credentials</li> <li>Proprietary business data</li> <li>Sensitive internal communications</li> </ul> <p>General Copilot Best Practices are as follows. </p> <ul> <li>Review all AI-generated code before implementation</li> <li>Be mindful of what information you include in prompts</li> <li>Use generic examples rather than real data when possible</li> <li>Follow your organization's data governance policies</li> </ul>"},{"location":"references/ai-usage-and-data-handling-guidelines/#data-retention-summary","title":"Data retention summary","text":"Data Type Retention Period Notes Code Prompts &amp; Responses Not stored by BI Intelligence Forwarded directly to Anthropic or Bedrock User Feedback 1 week Retained only when explicitly provided by the user Authentication Tokens Session-based Managed securely by Asgardeo Organizational Data Not stored Zero-retention policy at BI Intelligence"},{"location":"references/connector-development-guidelines/","title":"Connector development guidelines","text":"<p>This guide outlines the complete lifecycle for developing Ballerina connectors for BI, from initial conception through general availability. Following these guidelines ensures consistent quality and maintainability across all connector implementations.</p>"},{"location":"references/connector-development-guidelines/#naming-conventions","title":"Naming conventions","text":""},{"location":"references/connector-development-guidelines/#package-names","title":"Package names","text":"<p>Package names should use lowercase letters and be descriptive of the service or API being integrated. The name should clearly indicate the external system the connector interacts with.</p> <p>Examples: - <code>ballerinax/salesforce</code> - <code>ballerinax/stripe</code> - <code>ballerinax/mongodb</code></p>"},{"location":"references/connector-development-guidelines/#submodules","title":"Submodules","text":"<p>Submodules should be named in lowercase letters and should reflect the sub-functionality they contain. Submodules should primarily organize code structure within a single connector package. Avoid defining multiple sub-clients in each submodule.</p>"},{"location":"references/connector-development-guidelines/#hierarchical-packages","title":"Hierarchical packages","text":"<p>Hierarchical package names are recommended when you need to distinguish between multiple connectors for the same vendor or service provider. This approach is particularly useful for SaaS products with independent API versioning.</p> <p>When to use hierarchical packages: - When dealing with the same vendor but different services (e.g., <code>ballerinax/azure.cosmosdb</code> vs <code>ballerinax/azure.storage</code>) - When a SaaS product has multiple independent APIs with separate versioning (e.g., <code>ballerinax/salesforce.bulk</code> vs <code>ballerinax/salesforce.soap</code>)</p> <p>Examples: - <code>ballerinax/azure.cosmosdb</code> - <code>ballerinax/azure.storage</code> - <code>ballerinax/salesforce.bulk</code> - <code>ballerinax/salesforce.commons</code> - <code>ballerinax/openai.chat</code> - <code>ballerinax/googleapis.gmail</code></p>"},{"location":"references/connector-development-guidelines/#best-practices","title":"Best practices","text":"<ul> <li>Clarity and readability: Use simple nouns that clearly represent the service. Package names should be clear and descriptive, making it easy for developers to understand the purpose of each package. Names like <code>azureEventHub</code> or <code>azure_event_hub</code> are not idiomatic in Ballerina.</li> <li>Avoid complex patterns: Do not use camelCase or snake_case in package names</li> <li>Use recognized abbreviations: Only use abbreviations that are widely recognized by developers (e.g., <code>aws</code>, <code>gcp</code>)</li> <li>Maintain consistency: Ensure naming is consistent throughout the connector for an intuitive developer experience</li> <li>Avoid overly deep hierarchies: While nesting is useful for organization, avoid creating overly deep hierarchies as they increase complexity</li> <li>Avoid split-module conditions: A split-module condition occurs when different package versions contain the same module, which causes build failures. Choose hierarchical structures carefully to minimize such situations.</li> </ul>"},{"location":"references/connector-development-guidelines/#repository-setup","title":"Repository setup","text":"<p>Connectors are developed in dedicated GitHub repositories following a consistent naming pattern:</p> <p>Repository Pattern: <code>module-[org-name]-[connector-name]</code></p> <p>Examples: - <code>module-ballerinax-azure.cosmosdb</code> - <code>module-ballerinax-salesforce</code> - <code>module-ballerinax-stripe</code></p> <p>This structure allows for independent versioning, testing, and release cycles for each connector.</p>"},{"location":"references/connector-development-guidelines/#project-structure","title":"Project structure","text":"<p>A standard connector library includes the following components:</p>"},{"location":"references/connector-development-guidelines/#required-components","title":"Required components","text":"<ul> <li>Ballerina source code: Core implementation of the connector</li> <li>Tests: Unit and integration(if possible) tests to ensure functionality</li> <li>Examples: Sample code demonstrating connector usage</li> <li>Documentation: User-facing documentation and API references</li> </ul>"},{"location":"references/connector-development-guidelines/#optional-components","title":"Optional components","text":"<ul> <li>Native code: Platform-specific implementations when required</li> <li>Compiler plugins: Custom compile-time behaviors</li> <li>Compiler plugin tests: Tests for custom compiler plugins</li> <li>Integration tests: End-to-end tests (typically in separate packages)</li> </ul>"},{"location":"references/connector-development-guidelines/#build-and-release-tools","title":"Build and release tools","text":"<ul> <li>Build tool: Gradle</li> <li>CI/CD: GitHub Actions for automated testing and releases</li> </ul> <p>Build automation for different connector types:</p> <ul> <li> <p>Modules bundled with ballerina-distribution: Use intermediate packs that incorporate timestamped versions from the Standard Library. This approach maintains build stability and enables prompt failure detection via GitHub packages.</p> </li> <li> <p>Ballerinax modules published independently: Should use nightly builds from <code>ballerina-distribution</code> instead of timestamped builds. The nightly pack can be obtained from the Daily Build in the ballerina-distribution repository. This reduces resource consumption since timestamped builds aren't necessary for independently published modules.</p> </li> </ul>"},{"location":"references/connector-development-guidelines/#connector-design-principles","title":"Connector design principles","text":""},{"location":"references/connector-development-guidelines/#one-to-one-mapping","title":"One-to-one mapping","text":"<p>Maintain a one-to-one mapping with external services whenever possible. The connector should reflect the structure and operations of the external API it wraps.</p> <p>For REST services, resource functions should correspond to external service resources. Combining multiple external resources into single functions is not recommended as it reduces clarity and makes the connector harder to maintain.</p>"},{"location":"references/connector-development-guidelines/#design-decisions","title":"Design decisions","text":"<p>Any design decisions that differ from the external system require special justification and should be documented in the specification.</p>"},{"location":"references/connector-development-guidelines/#scope-management","title":"Scope management","text":"<p>It is not expected that one connector can map to multiple different APIs, even within the same system. Avoid combining multiple independent APIs into a single connector package.</p> <p>Important guideline: Systems with multiple independent REST APIs (e.g., separate sales and finance APIs within the same platform) should have separate, independent connectors rather than one package with multiple modules. Each connector should have a clear, well-defined scope aligned with a specific service or API.</p>"},{"location":"references/connector-development-guidelines/#implementation-approaches","title":"Implementation approaches","text":""},{"location":"references/connector-development-guidelines/#rest-api-connectors-oas-based","title":"REST API connectors (OAS-based)","text":"<p>For REST API-based services, follow these steps:</p> <ol> <li> <p>Obtain OpenAPI specification (OAS): Get the OAS directly from the service provider. Important: It is not recommended to obtain OAS specifications from third-party providers as they may be outdated or incomplete.</p> </li> <li> <p>Validate operations: The OAS must be validated against actual API operations, as specifications often become outdated. Test all operations against the actual API to ensure correctness before proceeding.</p> </li> <li> <p>Ensure examples in OAS: The OAS must include an example request and response for each resource action. These examples are essential for mock server generation and testing.</p> </li> <li> <p>Add OAS to repository: Include the specification file in the wso2/api-specs repository if not already present. The aligned OAS specification file against the Ballerina conventions should be added to the connector repository. Adding the specification to the repository signifies completion of validation and required modifications.</p> </li> <li> <p>Generate code: Generate Ballerina client code with resource functions (preferred) or remote functions using the OAS. If using remote functions instead of resource functions, provide justification for this decision.</p> </li> <li> <p>Generate mock server: Use the OAS to generate a mock server for testing purposes.</p> </li> <li> <p>Add unit tests: Implement unit tests to cover all connector functionality.</p> </li> <li> <p>Write documentation: Provide comprehensive documentation for users on how to use the connector, including getting started guides, examples and API references.</p> </li> </ol> <p>For more information on developing OAS-based connectors, please refer to the Create Your First Connector with Ballerina guide. </p>"},{"location":"references/connector-development-guidelines/#handwritten-connectors","title":"Handwritten connectors","text":"<p>Handwritten connectors are used when wrapping external SDKs or when the API does not have a suitable specification.</p> <p>Characteristics: - Wrap external SDKs with Ballerina code - Typically require minimal implementation - Operations should be stateless and independent - Must include a <code>spec.md</code> file describing functionality</p>"},{"location":"references/connector-development-guidelines/#specification-requirements","title":"Specification requirements","text":"<ul> <li> <p>OAS-based connectors: For REST API connectors, the OpenAPI Specification serves as the primary specification. Document all sanitations (modifications and alterations to the OAS) in a dedicated <code>sanitations.md</code> file within the repository.</p> </li> <li> <p>Handwritten connectors: Must include a <code>spec.md</code> file that comprehensively describes all functionality and operations provided by the connector.</p> </li> </ul>"},{"location":"references/connector-development-guidelines/#testing-strategy","title":"Testing strategy","text":""},{"location":"references/connector-development-guidelines/#test-necessity","title":"Test necessity","text":"<p>Tests are essential to guard against language changes and potential regressions. All connectors must include comprehensive tests. The primary reasons for testing include:</p> <ul> <li>Validating implementation against contracts: Ensuring the connector behaves according to specifications (not common)</li> <li>Catching regression issues: Detecting unintended changes in functionality (not common)</li> <li>Detecting breaking changes from language updates: Identifying issues when Ballerina language is updated (relatively common)</li> <li>Validating against platforms: Testing compatibility with GraalVM and different Java versions (not common)</li> </ul> <p>Note: Some handwritten connectors may include custom logic. In such scenarios, using tests to cover the custom logic and maintaining 80% coverage is mandatory.</p>"},{"location":"references/connector-development-guidelines/#test-execution","title":"Test execution","text":"<ul> <li>Timing: Run tests during releases or when we add new changes to the connector</li> <li>Coverage: Maintain at least 80% code coverage for any custom business logic</li> </ul>"},{"location":"references/connector-development-guidelines/#testing-environments","title":"Testing environments","text":"<p>Prioritize testing approaches in the following order:</p> <ol> <li>Mocking: Mock external backends using test frameworks.</li> <li>Docker Images: Use containerized versions of services.</li> <li>SaaS Connections: Connect to actual SaaS endpoints.</li> </ol> <p>Mocking is preferred as it provides faster, more reliable, and cost-effective testing.</p>"},{"location":"references/connector-development-guidelines/#graalvm-compatibility","title":"GraalVM compatibility","text":""},{"location":"references/connector-development-guidelines/#pure-ballerina-connectors","title":"Pure Ballerina connectors","text":"<p>Connectors written entirely in Ballerina are GraalVM-compatible only if all their dependencies are also compatible.</p>"},{"location":"references/connector-development-guidelines/#mixed-dependencies","title":"Mixed dependencies","text":"<p>Connectors with native code or Java dependencies require: - Explicit GraalVM compatibility verification - Documentation of compatibility status - Testing with GraalVM native image builds</p>"},{"location":"references/connector-development-guidelines/#observability","title":"Observability","text":"<p>Once a connector is developed, it is important to verify that it includes appropriate observability features for monitoring and tracing in production environments.</p>"},{"location":"references/connector-development-guidelines/#metrics","title":"Metrics","text":"<p>Connectors should implement appropriate metrics that are compatible with monitoring tools like Grafana. This enables users to track connector performance, error rates, and usage patterns in production.</p>"},{"location":"references/connector-development-guidelines/#tracing","title":"Tracing","text":"<p>Connectors should work well with distributed tracing tools like Jaeger. Proper tracing support helps users debug issues in complex integration flows and understand the full request lifecycle.</p>"},{"location":"references/connector-development-guidelines/#http-based-connectors","title":"HTTP-based connectors","text":"<p>HTTP-based connectors (those built on top of <code>http:Client</code>) already include built-in observability features through the underlying HTTP client. These connectors automatically support metrics and tracing without additional implementation.</p>"},{"location":"references/connector-development-guidelines/#other-connector-types","title":"Other connector types","text":"<p>Connectors that are not HTTP-based (e.g., database connectors, messaging connectors) may need to explicitly add metrics and tracing information. Review the observability requirements and add any missing instrumentation.</p>"},{"location":"references/connector-development-guidelines/#connector-maintenance","title":"Connector maintenance","text":""},{"location":"references/connector-development-guidelines/#version-management","title":"Version management","text":"<p>Connector versions should follow semantic versioning (SemVer) principles:</p> <ul> <li>Major Version: Breaking changes to the API</li> <li>Minor Version: New features, backward-compatible</li> <li>Patch Version: Bug fixes and minor improvements</li> </ul>"},{"location":"references/connector-development-guidelines/#tracking-endpoint-api-changes","title":"Tracking endpoint API changes","text":"<p>Connector maintainers must actively track changes to the underlying endpoint APIs:</p> <ol> <li>Monitor API Updates: Regularly check for new versions or updates to the external API</li> <li>Subscribe to Notifications: Subscribe to API provider's changelog or release notifications</li> <li>Review Breaking Changes: Assess whether API changes require connector updates</li> </ol>"},{"location":"references/connector-development-guidelines/#release-new-versions","title":"Release new versions","text":"<p>Release a new connector version whenever:</p> <ul> <li>New API Version Available: The endpoint API releases a new version with new features or operations</li> <li>Breaking Changes: The external API introduces breaking changes that require connector updates</li> <li>Deprecated Operations: The API deprecates operations that need to be marked or removed</li> <li>Bug Fixes: Issues are discovered in the connector implementation</li> <li>Security Updates: Security vulnerabilities are identified in dependencies or the connector itself</li> </ul>"},{"location":"references/connector-development-guidelines/#version-release-process","title":"Version release process","text":"<ol> <li>Update dependencies: Update to the latest stable dependencies</li> <li>Test thoroughly: Run full test suite against the new API version</li> <li>Update documentation: Reflect any API changes in connector documentation</li> <li>Update changelog: Document all changes in the changelog</li> <li>Tag release: Create a git tag following the versioning scheme</li> <li>Publish: Publish to Ballerina Central</li> </ol>"},{"location":"references/connector-development-guidelines/#backward-compatibility","title":"Backward compatibility","text":"<p>When possible, maintain backward compatibility:</p> <ul> <li>Deprecate rather than immediately remove features</li> <li>Provide migration guides for breaking changes</li> </ul>"},{"location":"references/connector-development-guidelines/#deprecation-policy","title":"Deprecation policy","text":"<p>When deprecating connector features:</p> <ol> <li>Mark the feature as deprecated in the code and documentation</li> <li>Suggest alternative approaches in deprecation messages</li> <li>Update examples and documentations to use recommended patterns</li> </ol>"},{"location":"references/enterprise-integrations-patterns/","title":"Enterprise Integrations Patterns","text":"<p>The WSO2 Integrator: BI supports the implementation of key Enterprise Integration Patterns (EIPs), enabling you to build robust and scalable integrations based on proven architectural best practices. These patterns\u2014originally defined in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf\u2014provide reusable solutions for common messaging and system integration challenges. This guide demonstrates how to implement each core pattern using the low-code capabilities and visual tools of the WSO2 Integrator: BI, helping you design clear, maintainable, and standards-based integration flows.</p> Enterprise Integration Patterns with Ballerina<p>For a code-centric implementation of Enterprise Integration Patterns using the Ballerina language, refer to the Ballerina EIP guide.</p>"},{"location":"references/enterprise-integrations-patterns/#messaging-systems","title":"Messaging systems","text":"Message How can two applications connected by a message channel exchange a piece of information Message Endpoint How an application connects to a messaging channel to send and receive messages Message Translator How systems using different data formats communicate with each other using messaging Message Router How to decouple individual processing steps so that messages can be passed to different filters depending on a set of conditions Pipes and Filters How to perform complex processing on a message while maintaining independence and flexibility"},{"location":"references/enterprise-integrations-patterns/#messaging-channels","title":"Messaging channels","text":"Channel Adapter How can two applications connected by a message channel exchange a piece of information Messaging Bridge How an application connects to a messaging channel to send and receive messages Point to Point Channel How systems using different data formats communicate with each other using messaging"},{"location":"references/enterprise-integrations-patterns/#message-construction","title":"Message construction","text":"Command Message How messaging can be used to invoke a procedure in another application Document Message How messaging can be used to transfer data between applications. Event Message How messaging can be used to transmit events from one application to another Format Indicator How a message\u2019s data format can be designed to allow for possible future changes Message Sequence How messaging can transmit an arbitrarily large amount of data"},{"location":"references/enterprise-integrations-patterns/#message-routing","title":"Message routing","text":"Content-Based Router How to handle a situation where the implementation of a single logical function is spread across multiple physical systems Aggregator How to combine the results of individual, but related messages so that they can be processed as a whole Message Filter How a component avoids receiving uninteresting messages Process Manager How to route a message through multiple processing steps, when the required steps may not be known at design time and may not be sequential Routing Slip How to route a message consecutively through a series of steps when the sequence of the steps is not known at design time and may vary for each message Splitter How to process a message if it contains multiple elements, each of which may have to be processed in a different way"},{"location":"references/enterprise-integrations-patterns/#message-transformation","title":"Message transformation","text":"Content Enricher How to communicate with another system if the message originator does not have all the required data items available Content Filter How to simplify dealing with a large message when you are interested only in a few data items Normalizer How to process messages that are semantically equivalent but arrive in a different format"},{"location":"references/enterprise-integrations-patterns/#messaging-endpoints","title":"Messaging Endpoints","text":"Idempotent Receiver How can a message receiver deal with duplicate messages"},{"location":"references/enterprise-integrations-patterns/#system-management","title":"System Management","text":"Message Store How to report against message information without disturbing the loosely coupled and transient nature of a messaging system"},{"location":"references/system-requirements/","title":"WSO2 Integrator: BI System requirements","text":"<p>Prior to installing WSO2 Integrator: BI, make sure that the appropriate prerequisites are fulfilled.</p> Minimum <p>(Suitable for smaller integrations)</p> <ul> <li>           0.2 core (compute units with at least 1.0-1.2 GHz Opteron/Xeon processor)         </li> <li>           512 MB heap size         </li> </ul> Recommended <p>(Suitable for larger integrations)</p> <ul> <li>           1 core (compute units with at least 1.0-1.2 GHz Opteron/Xeon processor)         </li> <li>           1 GB heap size         </li> </ul>"},{"location":"references/system-requirements/#environment-compatibility","title":"Environment compatibility","text":"<p>The details of the tested environments for the WSO2 Integrator: BI are given below.</p>"},{"location":"references/system-requirements/#tested-operating-systems","title":"Tested operating systems","text":"<p>The WSO2 Integrator: BI runtime is tested with the following operating systems:</p> Operating System Versions Windows 10+ Ubuntu 24.04 Red Hat Enterprise Linux 9 MacOS 14.6"},{"location":"references/system-requirements/#tested-java-runtime-environments","title":"Tested Java Runtime Environments","text":"<p>The WSO2 Integrator: BI runtime is tested with the following JREs:</p> <p>A compatible JRE version will be automatically installed during the Ballerina installation process if one is not already available in the environment.</p> JRE Versions CorrettoJRE 21 AdoptOpenJRE 21 OpenJRE 21 Oracle JRE 21"},{"location":"references/system-requirements/#arm-compatibility","title":"ARM compatibility","text":"<p>WSO2 Integrator: BI is compatible with ARM processors. It can run on ARM-based systems, such as those with Apple Silicon or ARM-based Linux distributions.</p>"}]}